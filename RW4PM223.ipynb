{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "df_imputed = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_imputed = df_imputed.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in ['Smoking', 'Alcohol', 'Punctuality', 'Lying', 'Internet usage',\n",
    "        'Gender', 'Left - right handed', 'Education', 'Only child',\n",
    "        'Village - town', 'House - block of flats']:\n",
    "    df_imputed = df_imputed.drop(col,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Fear of public speaking' in df_imputed:\n",
    "    y = df_imputed['Fear of public speaking'].values # get the labels we want\n",
    "    del df_imputed['Fear of public speaking'] # get rid of the class label\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[    90.77700297]\n",
      " [   455.69584136]\n",
      " [   310.12282053]\n",
      " [   296.38078132]\n",
      " [   214.42342046]\n",
      " [   201.30932795]\n",
      " [   284.12760523]\n",
      " [   263.20815517]\n",
      " [   333.14489982]\n",
      " [   364.99230948]\n",
      " [   228.86059827]\n",
      " [   241.9717399 ]\n",
      " [   274.68285292]\n",
      " [   264.37882356]\n",
      " [   260.24370786]\n",
      " [   300.44206565]\n",
      " [   277.56896059]\n",
      " [   265.78549451]\n",
      " [   220.90069261]\n",
      " [   206.92855259]\n",
      " [   442.66107212]\n",
      " [   260.16446656]\n",
      " [   325.49531991]\n",
      " [   430.09429155]\n",
      " [   335.33045895]\n",
      " [   298.48972215]\n",
      " [   297.55702358]\n",
      " [   364.28971172]\n",
      " [   368.65984636]\n",
      " [   343.53062955]\n",
      " [   193.1951201 ]\n",
      " [   335.3350492 ]\n",
      " [   301.30380176]\n",
      " [   295.91607141]\n",
      " [   236.5990918 ]\n",
      " [   226.2106571 ]\n",
      " [   194.13126135]\n",
      " [   401.21182102]\n",
      " [   294.74523909]\n",
      " [   246.5078163 ]\n",
      " [   257.8183756 ]\n",
      " [   205.99101786]\n",
      " [   310.03972669]\n",
      " [   295.91459597]\n",
      " [   363.12174831]\n",
      " [   239.86763523]\n",
      " [   198.50729774]\n",
      " [   246.35113701]\n",
      " [   250.71823881]\n",
      " [   211.53509964]\n",
      " [   348.52190748]\n",
      " [   221.99802145]\n",
      " [   212.94308208]\n",
      " [   178.36042427]\n",
      " [   319.9578776 ]\n",
      " [   295.29779702]\n",
      " [   182.57322387]\n",
      " [   227.53275878]\n",
      " [   312.07155741]\n",
      " [   307.3914249 ]\n",
      " [   284.13006429]\n",
      " [   430.56441139]\n",
      " [   266.10204987]\n",
      " [   323.6206603 ]\n",
      " [   201.84844301]\n",
      " [   188.73943256]\n",
      " [   226.90481208]\n",
      " [   254.77263776]\n",
      " [   285.99251056]\n",
      " [   302.54026857]\n",
      " [   240.09658848]\n",
      " [   249.15349509]\n",
      " [   301.52537782]\n",
      " [   283.26972165]\n",
      " [   285.84943808]\n",
      " [   242.214218  ]\n",
      " [   287.64108582]\n",
      " [   279.36806749]\n",
      " [   325.5753809 ]\n",
      " [   252.27757258]\n",
      " [   366.32146047]\n",
      " [   382.01125293]\n",
      " [   257.11946644]\n",
      " [   359.84230303]\n",
      " [   314.25883788]\n",
      " [   208.64186069]\n",
      " [   247.0486527 ]\n",
      " [   316.82691475]\n",
      " [   323.004763  ]\n",
      " [   354.29633598]\n",
      " [   374.59988874]\n",
      " [   186.08244208]\n",
      " [   372.48734128]\n",
      " [   210.98549258]\n",
      " [   271.79600753]\n",
      " [   382.47833993]\n",
      " [   392.70181649]\n",
      " [   289.66135894]\n",
      " [   356.01800489]\n",
      " [   312.69188102]\n",
      " [   293.56465248]\n",
      " [   306.84263753]\n",
      " [   313.47822836]\n",
      " [   194.12814654]\n",
      " [   296.23672521]\n",
      " [   247.13240229]\n",
      " [   311.92791115]\n",
      " [   316.59378109]\n",
      " [   336.0391224 ]\n",
      " [   278.20748126]\n",
      " [   284.05090496]\n",
      " [   295.05408939]\n",
      " [   340.40753569]\n",
      " [   315.20317602]\n",
      " [   289.12158814]\n",
      " [   316.68466196]\n",
      " [   377.53753227]\n",
      " [   332.28709821]\n",
      " [   296.53319823]\n",
      " [   343.76818952]\n",
      " [   325.97593176]\n",
      " [   278.66678123]\n",
      " [   306.76790451]\n",
      " [   270.07573209]\n",
      " [   352.50042586]\n",
      " [   316.21888645]\n",
      " [   314.09830606]\n",
      " [   268.04504894]\n",
      " [   297.00307218]\n",
      " [   306.06415919]\n",
      " [   279.37134624]\n",
      " [   294.90478729]\n",
      " [   288.0345054 ]\n",
      " [   270.2342147 ]\n",
      " [   330.26198894]\n",
      " [  1935.50433196]\n",
      " [ 16491.43991674]\n",
      " [  6240.38185302]\n",
      " [   120.91203859]]\n",
      "Accuracy of:  0.23293768546\n",
      "CPU times: user 344 ms, sys: 2.53 ms, total: 347 ms\n",
      "Wall time: 88 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "blr.fit(X,y)\n",
    "print(blr)\n",
    "\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[  2.45027462e+170]\n",
      " [ -1.31939092e+172]\n",
      " [  2.62226614e+171]\n",
      " [  2.69990104e+171]\n",
      " [ -8.10370053e+170]\n",
      " [ -1.25622611e+171]\n",
      " [  2.52865447e+176]\n",
      " [ -2.48917192e+175]\n",
      " [ -1.88031351e+175]\n",
      " [ -3.20587632e+175]\n",
      " [ -1.06333001e+176]\n",
      " [ -6.19847138e+174]\n",
      " [ -3.08920150e+174]\n",
      " [ -1.23578703e+174]\n",
      " [  8.72544465e+173]\n",
      " [ -2.69228095e+173]\n",
      " [ -5.04004842e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94636418e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94636418e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94636418e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]]\n",
      "Accuracy of:  0.207715133531\n",
      "CPU times: user 390 ms, sys: 6.55 ms, total: 397 ms\n",
      "Wall time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X + 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "hlr = HessianBinaryLogisticRegression(eta=0.1,iterations=20,C=0.1) # note that we need only a few iterations here\n",
    "\n",
    "hlr.fit(X,y)\n",
    "yhat = hlr.predict(X)\n",
    "print(hlr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 0.0053308 ]\n",
      " [ 0.02543407]\n",
      " [ 0.01737123]\n",
      " [ 0.01652036]\n",
      " [ 0.0119943 ]\n",
      " [ 0.01125106]\n",
      " [ 0.0158728 ]\n",
      " [ 0.01470071]\n",
      " [ 0.0185587 ]\n",
      " [ 0.02034076]\n",
      " [ 0.0127307 ]\n",
      " [ 0.01341243]\n",
      " [ 0.01534826]\n",
      " [ 0.01476222]\n",
      " [ 0.01457085]\n",
      " [ 0.01679544]\n",
      " [ 0.01546786]\n",
      " [ 0.01487498]\n",
      " [ 0.01232577]\n",
      " [ 0.01154665]\n",
      " [ 0.02472159]\n",
      " [ 0.01454181]\n",
      " [ 0.01813156]\n",
      " [ 0.0240279 ]\n",
      " [ 0.01868001]\n",
      " [ 0.01666729]\n",
      " [ 0.01669805]\n",
      " [ 0.02029976]\n",
      " [ 0.02052187]\n",
      " [ 0.01922505]\n",
      " [ 0.01087517]\n",
      " [ 0.01877569]\n",
      " [ 0.01690821]\n",
      " [ 0.01656991]\n",
      " [ 0.01338851]\n",
      " [ 0.01266578]\n",
      " [ 0.01091618]\n",
      " [ 0.02238595]\n",
      " [ 0.01650498]\n",
      " [ 0.01384812]\n",
      " [ 0.01431969]\n",
      " [ 0.0114766 ]\n",
      " [ 0.01726188]\n",
      " [ 0.01653915]\n",
      " [ 0.02029463]\n",
      " [ 0.0133663 ]\n",
      " [ 0.01126132]\n",
      " [ 0.01382762]\n",
      " [ 0.01398652]\n",
      " [ 0.01182857]\n",
      " [ 0.019413  ]\n",
      " [ 0.01248125]\n",
      " [ 0.01196867]\n",
      " [ 0.00996621]\n",
      " [ 0.01791798]\n",
      " [ 0.01666388]\n",
      " [ 0.01015415]\n",
      " [ 0.01263844]\n",
      " [ 0.0174242 ]\n",
      " [ 0.01723113]\n",
      " [ 0.01592406]\n",
      " [ 0.02409112]\n",
      " [ 0.01498263]\n",
      " [ 0.01799999]\n",
      " [ 0.01112975]\n",
      " [ 0.01049245]\n",
      " [ 0.0125308 ]\n",
      " [ 0.01411808]\n",
      " [ 0.01580104]\n",
      " [ 0.01671855]\n",
      " [ 0.01327062]\n",
      " [ 0.0138242 ]\n",
      " [ 0.01665875]\n",
      " [ 0.01584034]\n",
      " [ 0.01606416]\n",
      " [ 0.01359867]\n",
      " [ 0.01608808]\n",
      " [ 0.01564727]\n",
      " [ 0.01817769]\n",
      " [ 0.01403607]\n",
      " [ 0.02046036]\n",
      " [ 0.02134371]\n",
      " [ 0.01435557]\n",
      " [ 0.0200896 ]\n",
      " [ 0.01758139]\n",
      " [ 0.01156032]\n",
      " [ 0.01376269]\n",
      " [ 0.0175626 ]\n",
      " [ 0.01814352]\n",
      " [ 0.01969833]\n",
      " [ 0.02101395]\n",
      " [ 0.01028059]\n",
      " [ 0.02079183]\n",
      " [ 0.01173118]\n",
      " [ 0.01521328]\n",
      " [ 0.02134371]\n",
      " [ 0.02187337]\n",
      " [ 0.01600949]\n",
      " [ 0.01988628]\n",
      " [ 0.01737294]\n",
      " [ 0.01623673]\n",
      " [ 0.01715083]\n",
      " [ 0.01753697]\n",
      " [ 0.01085125]\n",
      " [ 0.01676298]\n",
      " [ 0.01388571]\n",
      " [ 0.01767536]\n",
      " [ 0.01757114]\n",
      " [ 0.01884746]\n",
      " [ 0.01579591]\n",
      " [ 0.01589672]\n",
      " [ 0.01645201]\n",
      " [ 0.01903369]\n",
      " [ 0.01779326]\n",
      " [ 0.01611713]\n",
      " [ 0.0178428 ]\n",
      " [ 0.02058509]\n",
      " [ 0.01852795]\n",
      " [ 0.01645201]\n",
      " [ 0.01930877]\n",
      " [ 0.01841347]\n",
      " [ 0.0156336 ]\n",
      " [ 0.01721575]\n",
      " [ 0.01505439]\n",
      " [ 0.01958557]\n",
      " [ 0.01787014]\n",
      " [ 0.01748058]\n",
      " [ 0.01491599]\n",
      " [ 0.0165101 ]\n",
      " [ 0.01715083]\n",
      " [ 0.01571561]\n",
      " [ 0.01658528]\n",
      " [ 0.01617522]\n",
      " [ 0.01511248]\n",
      " [ 0.01850574]\n",
      " [ 0.10817083]\n",
      " [ 0.92166979]\n",
      " [ 0.34932975]\n",
      " [ 0.00678994]]\n",
      "Accuracy of:  0.23293768546\n",
      "CPU times: user 131 ms, sys: 7.27 ms, total: 138 ms\n",
      "Wall time: 36 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += 2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "bfgslr.fit(X,y)\n",
    "yhat = bfgslr.predict(X)\n",
    "print(bfgslr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            hblr = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ -3.21706244e+00   2.34172561e-02   6.63592728e-02   3.16253335e-02\n",
      "   -2.77161794e-02   2.73993499e-02   6.71017765e-02  -3.80480362e-02\n",
      "    2.24731870e-02   1.00764495e-01  -3.41310432e-02  -7.53741169e-02\n",
      "   -8.88313083e-02   6.28952354e-02  -7.61642514e-02   2.41592043e-03\n",
      "   -1.17151137e-02  -1.26963453e-02  -3.87819335e-02  -4.35865594e-02\n",
      "   -9.21133278e-02   6.22405342e-02  -1.54863995e-01   1.28738628e-01\n",
      "    1.78465088e-02  -2.71500022e-02   2.37307712e-03   2.10742002e-02\n",
      "   -6.22611823e-02  -5.92794663e-02   7.54948006e-03   1.88119046e-03\n",
      "    3.39335146e-02   3.71593252e-02  -2.61382559e-02   1.96259661e-02\n",
      "   -3.89869688e-02  -3.28651004e-02   8.00792579e-02   7.79963818e-03\n",
      "   -6.12550020e-02  -2.34705061e-03   3.37893317e-02  -7.69752796e-03\n",
      "   -4.65539728e-02  -1.90173488e-02   8.53542415e-02  -2.59244334e-02\n",
      "    2.94037088e-02   4.73827500e-02  -9.75182329e-02  -3.31325185e-02\n",
      "    5.10387086e-02  -3.62618843e-02   2.13583660e-02   5.87171433e-02\n",
      "    4.75676716e-03  -6.08219905e-02   3.99099997e-02   4.49480337e-02\n",
      "    5.98861812e-02  -1.61232584e-01  -3.52960828e-02  -3.50107901e-02\n",
      "   -5.66142869e-02   4.01832748e-02   1.21280514e-02  -1.80622778e-02\n",
      "    3.55893515e-02  -3.63993894e-02  -4.40225136e-02   4.73707523e-02\n",
      "   -7.58888659e-02  -9.82993941e-03  -3.95029795e-03   1.22193459e-01\n",
      "   -6.60737681e-03  -1.43778040e-02  -2.95211846e-02  -8.98777398e-02\n",
      "    4.33207969e-02   2.63114999e-02  -4.28756405e-02  -6.28126169e-02\n",
      "    5.08555547e-02  -3.63213080e-02   2.75821346e-02   5.63267968e-02\n",
      "   -1.29631383e-02  -1.86578655e-02   8.44259052e-02   2.57095243e-03\n",
      "   -3.09150323e-02   2.84283893e-02   2.10397157e-03   5.94055908e-02\n",
      "   -5.54026365e-03   4.14622917e-02   1.31246378e-02  -1.00770162e-01\n",
      "    1.90651344e-02   9.62132015e-02   1.14462910e-01  -5.98754062e-02\n",
      "    1.74479088e-01  -3.96856517e-02   1.63777482e-01  -4.38124642e-02\n",
      "    7.74695787e-02  -1.33254300e-02  -3.27676454e-02   5.81379714e-02\n",
      "    1.21028074e-02   8.35137073e-02   8.72560634e-02   7.49204080e-02\n",
      "   -2.71461626e-01  -3.50776603e-02   1.83980637e-02   1.02087374e-02\n",
      "    6.92979072e-03  -2.76055146e-02   7.62622957e-02   4.38665610e-02\n",
      "   -7.45159352e-03  -1.30470406e-02   7.13942839e-03  -1.01343749e-02\n",
      "    4.19527000e-02   1.45527336e-02   1.21991429e-03   5.26786580e-02\n",
      "   -3.91723623e-03  -3.79241006e-02   8.70349912e-02   5.60303181e-03\n",
      "   -4.10376234e-04   6.27733042e-03   2.24284318e-02]\n",
      " [  6.63008741e-01  -3.90932290e-02   6.90927829e-02  -1.33389564e-01\n",
      "    4.20758541e-02  -8.46375781e-02   1.63021381e-02   4.34921768e-02\n",
      "   -6.29339074e-02  -1.44263192e-01  -7.31256722e-02  -2.41272863e-02\n",
      "    9.09762671e-05   2.45676608e-02   9.63684383e-02   3.16390312e-02\n",
      "   -1.00177212e-01  -5.97641731e-02   7.38353961e-02  -5.49671009e-02\n",
      "    1.03559210e-01  -1.53698015e-02   1.06226150e-01  -5.86114481e-02\n",
      "   -1.17957533e-03   1.64059288e-02  -1.30034894e-02   1.12484489e-01\n",
      "   -2.66348717e-02  -2.12397422e-02   2.07647184e-01  -8.24849948e-02\n",
      "   -5.57302723e-02  -6.25021502e-02   1.30108805e-01  -8.16514539e-02\n",
      "    1.26775829e-01  -4.30438954e-02  -4.38124150e-02   3.91737944e-03\n",
      "   -4.28713025e-02   5.29722287e-02  -7.00177530e-02  -1.25942415e-02\n",
      "    7.61813715e-02  -3.42987502e-02  -1.27696887e-02  -1.93282372e-02\n",
      "   -7.18239041e-02  -1.20399566e-02   8.80322421e-02   8.45719823e-02\n",
      "   -7.10896944e-02   4.08943663e-02  -3.12686674e-02   4.44574060e-03\n",
      "   -4.14287688e-02   2.98568792e-02  -1.19818421e-01  -1.81454648e-02\n",
      "    7.62095003e-02   7.72369624e-02  -5.16576968e-02   4.41797328e-02\n",
      "   -5.35280336e-02  -6.94812499e-03  -4.86654698e-02  -7.46372939e-02\n",
      "   -2.85655058e-02  -5.12588170e-02   5.77378851e-02  -6.06882973e-02\n",
      "    4.40898604e-02  -1.30542803e-02  -7.56540469e-03  -4.17831115e-02\n",
      "    4.70924777e-02  -3.39174129e-02   5.02937853e-02   1.37618894e-02\n",
      "   -6.20292502e-02   1.29931309e-02   8.39085773e-03   4.31498184e-02\n",
      "   -1.00471136e-01   2.92463133e-02  -2.47729791e-02  -4.08482959e-02\n",
      "    3.71696233e-02   4.44082081e-02  -6.14818055e-02  -3.30778775e-02\n",
      "    7.73946892e-03   4.56156395e-02   9.38181121e-04  -7.24808183e-02\n",
      "    2.49973989e-02  -4.84318603e-02  -3.88401222e-03   4.83143760e-02\n",
      "   -1.24634357e-01   3.73255482e-02  -1.30603268e-01  -3.91131927e-03\n",
      "    7.36222242e-04   2.99468523e-02   4.75170593e-03  -7.04860181e-03\n",
      "    1.67167391e-01   7.30389521e-02   5.75288964e-02   7.12994092e-02\n",
      "   -3.58430724e-02   4.09204324e-02  -7.95647643e-02  -5.10063561e-02\n",
      "   -1.46963433e-01   1.05482519e-01   4.38569827e-02  -1.25672177e-01\n",
      "    4.04675258e-03  -1.18742123e-02  -1.92435189e-02  -1.87798176e-02\n",
      "   -6.48496503e-03  -4.32604761e-03  -1.82239682e-01   2.50294038e-02\n",
      "   -1.16344875e-01   1.10862815e-01  -1.66407344e-02  -6.31334319e-02\n",
      "    6.04801925e-03  -4.55778576e-02  -1.51192698e-03   4.38825034e-03\n",
      "    8.70644373e-03  -3.24341638e-03  -1.22663117e-02]\n",
      " [  8.74998077e-01  -4.76909873e-02  -1.10082535e-01  -3.20803289e-02\n",
      "    2.54017344e-02   6.11214031e-02  -5.30144467e-02   3.83466753e-02\n",
      "    4.68918346e-03   5.94467204e-02  -3.18400973e-03   7.74416925e-02\n",
      "    3.62273029e-02  -1.73312571e-01   1.83848695e-02  -9.89942584e-02\n",
      "    9.52106107e-02  -1.45796563e-03   4.56516839e-03  -2.28686031e-02\n",
      "    1.20512305e-01  -5.67072074e-02   1.30497436e-01  -7.28008339e-02\n",
      "   -6.14282128e-02  -2.51851158e-03   1.54852517e-02  -2.52952578e-01\n",
      "    2.02989116e-01   1.55340220e-01  -9.05377698e-02   7.82893468e-02\n",
      "   -4.65653786e-02   2.45809926e-02  -8.16804751e-02   5.04158754e-02\n",
      "   -7.20095090e-03   3.86783158e-02   1.01445237e-02  -4.03113961e-02\n",
      "    8.09845390e-02  -3.16483779e-02   4.28471408e-02  -2.04019446e-02\n",
      "   -4.17396914e-02   3.50662535e-02   2.03206212e-02  -2.21619590e-02\n",
      "   -6.21303143e-02  -1.38219757e-01   8.71253164e-02   2.12197395e-02\n",
      "   -1.58330624e-02  -5.50276649e-02   3.40454940e-02  -5.75062746e-02\n",
      "   -1.13630199e-02   1.03073070e-01   2.66131773e-02  -5.76387710e-02\n",
      "   -6.13799712e-02   7.83748102e-02  -5.96023715e-03  -3.19339892e-02\n",
      "    5.74467864e-02   1.03829344e-01  -3.35146836e-02   7.14138572e-02\n",
      "   -1.05752397e-01   3.22699635e-02  -4.56103736e-04  -3.06695217e-02\n",
      "   -7.78954139e-03  -5.27860825e-02   3.07312435e-02   6.20237855e-02\n",
      "   -9.69914868e-03  -2.89584744e-02   1.82075169e-02   4.70340270e-02\n",
      "   -1.38069071e-01  -3.06977869e-02   5.86365054e-02   2.84872606e-02\n",
      "    5.84589962e-02  -8.15780063e-02  -4.36336358e-02  -9.23356774e-03\n",
      "    4.83281245e-02  -2.69144688e-02   8.26738139e-02  -3.63193290e-02\n",
      "    2.13529483e-02  -1.09310195e-01   2.26696437e-02   6.65380613e-03\n",
      "   -2.76323793e-02   2.02930576e-02  -5.96733581e-02   3.99779652e-02\n",
      "    8.26986564e-02  -4.65281357e-02  -9.48165081e-02   1.09918716e-01\n",
      "   -2.85217604e-02   8.14975860e-02  -1.09523897e-01   4.62386977e-02\n",
      "   -9.44738344e-02   2.84492031e-02  -8.37593542e-02  -1.24172982e-01\n",
      "   -2.46529482e-02  -7.60573781e-02  -2.78727800e-02   1.87354742e-02\n",
      "    1.08267721e-01  -3.64237742e-02  -1.43266058e-02   1.28273927e-01\n",
      "    6.92398975e-02   8.59692455e-02  -8.28379299e-02  -1.60909385e-02\n",
      "   -2.45589493e-02   4.78939394e-02  -3.33457640e-03   1.29617616e-02\n",
      "    2.01598895e-02  -6.53370796e-02   7.88509955e-02  -7.88597076e-02\n",
      "    1.13910466e-01  -1.09162218e-02  -4.61820034e-02  -2.14039794e-02\n",
      "   -2.46376406e-03  -4.35483170e-03   5.91144294e-02]\n",
      " [ -1.15913313e+00  -5.73836307e-02  -3.39902629e-02   7.30905193e-02\n",
      "   -3.77518541e-02  -2.05668227e-02  -4.10728057e-02  -4.83875707e-02\n",
      "    7.77730133e-02  -3.45841812e-02   1.09355570e-01  -3.48869405e-03\n",
      "    1.02119664e-01   1.41435737e-02  -1.46853974e-03   1.40915576e-01\n",
      "   -4.68906648e-02   8.58149537e-02  -2.00422642e-02   1.45908871e-01\n",
      "   -6.05581744e-02  -4.04649934e-02  -9.22357667e-02  -1.14366158e-03\n",
      "    2.19765708e-02   7.81903997e-02  -3.68868071e-02   2.76669972e-02\n",
      "   -8.60645694e-02  -1.20085307e-02  -1.20071421e-01  -7.51492636e-02\n",
      "    4.85506657e-02  -1.57637359e-02  -5.00918410e-02   1.00132624e-02\n",
      "   -4.41605646e-02   6.36718510e-02  -4.13805444e-02  -1.61689189e-02\n",
      "    2.66959150e-02  -4.88595050e-02  -1.88209293e-02   4.19367235e-02\n",
      "    4.04888189e-03   3.82753860e-02  -4.91120386e-02   5.82999090e-02\n",
      "    3.29052902e-02   4.31286133e-02  -1.30785152e-01  -1.02996602e-01\n",
      "    8.36011902e-02  -7.79811527e-04  -3.62725122e-02   3.16994908e-02\n",
      "    4.61768212e-02  -6.34297733e-03   3.33726008e-02   4.87070100e-02\n",
      "    1.85548373e-02  -8.93582819e-02   8.09724519e-02   2.81607709e-02\n",
      "    2.68676259e-02  -3.65466775e-02   2.31043228e-02   1.45384455e-02\n",
      "    6.65325676e-02   5.53763505e-02  -2.38453536e-02   3.56888291e-02\n",
      "    2.37905238e-02   2.71286495e-02  -8.68955023e-03  -6.84902186e-02\n",
      "   -6.52054707e-02   1.96802768e-02  -6.98156638e-02   5.88864179e-02\n",
      "    1.17021957e-01  -2.19385347e-02  -2.75180260e-02  -2.37429759e-02\n",
      "   -7.42698726e-02   5.55951945e-02   2.45275710e-02   2.15987998e-02\n",
      "   -6.60026748e-02   4.58845693e-03  -1.15211246e-01   3.96894162e-02\n",
      "    3.19308351e-02  -1.05527769e-02  -4.80336592e-03   5.58897760e-02\n",
      "    2.93272521e-02  -4.04333201e-02   3.89537290e-02   3.70920533e-02\n",
      "   -5.00516558e-02  -8.73025954e-02  -2.56557723e-02   6.20548612e-03\n",
      "   -7.37103218e-02  -4.92532038e-02  -8.20378946e-02   3.07129649e-02\n",
      "   -6.40468877e-02  -4.13585487e-03   1.16747481e-01   8.17896059e-02\n",
      "    5.42730028e-02   6.02594521e-02  -1.48927790e-02  -6.54530854e-03\n",
      "    1.48078205e-01   6.19114033e-03  -8.42983254e-02  -9.66587783e-02\n",
      "   -2.31078526e-03  -9.85601860e-03   5.37184009e-02  -4.69101434e-02\n",
      "    1.46721346e-02  -4.62387594e-02   9.26601978e-02  -4.78457778e-02\n",
      "    6.42791953e-02  -3.67451342e-02  -5.42579521e-02   8.04835065e-02\n",
      "   -1.07428911e-01   5.35817139e-02  -6.03264779e-02   1.62027400e-03\n",
      "    3.62706607e-03  -1.95488702e-04  -3.54482478e-02]\n",
      " [ -1.20574672e+00   1.30995109e-01  -4.79288671e-03   7.28100631e-02\n",
      "    5.64099922e-03   4.42527076e-06   2.17872707e-04   1.36195518e-02\n",
      "   -4.54846448e-02   2.49413008e-02  -1.02843854e-02   1.72625833e-02\n",
      "   -4.81819879e-02   7.84662993e-02  -5.65431762e-02  -5.20892478e-02\n",
      "    4.10946431e-02   1.51550395e-03  -3.22228498e-03  -1.90592528e-02\n",
      "   -7.88298011e-02   4.09879929e-02   1.19089309e-02   8.55752525e-03\n",
      "    4.23148077e-02  -7.47836188e-02   2.27195862e-02   1.02442214e-01\n",
      "   -3.95924683e-02  -8.71942383e-02  -1.72662999e-05   5.97910384e-02\n",
      "    3.77089063e-02   1.10402268e-04   1.76829854e-02  -3.35723412e-04\n",
      "   -4.15879720e-02  -4.30976686e-02   7.96192823e-03   5.54098615e-02\n",
      "   -7.78249207e-03   2.39381747e-02   2.51305876e-03   1.69111942e-02\n",
      "    1.10831932e-03  -2.93791770e-02  -5.28025054e-02   4.92784556e-03\n",
      "    1.08797245e-01   7.95469366e-02   6.53598615e-02   5.47022029e-03\n",
      "   -5.85187035e-02   5.56051673e-02   2.15085032e-03  -3.90247069e-02\n",
      "    5.48212319e-03  -7.71506489e-02   2.30009318e-03   3.89176630e-04\n",
      "   -9.67328229e-02   7.25446337e-02   1.16906843e-02  -8.64704828e-03\n",
      "    1.16657333e-02  -8.73914265e-02   6.35703876e-02  -6.92427155e-03\n",
      "    4.35151811e-02  -2.42834421e-03   5.34527552e-03   2.98330889e-02\n",
      "    7.61343439e-03   4.99097668e-02   8.93757542e-03  -6.27150867e-02\n",
      "    1.80171934e-02   3.03987659e-02   3.64966490e-02  -4.36449386e-02\n",
      "    2.72001842e-02   1.34951857e-02  -8.86671838e-03   1.71673051e-02\n",
      "    5.77471944e-02   2.22284207e-02   3.18746413e-02  -2.66072287e-02\n",
      "   -4.78619201e-03  -6.29583818e-04   1.34099905e-03   2.97188830e-02\n",
      "   -1.47209600e-02   3.45164086e-02  -3.31671223e-03  -6.47122991e-02\n",
      "   -9.40255429e-03   3.30658779e-02   1.61515350e-02  -1.91611212e-02\n",
      "    6.14427450e-02   9.33716202e-03   1.65655791e-01  -5.59492331e-02\n",
      "   -8.53244958e-02  -2.01921881e-02   3.58531896e-02  -3.84045352e-02\n",
      "   -5.87158947e-02  -8.83920723e-02  -3.54313712e-02  -7.60885127e-02\n",
      "   -1.42355108e-02  -1.06273366e-01   2.49097991e-02  -4.08391770e-02\n",
      "    1.81730687e-01  -3.77386711e-02   3.33476810e-02   5.52405211e-02\n",
      "   -6.80332023e-02  -3.28433608e-02  -1.27720808e-02   3.20979149e-02\n",
      "    2.44895787e-02   4.55626580e-03   8.81885335e-02   4.81262264e-03\n",
      "   -1.68540255e-02  -2.43705181e-02  -2.16565162e-02   1.95173062e-02\n",
      "   -1.32201768e-02   4.82358539e-02   2.29261918e-02   8.58582056e-03\n",
      "   -1.15743113e-02   3.35256963e-03  -5.19072464e-02]]\n",
      "Accuracy of:  0.63649851632\n",
      "CPU times: user 1.13 s, sys: 17 ms, total: 1.15 s\n",
      "Wall time: 291 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear boundaries visualization from sklearn documentation\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_decision_boundaries(lr,Xin,y,title=''):\n",
    "    Xb = copy.deepcopy(Xin)\n",
    "    lr.fit(Xb[:,:2],y) # train only on two features\n",
    "\n",
    "    h=0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "    y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb[:, 0], Xb[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Fear of Public Speaking')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5.,    3.,    2., ...,  163.,   48.,    1.],\n",
       "       [   4.,    4.,    2., ...,  163.,   58.,    2.],\n",
       "       [   5.,    5.,    2., ...,  176.,   67.,    2.],\n",
       "       ..., \n",
       "       [   4.,    3.,    1., ...,  173.,   75.,    0.],\n",
       "       [   5.,    3.,    3., ...,  173.,   58.,    1.],\n",
       "       [   5.,    5.,    4., ...,  185.,   72.,    1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAFzCAYAAAA9nXBaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3Xl4VOXdxvHvbyYhJCQQVtkjm2xKARUVcEOr4lqX11oV\nl+rrUmsX29ribrXWumvrvlKtVeuCK+5aBQR8UZRdkFWC7PuWZZ73jzMzTGICyWSSM3Nyf64rVzJn\nzsz5Ja3c8zznWcw5h4iIiARPyO8CREREpH4o5EVERAJKIS8iIhJQCnkREZGAUsiLiIgElEJeREQk\noBTyIiIiAaWQFxERCSiFvIiISEAp5EVERAJKIS/SgMysu5k9bGbfmtk2M9tgZuPN7Fdm1jQF73+i\nmU2NvvdiM7vBzMK1eP0FZjYr+vpvzOyX1ZzXwsweMbOVZrbZzD40s0FVnPdjM3vczKabWZmZLajm\n/TqY2TNmNsfMNprZOjObbGbnVHP+GQm/50oze8zMWtf09xRpLLL8LkCksTCz44AXgO3AP4EZQBNg\nOHAb0A+4pA7vPxJ4BfgQ+CWwD3AN0Ba4rAavvxh4EPgPcCdwMHCfmeU6525POM+At6LvfxuwBvgF\n8LGZDXbOfZvwtmcCpwNfAMt2cfk2QMfotZcA2cCPgafMbC/n3DUJ178UuB94D/gt0Bn4DbCvmR3g\nnCvZ3e8q0liYNqgRqX9mtifwNV6AjXDOraz0fHfgOOfc3+twjZl4HyD2d85FosduAkYD/Zxz3+zi\ntU2BpcBE59xJCcefBk4CujjnNkSPnQ48B5zqnHsleqwN8A3wlnPu7ITXtwdWOefKzex1oL9zrnst\nfqfXgMOAFs45Z2bZwApgmnNuRMJ5xwGvA5c75+6v6fuLBJ2660Uaxh+BZsAFlQMewDm3oI4B3xfo\nCzwSC/ioB/D+Oz9tN29xONAqen6i+4F84LiEY6cC38cCHsA5txqvl+KkaBDHjn/vnCuv5a+TaDGQ\nh9fjAbA3UBi9Vpxz7k1gM3BGHa4lEjjqrhdpGMcDC5xzk3d3opk1x+uu3p3tzrkt0Z8HAQ6YmniC\nc265mX0XfX5XYs9PrXR8KhCJPv9swrlfVPEeU4D/BfYCZtag/h+I9ig0w/tgcRhwHl7vwo7oKTnR\n79uqePk2dv97ijQqasmL1DMzKwA6AdNr+JJXgVW7+VoJJLb8O0S/L6/i/Zbj3e/elQ5AebRFHuec\nK8W7596x0rnVXYcaXGtXfo33+y0EngQ+A36W8Pw8vA8zwxJfZGa98cYe5JpZyzpcXyRQ1JIXqX/N\no9831fD8K4CaBFVxws+50e87qjhvO1Cwm/fKBaobsLY94f1j51Z3Hat0bm09C3yOF9jHA3vgddcD\n4JxbY2YvAOea2Ry8gYadgfui9WdHr7+uDjWIBIZCXqT+bYx+313QAuCc+zKJa8S6r3OqeK4pVXdv\nV359k2qeq/z6bbu4jqvBtarlnFuKNwAQ4Hkzexh4PzrCPvbB4uLotW4H7ohe8xngW+BkvHvzIoJC\nXqTeOec2mVkx3qCx3Yp2N1cXuIm2OediHyBiXeUd+OFUtQ7A7sYCLAfCZtYmscs+OoiuNRV7DZaz\n8/ZA5etQ6dy6ehG4EDgEb8oc0d/5ZDPrDOwJLHbOLTWzCXgj+TdW92YijY3uyYs0jDeAHmZ2QA3O\nfRkvSHf1VQzck/CaaXhd5fslvpGZdcDrzt5d70CVrwf2x/t3YlqlcwdX8R4HAlvxptKlSm60rhaV\nn3DOfeecGx8N+EJgX6IfBETEo5a8SMO4DTgLeMzMjqhinnwPvHny95HEPXnn3KzoPeqLzOxht3MB\njF/gjY5/KeFauUBXYLVzbk308IfAWuBS4O2Ea1wKbAHeTDj2InCqmZ3inHs5+p5t8KbpvRYdrFcr\nlXsQElwYrb+q0fyJ/gqEgbtre22RIFPIizQA59wCMzsTbxGZ2WaWuOLdMLyAfDJ6bjL35AH+gDcy\n/z0zew5vRbrLgEedc3MTzhsCfATcAPw5es3tZnYt8I/owLZ38LrIzwSucs6tT3j9i3grzD1pZv2B\n1XgfJkLR94wzs32AE6MPewItzOzq6OOvnHNvRH++2syG4X3AWII3Z/9UvJ6F+5xzCxLe8494tz4m\nA2V49+GPBK52zu3uw4BI4+Kc05e+9NVAX0AP4CG8QWLbgA3ABLxlaJuk4P1PxJvbvhVvIZkbgHCl\ncw4FyoFrq3j9BcCsaG3f4K0gV9V1WgCP4E3l2wR8AAyq4rxzo9eq6uuJhPOOwPuAshRvlP564BNg\nVBXveSze1Lr10WtPAE7x+39bfekrHb+0rK2IiEhAaeCdiIhIQCnkRUREAkohLyIiElAKeRERkYDy\nbQqdmbUGjgYW4Y2mFRERkZppirfi4ztu53oXP+DnPPmjgX/5eH0REZFMdxY7t4H+AT9DfhHAo+ef\nT+/27X0sQ0QkeBZ/P51tJeUArPjxZT5XI6lWvGg+D97wG4hmaXX8DPntAL3bt2dg164+liEiEjyx\nf1e/mDee/EHllG/7kc8VST3Z5e1uDbwTEQmwwb2G03HMC2xdW22PrgSYQl5EJODa5bem9/sz6fLC\n1bs/WQJFIS8iEnCdO/RlcK/hAAr6RkYhLyLSSCjoGx+FvIhII6Kgb1wU8iIijYyCvvFQyIuINEKD\new2nWU6Wgj7gFPIiIo1U764HAl6LPpz7lc/VSH1QyIuINGKDew2nXX5rOo55we9SpB4o5EVEGrnO\nHfrGu+61aE6w+LmsrYiIpIlY1z3vj6f43K+0DG5AqCUvIiJxsa57DcgLBoW8iIjEJa6Op677zKeQ\nFxGRHxjca7jWuw8AhbyIiFRJi+ZkPoW8iIhUS0Gf2RTyIiKyS1odL3NpCp2IiOxW764H8jVL6Pbl\nwwCUzVsCwNLT/+JnWbIbCnkREdmlmaFiAMJk0feYocz5aCrh3t0pj5TFQx92Bn/xuadrnn2aUMiL\niEiV3pmzmc79NmI5OfQ5fN/48cSfY2a/PZFw7+4AdJ04CZgEwMyiAgDyWp1Z/wXLDyjkRUSkgljL\nvXM/6HvM0Bq9pqrzZr89kb2XbMG5CCze2eLvxEbeLeqk4G8ACnkREQFgR+5S5u8IAzUP912p7j1m\njhvvhX9C8C8ZemD8Z3X1p45CXkSkkcsuWMi0LTmwI0x2+270HNihXq/Xf+TwCo9nvz2Rrp9N9h44\nR6yrvxMb+WzoUQr9OlDIi4g0UvFw35Lzg/vuDam6Fv+McZ964e8mxY8tGXqgQr8WFPIiIo2QN6gu\nB8tvSZ/hff0up0p7jzy4wuPZb0+sMKgvphMbGT/oDw1YWeZQyIuINCKxQXVdBuXQ5/C633dvSNW2\n+N/6pMJUvpiFgy6u75LSnkJeRKQRSPWgunSy97GH/ODYjHGfVgj+xTPyGLr3942uxa+QFxEJsNhc\n94YaVJcuErv6509bTu+OsKx4azz4F8/Iiz8fGTWqwetrKAp5EZEAig2qq81c96CKf7BJ+IDTtONy\nALYXz4MvK87hdyVrmHDArQ1aY31RyIuIBMiO3KV8/GXLtB9U57eqgn/O+NmsoC1lG5tX6OrvxEaA\njOzqV8iLiATEzFAx7AjTbURreg7c2+9yMs7OD0Q7PxjtDP5VFYK/Y8lCAN7rNSCtV+5TyIuIZLDs\ngoW88XnbKteYl7qrMvg/msrKXO9DVP/Fm+JL9saCP526+hXyIiIZamaoGLbk0GVImKz8xjOozm9V\nfZCaM342K0ubUbZtS5U78809sr8vLX6FvIhIhgnydLhMVdXYh8Sd+RJb/GXzljRY6CvkRUQyRHzr\nV/Kw/DwNqktzVX0AmzN+NuHeWRVCH7wpfZuPC1PQqX9Kl+1VyIuIpDlNhwuO6j6YmY2HJeAW71y2\nd2ZRQZ1DXyEvIpKmKmwgo+lwgVZ5Z745H02Nbsdbca3+xTPysIsGEGr6bY3eVyEvIpKGYoPqFO6N\nU7WzJOxT+Gwy2xYvr9H7KORFRNJIbAMZUNe8/FBsud6y6fOBJ3d7vkJeRCQNxAfVaa67pJBCXkTE\nR7GWuwbVSX1QyIuI+EBz3aUhKORFRBpQY936VfyhkBcRaSAzQ8XqlpcGpZAXkQrKy8u5buxYXp82\njbwmTbjhpJM4ZsAAv8tKe0vXruWSMWNYvHo1e7Vvz8Pnnkvb5s2BhEF1mg73A4VFIys8Xr94nE+V\nBJNCXkTiVm3cSP9rrmF7aSm5OXmUlq3n9AcfZP9u3fjgyiv9Li9t3fPuu9w4diwR52jZNJv3Z62l\n9+g/MfqcEzj2oB+p9V6NWMCHskNY2CjfXk5h0Uh+c9FJ3HD1JT5XFwwhvwsQkfQx4rbbKC2P8Pvz\nbuKZW99hzC1vcdTQk/h84UIe+fhjv8tLSyUlJdz06lj6tm7GF+cdzMJLRjBp1DA65zflb0+/Qd9j\nhirgqxAL+F4nd+W4pw/m+GcOZtBlfQC455FX/SwtUBTyIhL33br1jDjgOIYNGkEoFCKvaTMuPPW3\n5OcVcPs4daNW5ZqxYymNOO4c0Y/uhXkA9G2dzy2H9qGkPMKjY17zucL0ldeuKf3P7kFWTphQOMSe\nR3akw5A2WNj8Li0wFPIiEueco0PbzhWOZWdl07Zle7aWlPhUVXpbumYNAD2iAR8Te/zNt981eE2Z\nIr9THhayHxyT1FHIi0hcVijEhC8+oDxSHj+2bOUSFhXPp9cee/hYWXrakbuUoQf2AuDlb76v8NzL\n33xPyOCcnx7lR2kZYfX0dWxfuyP+uLw0wrKJK3HlzseqgkUD70QkbtTQg3j800+5/v5fc9RBJ7Jh\n8zpefv8ZQhZizIUX+l1eWpkZKoYdYY4+7jj+9uw4Rv93Lks2bmNIh0L+u3QtT3y9lD3atWKf/j39\nLjVtuQj8d/RUev2kK1lNwywY9x1bV273u6xAUciLSNzdZ57J5h07eGnqF8yc/yUAudnZ/Pviiyhq\n08bn6vwX3/oVKqwxP33iGA466hIe/HIJ//hiMVlmdNuzI5Pee9DPctPa+sXjKCwaydZV2/nqkW8A\nvK57p2l0qaSQF5EKHj3/fB465xxmLltGy/x8urRq5XdJaWFXW7+2aJHPrMnPsHnzNmZ/s5C9+3Yj\nNzfXp0ozRyzMi/Y5lQ0bt7Ju4Vs+VxQ8CnkR+YFwOMyArl39LiMtJLbedzcVLj8/l/0H92uIsgJl\n8fSX/C4hsBTyIiJViG0gY2XNsfw8rVQnGUkhLyJSSWxQnRaxkUynkBcRofpBdSKZTCEvIo3ergbV\niWQyhbyINFozQ8Xxn9U1L0GkkBeRRie+9au65SXgFPIi0mjEWu7a+lUaC4W8iARebDocKNylcVHI\ni0hgxcN9R5js9t3oObCD3yWJNCiFvIgEUmyuu0bMS2OmkBeRwMguWMgbn7f1BtUp3EUU8iISDLG5\n7l2GhOkzXPfdRUAhLyIZLLtgIZvLsjSoTqQaCnkRyUjeXPccLCeH7PYdNahOpAoKeRHJKLE15jXX\nXWT3FPIikhHi0+G0xrxIjSnkRSTtxabDaa67SO0o5EUkLWnrV5G6U8iLSNrR1q8iqaGQF5G0Edsd\nDjSoTiQVah3yZrYOcFU85YDtwHzgKefck3WsTUQaidigui6DcshqqfvuIqmSTEv+RuBq4G1gSvTY\nEOAY4H6gG/CgmWU55x5NSZUiEkjxlrsG1YnUi2RCfihwrXPuocSDZnYxcJRz7lQz+xr4FaCQF5Ef\nSJzrrnAXqT/JhPyxwFVVHP8AuDP681vArckWJSLBpUF1Ig0nmZBfC5wA3F3p+AnR5wCaAZvqUJeI\nBMzMUHH8Zw2qE2kYyYT8TXj33A9n5z35/fFa+JdEH/8Y+G/dyxORTBe776657iINr9Yh75x71Mxm\nAb8ETokengsc6pybGD3nzupeLyKNQ6zlrjXmRfyT1Dx559wEYEKKaxGRAEhcqU7hLuKvpELezEJA\nT6AdEEp8zjn3SQrqEpEME58OtyVHI+ZF0kQyi+EcCDwLFAFW6WkHhFNQl4hkkJmhYnXLi6ShZFry\nDwH/BxwHLKfq1e9EJODiW7+iDWRE0lUyId8LOM05Nz/VxYhIZtDWryKZIZmQn4x3P14hL9LIJLbe\n1TUvkv6SCfm/A3eaWXtgOlCa+KRz7utUFCYi6UNrzItkpmRC/qXo9ycSjjm8QXgaeCcSIIlrzKvl\nLpJ5kgn5bimvQkTSSrxbXmvMi2S0ZFa8W1wfhYhIetCgOpHgqFHIm9mJwDjnXGn052o5515LSWUi\n0mASV6nTdDiR4KhpS34s0B5YGf25OronL5JhvEF16pYXCaIahbxzLlTVzyKSuWL33TWoTiS4klnW\ntrNz7rtqnjvQOTep7mWJSH2JhbuRh+XnqfUuEmDJjK5/18yGO+fWJh40s2HAm0BhSioTkZSK33fX\noDqRRiOZkJ+EF/SHO+c2AZjZIcDrwA0prE2kTiKRCNePHcvzU6ZQUlpK/86dueuMM+jdoXGFWzzc\nt+TUaFDdm+9O5ILL/8aOkjJwjs4d2zH5/QfIy8troIozU9sex1NaVk7IQkRchNzcHJbP2dUQJiks\nGrnzQWylFWD94nG+1BNEydxfvxBYArxuZjlmdjheC/4659zdKa1OJEmRSIQDbrqJe997jw5NHUM7\n5DPl2/kM+8vNTF6wwO/yGkR2wUJmhoqZFp3r3veYobsN+EfHvMaoS26hrNwx+OAj6bH3QJYuW0Gn\n/v9DaWnpLl/bmBUWjaS0rJzWhe0YOvBwWuS3ZPv2koohJhUUFo2M72Paul8hewxujYUMC5n+bimU\nzDz5iJmdgRfsHwIDgNHOuX+kujiRZD05fjxzv/+eu0b044IBXQBYtmk7h/77My584gmm33yzzxXW\nr5mh4hq33BNdef1D5BW04JZn36JtR+/v9snr/+Gh66/gqFN+z0ev31tfJWesv93zNCELcdDAw/nt\nqOsIh7MoLSvhlkf/yPRvpjJt2jwGDuzld5npycFBVw+g/X5tANi4dAsf//5zyku0uWmq1Kglb2YD\nEr+APnhd812AZ4BPEp4T8d2YCRPYI68J5+/TOX6sU0FTLvpRV5auXeNjZfXrnTmbmRkqxnJyatRy\nr8xCIY449ax4wAMcfPxptOnQia9mzEt1uYHw17ufJeIi/GzkBYTDXrspO6sJpx99PuWRckac/Guf\nK0xfhT0L4gEP0LxLM7oc2h4Lm49VBUtNW/LT2Lk+fUzs8cXARWjtekkj5ZEI4ZBR+Z+KrJDF7/sF\nycxQMUBKpsNlZTep8NjMCGdl1+k9G4OsSn+jrLD+ZrsTyvphOzOUpYBPpZrek+8GdI9+71bF4+4J\n30V8d9p++1G8eQdj562IH1u3vZQnvl5K+8LgTACJ3XcHL9zrGvAuEuHDl59l47qdvR1T//seK5Yu\nYq8eXXbxysbr9J8cRshCvPLBv3DO+wQZiUR45YN/EQqFeezeP/hcYfpaO2cDa2avjz/euno7S/77\nPa48gJ/EfWKx/1M2+IXNBgNTPxk9moFdu/pSgwRXJBKh/9VXU7x+PUd3a0OH/KaMnbeCTSVlPHfp\nLzhq7739LrFO4lu/Qkqnw/3ljqe484EXyc3LZ8iRx7Fx7Wq++PR9DFg47VlatGiRkusETWygWLdO\nvejX40d8NfdzvlvhbfOhkeJVKywaiYUAjI4HtiUrN8x3E1YSKYngIk5/t92YNn0+hx1/OcC+zrkv\nqjsv6ZA3s35AV6BC315N165XyEt9215Swi+efpr3ZsygLFJOUZu23HHGGQzvldmDoGIt9/qa637v\nQ//hz7c9BRbGuQj5eTmMf/sBunbeI+XXCpLCopGELEQoFKK8vAwM1i1SUO1K7MNR7B58rAWvgN+9\negt5M+sOvALsQ8X79A7AOVeje/IKeZGa25G7lI+/bEnnfhu1gYyI1Djkk1kM515gIXBE9PsQoDVw\nJ/D7JN5PRHYhtvVrtxGt6Tkws28ziEjDSibkDwJGOOdWm1kEiDjnxpvZaOA+YFBKKxRphLILFrK5\nLIv5O7yOMW0gIyLJSCbkw8Cm6M+rgY7AXGAx0DtFdYk0Wolbv2a3LNQa8yKStGRCfgbwI7yu+snA\nlWZWgjdXvnGsFypSD7T1q4ikWjIhfzPQLPrzdcAbwKfAGuCnKapLpNHQ1q8iUl+SWbv+nYSf5wN9\nzKwVsM75NeleJEPFBtVp61cRqQ/JtOQBMLOeQA/gE+fcWjPTWoQiNRDf+hU0HU5E6lWtQ97MWgMv\nAIfjzY3vhXcv/nEzW+ec+11qSxQJjsRBdeqWF5H6lkxL/m6gFG+1u9kJx58H7gIU8iKVpHIDGRGR\nmkom5I8CjnbOfVeph34eUJSSqkQCIj6oTt3yIuKDZEK+GbC1iuOtgB11K0ckGGItdw2qExE/JRPy\nnwLnANdGHzszCwFXAh+lqjCRTJQ4qE7d8iLit2RC/krgAzPbD28HutuA/ngt+WEprE0ko8wMFcMW\nDaoTkfSRzDz5GWa2F/BLvOVt84GXgfudc8tTXJ9I2ot3zaPWu4ikl6TmyTvnNgB/SXEtIhnFmw6n\nrV9FJH0lFfJm1hK4AIj1Sc4CnnTOrU1VYSLpStPhRCRTJLMYziHA68AG4P+ih38FXGdmJzjnPklh\nfSJpQ4PqRCTTJNOSvx9v4ZtLnXPlAGYWBh6IPrdP6soT8V+sW16D6kQk0yQT8j2B02IBD+CcKzez\nu/Cm1okExsxQsbrlRSRjJRPyX+Ddi59b6Xhf4Ks6VyTis9gqdaANZEQksyUT8vcB90Z3oZsUPXYg\ncBnwJzMbEDvROfd13UsUaTja+lVEgiSZkP939Ptt1TznAIt+DydZl0iDSmy9q2teRIIimZDvlvIq\nRHwSn+tOHtntO6r1LiKBksyKd4vroxCRhqZBdSISdDUO+ehStoXOuSkJx44ArsHbmW6sc+6W1Jco\nkjoVBtVpOpyIBFxtWvJ/A6YDUwDMrBveojifAl8Do81sq3PunpRXKZICGlQnIo1NbUJ+PyoOtjsL\n+MY5dzSAmX0NXA4o5CVtJK5SB+qaF5HGpTYh3wb4LuHx4Xgt+ZiPgTtTUJNISniD6rRKnYg0XrUJ\n+bVAB2CpmYXwWvZ3JTzfBG/qnIivtIGMiIinNiH/MXCtmf0C+B8gFD0W0w9YlKrCRGorNqhOq9SJ\niHhqE/JXA+8Bi4Fy4FfOuS0Jz48CPkxhbSI1Eh8xr0F1IiIV1DjknXOLzKwv0B9Y5ZwrrnTK9VS8\nZy9Sr+KD6naEdd9dRKQKtVoMxzlXRjWb0DjntDmNNIh4y11bv4qI7FIyy9qK+CY2113hLiKyewp5\nSXvZBQvZXJalQXUiIrWkkJe0NjNU7HXL5+TQ9xiFu4hIbSjkJS0lrlSnue4iIsmpdcib2fnAZufc\nfyod/x8gzzk3JlXFSeMT2/pVg+pEROoumZb8aODCKo6vBB4BFPKSFG39KiKSWsmEfFdgSRXHF0ef\nE6mxClu/alCdiEhKJRPyK4EB/HAJ2x8Ba+pakDQe2vpVRKR+JRPy/wbuM7NNwCfRY4cC9wLPpaow\nCa74fXfUNS8iUp+SCflrgT2BD4Cy6LEQ8E/gqtSUJUEUC/cuQ1rSZ7jCXUSkvtU65J1zJcBPzexa\nvC76bcB059ziVBcnOz07aRL3vv8+c4uX06l1Ky4+5BAuO+IIwqGQ36XtVizcG3pQ3ZQvZnPiGX+i\ntKSUiHOEQsaJxw7nyfv1WXRXuu59Khu3bIOIw0KGizguOf8kbr3hEr9LS1t3/ON5br79KcIG5Y74\n9zefvYZhw4b5XV7aKiwaCUDIwDnvmAPWLx7nX1EBk3RCOOe+cc79xzn3hgK+fj3w4YdcMmYMzXr2\n59w/3UT3oSO4buxY/vD8836Xtks7cpdGR8xvJLt9twYN+NWr1zPy1N8RipTzy8FF/PXQPvRrnc/Y\nNz7l/MtuabA6Ms2eA05j4+atNO/SjH1+3oteJ3clKzfMw2Ne4413JvhdXtq6+fanADi4qDmX7LcH\n+3fKB+C4M2/2sar0Vlg0krBBQZMwp/dvzTkD29I+P5uQ7Qx/qbsateTN7C7gWufclujP1XLOXZGS\nygSAbSUl/PWttzji1LO54Oq/egf/Bzr37M0Td/+F3xx1FF1bt/a3yCr4PajulHOvpTziePW0/RjS\nsRCACwd04fDnJvHaW+MbvJ5MsX7jFgo6NeOw2/cjnO21AToNbcdHv/ucsy+6WS2sKhQWjcSAMwe0\n4fT+bQAY2aslj32xgje/WUfLopGs09+tSg64/agiOhQ0AeCYnoVc9uZC1m4r2/ULpcZq2pIfBGQn\n/Fzd18BUF9jYzSouZsOWLRx60ukVjh920k+JuAiTvv3Wp8p+KLtgITNDxV7A43XN+zVqfu43i+lW\nmBcPeIDscIiz+nUiEnG+1JQJzKDLYXvEAx6gsHsBzYvyfawq/TngyO6FFY4d2b0FEec9J1Xbu11e\nPOAB8rLDHFLUnKz0vwuZMWrUknfOHV7Vz1L/mufmArBu1YoKx9et/B6AgujzfvPuu6fPKnXZ2Vms\n3VZCSXmEJuGd/2IUb95OyMzHytKcGdvXlVQ4FCmPsH3dDp8Kyhxrt5XRKjerwmPZtTXbynDOYQn/\nTa7ZWhq/Py91p89Laa7XHnswaM89efH+21jzvddC3rxxPU/fcSNtmjdnRJ8+vtYXa7l37reRvscM\nTYuAB/jfc09gw44ybpwwj5LyCABTitfz2NdLyW6SvZtXN16u3LHo3WJWz1gHQKQswux/L6RkYymt\nW7bwubr0dNyRQwgZPPbFCjbu8IJ9zdZSxkxbRci8wXdStWUbS3h1zlrKIw7nHJ8v28yEpZsoV8in\njLkafGQys5dr+obOuVNqdGGzwcDUT0aPZmBXLZS3K7OLiznhvvtYvWkzXbr35PuliwgDz11yCYf5\nFPKxlerSeZW6vfb9GStXr6d5ThatmzZh4YathEPGuJfuZMjg9Pgwkm4efmIsf7rpEVzE0WyPXEq2\nlFK62Qsu3Y+vXmHRSEIGBnQoaELxJq83JOL0d6tO4uC65jlhcsLGqq1lhIAI+rvtzrTp8zns+MsB\n9nXOfVFXgKSKAAAUWklEQVTdeTUN+SdremHn3Pk1OU8hXzsbtm3jhSlTmF1cTNfWrfnZAQewR4uG\nb1nF7rcDGbFS3c23P8VDT75GaWkZvfcq4uUxN9GmTeHuX9iIzZi1kEOOuyw+dqHDHq2YPeVfPleV\n/qoaEa6g2j393ZKT0pCvDwr5zKKtX0VE0kdNQz7p/eTNrB3QO/pwrnNuZbLvJekrvoGMtn4VEck4\nyewn3xy4HzgDCEcPl5vZ88BlzrkNKaxPfOT3XHcREambZFryj+LNiT8e+Cx67CC8DWoexgt/yWCZ\nMKhORER2L5mQPx442jmXuGzYO2b2v8DbqSlL/BAfVKfWu4hIICQT8muAqrrkNwDr6laO+EGD6kRE\ngimZxXBuBu4ys/axA9GfbwduSlVhUv/embOZmaFipkUH1SngRUSCpaYb1HxJxSWYewFLzGxJ9HFX\nYAfQFu++vKQ5b5U6tdxFRIKspt31Y+u1CmkQid3yGlQnIhJ8Nd2g5sb6LkTq18xQMWzJ0YA6EZFG\nJOnFcCQzxBezQV3zIiKNTTKL4UTYxRbJzrlwdc9Jw/G2ft2IkUd2+45qvYuINELJtORPrvQ4G29x\nnHOB6+tckdRJ7L67BtWJiEitQ94592oVh180s5nAT4HH61yV1JrWmBcRkcpSeU9+EvBICt9PaiA+\nYl6r1ImISCUpCXkzywV+BSxLxfvJ7sXDfUuOpsOJiEiVkhl4t46KA+8MKAC2AmenqC7ZBW9Qnbrl\nRURk15Jpyf+WiiEfAVYBk51zWru+HsU2kNGgOhERqYlkBt49VQ91yC5o61cREUlGjTeoMbNmZvaA\nmS0zs1Vm9pyZta3P4hq72AYy86OD6hTwIiJSG7Vpyd8EnAM8g7cZzc/wRtNXnjcvdaS57iIikgq1\nCfmTgfOdc/8BMLN/ApPMLMs5V1Yv1TUyO3KX8vGXLTWoTkREUqI2Id8ZmBB74JybamalQEdgSbWv\nkhqZGSqGHWG6jWhNz4F7+12OiIgEQG1CPgSUVjpWBmit+iRlFyxkc1mWBtWJiEi9qE3IG/CBmSV2\nzecBr5tZSeyAc25wqooLstjWr5bfkuyWhVqpTkREUq42IV/VnvJVrWMvu6CtX0VEpKHUOOSdc1WF\nvNRQ4tavlp+nQXUiIlLvUrlBjVRjZqhY0+FERKTBKeTrSXwDGdCgOhER8YVCvh7EBtVp61cREfGT\nQj6FYhvIgLrmRUTEfzUKeTNbC+zlnFttZk8Av3bObarf0jJHfFCduuVFRCSN1LQl3wRoDqwGzgX+\nCDT6kNfWryIiks5qGvKfAWPNbCreojj3mdm2qk50zv08VcWlK811FxGRTFDTkD8b+C3QA3BAC6Bp\nfRWVzmJrzGtQnYiIpLsahbxzbgXwJwAzWwiMcs6tqc/C0knidDhQ611ERDJDrUfXO+e61Uch6cob\nVKetX0VEJPMkNYXOzA4Ffg/EUm8WcLtz7tNUFeY3DaoTEZFMV+uQN7OzgSeBl4H7ooeH4e1Qd55z\n7tkU1tfgYoPqNB1OREQyXTIt+auBK51zdyccu8/MrgCuBTIy5GNz3TWoTkREgiKZkO8OvF7F8deA\nW+pWTsOLDapTt7yIiARNMiG/FDgCmF/p+JHR5zJCfK77Fg2qExGRYEom5O/E654fCEyMHhsGnAf8\nOkV11SvNdRcRkcYgVNsXOOceBM4A9gHuiX7tDfzUOfdwastLneyChezIXcrMUDGWk0PfY4ZmXMCv\nXLWOiVNmsOS7FX6XIgG3eu0GnnjmTca++SmRSMTvcjLGpVfcQd8hZ/OH6x/wu5SMMXHiZxQWjaSw\naCS/veru3b9AasWcc/5c2GwwMPWT0aMZ2LVrvV4rNh3OcnLIatkx48J9+/YS/nDt/fz7pfcpK/f+\nwT16xP48cOfvaN2qhc/VSdCMuvhmxr07kbKI929DXk42d//t1/z05CN8rix9PfT4K4y+6RFi/5xa\n9Pgzj17HcUcd5Ftd6a6waCThUJjySDkAoVCYSKSc9YvH+VxZ+ps2fT6HHX85wL7OuS+qO6/WLflM\nkl2wMB7wfY8ZSp/D9824gAf44/UP8J9XPuDPw3oxedQwHjxqb6ZO/ppzLr7J79IkYK68/gFef3sC\n5/TvzCdnHsRLP9mXXs2bctkVdzJ3/hK/y0tbV9/0KPnZYf5+ZH8mjxrGbYf1IScc4uyL/ux3aWmr\nsGgkZiH22rM/f/7lfdx2xWMMH3RE/DlJjUDuJx/f+rWsOZafl9GD6tas3cCzL77PdQf14LLBewLQ\np3U+LXKyOfP1L5k2fR4D9+nlb5ESGM8+/y5HFLXm7iP6xY8NbNecPo99zLU3P8YLTym0KvvxT35L\nuXPce0R/Tu3tNSL6tM7HzPj9R7P53TX/4M6bf+lzlekpNyeX6y65i6Y5uQD8ZtR1LFu5mIXfzfO5\nsuAIXEt+ZqiYzv02xlvumRzwAAsXL6e0rJzDurSucPzwrt7jufMyZkKDZIBt20sYUdSmwrE2eU3Y\nu00B8xcu86mq9DZj9gJg53+TMbH/Zt94e+IPXiOevt0HxAMewMwY2HsIoVDgosk3gfhLxrrlEwfV\nBUXnju0IhYzPv99Q4fiU5esB6NqlnR9lSUDlNMliSvH6Csc27ihjztrNdOrQpppXNW5dOnv/DVb+\nb/Tz772/44H792/wmjLFvMWzKS0rrXBszsLpGuyZQrUKeTPLNrNvzSxtmsczQ8VMi851j7Xeg6T9\nHq04aeRwbpw4nxfnLmfd9lLeX7Sayz+cxYC+3ThwP/0DIqlz3DHDeXX+Cv42aT4rtuxgxqpNjHpz\nGjvKI9ww+ud+l5eWPnnz74RDxuXvzeCdhatYv72UsfO+50//nUPIYMyDV/tdYtratGUDd465ju9W\nLGb1+pU8NfYfzPx2GhGnkE+VWt2Td86Vmlla7CMfX4aW4K9Ud8+tv+bCX/6VC8ZNjR8b2L8HTz96\nHWa2i1eK1M7D9/yeRUuW89dJc7hl0rcAZIdDXHvl+ez7o94+V5eemjZtynlnHctTz7zF6a/uHOQc\nNuPGqy7wsbL0tn7xOAqLRvL59PFM/voTgPi/Zxpdnzq1nkJnZlcBewEXOufKkr5wklPo4oPqMnQ6\nXF3MmruIufOW0KVTO/Yd2FsBL/VmxqwFPPfS+7RoUcClPz+J/Pw8v0tKe9u3b2fY0Zfx3fJV9OlV\nxH/f/LvfJWWEsy+6gTfemQxAm1YFzP/yBZ8rygw1nUKXTMi/gres7WZgOrAl8Xnn3Ck1fJ9ahXxj\narmLiIjsSk1DPpkpdOuBl5ItrLZia8x37oeWoRUREamFWoe8c+78+iiksvgGMlpjXkREJClpuRhO\nbAMZ7Q4nIiKSvKRC3sxOA04HugJNEp9zzg1O5j2zCxbyxudtvUF1CncREZE6q/ViOGb2K+BJYAUw\nCJgCrAG6A0nNe4jNde8yJOzNdVfAi4iI1FkyLflfABc55/5tZucBtznnFpjZn4FWtX2zBaFV9KaD\nRsyLiIikWDLL2nYFYosxbwMKoj8/Dfystm+W1bqTAl5ERKQeJBPy37Ozxb4EODD6czd2bqNcY533\n0nrYIiIi9SGZkP8QODH685PA3Wb2HvA88EqqChMREZG6Seae/EVEPxw45+43szXAUOA14OEU1iYi\nIiJ1kMxiOBEgkvD4OeC5VBYlIiIidZfUfvJmdrCZPWNmn5lZp+ixUWY2PLXliYiISLKSmSd/KvAO\n3sj6QUBO9KkWwFWpK01ERETqIpmW/DXAJc65/wVKE45PAJJa7U5ERERSL5mQ7w18UsXxDUBh3coR\nERGRVEl2nnzPKo4PBxbUrRwRERFJlWRC/lHgXjM7AHBARzM7C7gDeDCVxYmIiEjykpknfyveh4MP\ngDy8rvsdwB3Oub+nsDYRERGpg2TmyTvgL2Z2O163fT4wyzm3OdXFiYiISPJqHPJm1h1YGA15nHMl\nwKz6KkxERETqpjb35OcBbWMPzOx5M9sj9SWJiIhIKtQm5CvvMHcs0CyFtYiIiEgKJbWsrYiIiKS/\n2oS8i35VPiYiIiJpqDaj6w14ysx2RB83BR4ysy2JJznnTklVcSIiIpK82oT8mEqPn0llISIiIpJa\nNQ5559z59VmIiIiIpJYG3omIiASUQl5ERCSgFPIiIiIBpZAXEREJKIW8iIhIQCnkRUREAkohLyIi\nElAKeRERkYBSyIuIiASUQl5ERCSgFPIiIiIBpZAXEREJKIW8iIhIQCnkRUREAkohLyIiElA13k9e\n/FVaWsbbH0xmzjdL6NK5HSeOHEZeblO/y5IAcs7xf1/O4dPPvqZZXlNOPHY4HfZo7XdZElAHjLiI\nud8uBaAgP4+lM1/yuaJgUchngKXLVnLS2aNZsKCY3BY5bN+4g6tvfoSXxtzMwH16+V2eBEhJSSk/\n/+WtvPHORJrl5VNSsoOrb3qUu265nHPOONrv8iRgCotGApCVnU0onMWmzVspLBrJrM8ep2PHjj5X\nFwzqrs8Al/zuDlZuXs/hd+zPMU8N48cPHIRrZZx18Z8pKyv3uzwJkHsfepG335/CFefeyD//Mo4n\nb36Dw4Ycy29G38fceUv8Lk8CpF2vEwA4btTFPPLxDB7/ZBYXXvM3wOh30AX+FhcgCvk0t2jJ90z4\nbDp9zt6Twh4FADRrn8uAi3qxbNlqPpk4zecKJUiefv5dDhsykoMHH0koFKJZbj4XnXYFBc0K+PdL\n7/tdngRISUkZbTp05me/voqmuXmEs7IYccqZ7HvojwmF1cmcKgr5NLdm7QYA8jvmVTie38l7vHrN\nhgavSYJrzdoNdGzbpcKx7KwmtGvVgVWr1/tUlQRVx249CIVClY71xMx8qih4FPJpbq+eXcjNzWHZ\nhJUVjsceDxqwlx9lSUANGrAXk776iPLIzttAy1d9x7dLv2Hfgb19rEyCaNbnn7Fhzar447LSEia/\n/ybl5WU+VhUs6hNJcwX5eVx+0ancft+zlG0tp92gVqybt5EFr3/HySccQq8enf0uUQLkD786g5PP\nvpqbHrqCIw88gQ2b1/HqR8/SuWM7Tj95hN/lSYAcfdh+vPfpl1x/3k844dxLyclrxrvPP8WqZUvI\nz83xu7zAMOecPxc2GwxM/fiNvzNwn56+1JApIpEIf3/kJe579CXWrN5As2ZNOe9nx3LdleeSk9PE\n7/IkYN7+YDI33voUs79ZRCgU4tgfH8itN1xC545t/S5NAqb/AWdTvGIdzkUACIXDGI41C970ubL0\nN236fA47/nKAfZ1zX1R3nkI+g5SVlbN23UYKW+TTpEm23+VIgDnnWLtuIzk5Tchvlut3ORJw193y\nGKtWr+fBu37vdykZo6Yhr+76DJKVFaZd25Z+lyGNgJnRulULv8uQRuLPV13odwmBpYF3IiIiAaWQ\nFxERCSiFvIiISEAp5EVERAJKIS8iIhJQCnkREZGAUsiLiIgElEJeREQkoBTyIiIiAaWQFxERCSiF\nvIiISEAp5EVERAJKIS8iIhJQCnkREZGAUsiLiIgElEJeREQkoBTyIiIiAaWQFxERCSiFvIiISEAp\n5EVERAJKIS8iIhJQCnkREZGAUsiLiIgElEJeREQkoBTyIiIiAaWQFxERCSiFvIiISEAp5EVERAJK\nIS8iIhJQCnkREZGAUsiLiIgElEJeREQkoBTyIiIiAaWQFxERCSiFvIiISEAp5EVERAJKIS8iIhJQ\nCnkREZGAUsiLiIgElEJeREQkoBTyIiIiAaWQFxERCSiFvIiISEAp5EVERAJKIS8iIhJQWT5euynA\nN/OX+liCiIhI5knIzqa7Os+cc/VfTVUXNjsT+JcvFxcREQmGs5xzz1b3pJ8h3xo4GlgEbPelCBER\nkczUFNgTeMc5t6a6k3wLeREREalfGngnIiISUAp5ERGRgFLIi4iIBJRCXkREJKAU8iIiIgGlkBcR\nEQkohbxIBjKzJ80sYmbl0e+xn7un8P1fTsV7iYh//FzWVkTqZhxwHmAJx1b5U0rVzCwEOKcFOUR8\noZa8SOba4Zxb5ZxbmfDlzDPazBaY2VYz+9LMTo29yMxCZvZYwvNzzOxXCc9fD5wLnJTQQ3CImR0a\nfdw84dwfRY91jT4+18zWmdkJZjYTbzXLLtHnLjSzWWa2Lfr90ob6Q4k0VmrJiwTPVcCZwEXAfOAQ\n4GkzW+mc+xTvw/1S4FRgLTAUeMTMip1zLwJ3AH2BAnb2FKwFhgFVtcgrH8sDrgQuANYAK83sLOAG\n4DJgGjAIeNTMNjvnnk7Nry0ilSnkRTLXCWa2KeHxW8AoYDRwhHNucvT4IjM7GLgY+NQ5VwbcmPC6\nxWY2FDgdeNE5t8XMtgFNnHPx7n+zxLsCu5QFXOqcm5Hw2huA3znnXk24Zn/gEkAhL1JPFPIimetD\nvJCMpe8WoCdeS/o9q5jK2cCXsQdmdhlwPtAVyAWaJD5fRyWVAj4P6AE8bmaPJZwXBtan6JoiUgWF\nvEjm2uKcW5h4wMyKoj8eCxRXOn9H9JwzgNuB3wKTgE143etDdnO9SOwyCceyqzhvW6XH+dHvFwJT\nKj1XvptrikgdKORFgmUWXpgXOefGV3POUGCCc+7h2AEz61HpnBK8lnaiVXgB3wHYED02aHcFOedW\nmlkx0MM599zufwURSRWFvEiAOOc2m9kdwN1mFgbGAy3wBs1tiA5ymweMMrOjgIV49/H3BxYkvNUi\n4Cgz2wtv8NwGvEF8S4EbzOwaoDdwRQ1Lux6418w2Am8DOcB+QKFz7p46/MoisguaQicSMM65a4Gb\ngD/htezH4XXfx7r2HwZeBp7D665vBdxf6W0eBeYC/wesBIZGB+ydAfQBvgL+AFxdw5oex+uuPx/4\nGvgYb5rewl28TETqyLRGhYiISDCpJS8iIhJQCnkREZGAUsiLiIgElEJeREQkoBTyIiIiAaWQFxER\nCSiFvIiISEAp5EVERAJKIS8iIhJQCnkREZGAUsiLiIgE1P8Dgtui6UMtfPoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c845da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets as wd\n",
    "\n",
    "cost_vals = np.logspace(-3,-2,15)\n",
    "def lr_explor(cost_idx):\n",
    "    C = cost_vals[cost_idx]\n",
    "    lr_clf = MultiClassLogisticRegression(eta=0.1,\n",
    "                                           iterations=2500,\n",
    "                                           C=C) # get object\n",
    "    \n",
    "    plot_decision_boundaries(lr_clf,X,y,title=\"C=%.5f\"%(C))\n",
    "\n",
    "wd.interact(lr_explor,cost_idx=(0,15,1),__manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.685459940653\n",
      "CPU times: user 113 ms, sys: 2.4 ms, total: 116 ms\n",
      "Wall time: 114 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression() # all params default\n",
    "lr_sk.fit(X,y)\n",
    "# print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "# yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "99867e5ec9ad415b835215ca6e5834c1": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
