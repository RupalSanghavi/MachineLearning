{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "df = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change NaN number values to the mean\n",
    "df_imputed = df.fillna(df.median())\n",
    "# get categorical features\n",
    "object_features = list(df.select_dtypes(include=['object']).columns)\n",
    "# one hot encode categorical features\n",
    "one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in object_features], axis=1)\n",
    "# drop object features from imputed dataframe\n",
    "df_imputed_dropped = df_imputed.drop(object_features, 1)\n",
    "frames = [df_imputed_dropped, one_hot_df]\n",
    "# concatenate both frames by columns\n",
    "df_fixed = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Fear of public speaking' in df_fixed:\n",
    "    y = df_fixed['Fear of public speaking'].values # get the labels we want\n",
    "    del df_fixed['Fear of public speaking'] # get rid of the class label\n",
    "    X = df_fixed.values # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53 µs, sys: 1 µs, total: 54 µs\n",
      "Wall time: 59.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "# blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "# blr.fit(X,y)\n",
    "# print(blr)\n",
    "\n",
    "# yhat = blr.predict(X)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 µs, sys: 1 µs, total: 57 µs\n",
      "Wall time: 61 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X + 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "# hlr = HessianBinaryLogisticRegression(eta=0.1,iterations=20,C=0.1) # note that we need only a few iterations here\n",
    "\n",
    "# hlr.fit(X,y)\n",
    "# yhat = hlr.predict(X)\n",
    "# print(hlr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61 µs, sys: 1 µs, total: 62 µs\n",
      "Wall time: 66 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += 2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "# bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "# bfgslr.fit(X,y)\n",
    "# yhat = bfgslr.predict(X)\n",
    "# print(bfgslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            hblr = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.30198019802\n",
      "confusion matrix\n",
      " [[17 12  8  1  1]\n",
      " [10 11 12  5  0]\n",
      " [13 14 21 11  2]\n",
      " [ 6 10 20  9  0]\n",
      " [ 3  2  6  5  3]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.331683168317\n",
      "confusion matrix\n",
      " [[15  7  7  4  0]\n",
      " [13 16 23  1  2]\n",
      " [10 12 23  7  6]\n",
      " [ 6  5 16 12  3]\n",
      " [ 0  2  6  5  1]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.311881188119\n",
      "confusion matrix\n",
      " [[16 10 10  2  2]\n",
      " [14 13 17  4  0]\n",
      " [ 4 14 19  9  3]\n",
      " [ 7  7 14 14  3]\n",
      " [ 1  1  5 12  1]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = MultiClassLogisticRegression(eta=0.1,iterations=10, C=0.0001) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ -4.07580269e+00   7.41936680e-03   3.64642207e-02  -4.18996660e-02\n",
      "   -5.98986371e-03   8.65249224e-03   2.72006112e-02   2.67448283e-02\n",
      "    3.02763619e-02   7.24095007e-02  -1.20726409e-02  -5.41489422e-02\n",
      "   -5.32846418e-02   5.12798305e-02  -3.08369934e-02  -2.95636634e-02\n",
      "    1.88984440e-02  -2.30949848e-02  -1.10592003e-02  -6.06537601e-02\n",
      "   -4.91874784e-02   5.36080999e-02  -9.48800718e-02   1.34150474e-01\n",
      "    2.62650334e-02  -3.00871745e-03   7.38844160e-03   1.50954727e-02\n",
      "   -5.01910002e-02  -5.22515876e-02  -3.03740334e-02  -4.06637007e-03\n",
      "    1.43369979e-02   4.97225327e-02  -5.76903034e-02  -2.43643550e-02\n",
      "   -1.10798499e-02   4.67696777e-03   3.95580712e-02  -1.91392363e-02\n",
      "   -3.66505202e-02   2.52332052e-02   2.29970680e-02  -1.18992201e-02\n",
      "   -4.56351971e-02  -4.91694647e-02   9.59628870e-02   8.64893169e-03\n",
      "    2.47348575e-02   2.21296766e-02  -5.56620561e-02   1.10754034e-02\n",
      "    2.28604849e-02  -1.08104167e-02   2.96900567e-03  -8.55055320e-03\n",
      "   -2.21049840e-02  -6.42094509e-02   8.33620159e-02   1.89467850e-02\n",
      "    5.38366481e-02  -1.21466467e-01  -2.32375263e-02  -4.63797586e-02\n",
      "   -5.10414730e-02   2.02975368e-02  -2.38297408e-02  -3.04693493e-02\n",
      "    4.69547202e-02  -6.19779330e-02  -3.49675720e-02   4.69798846e-02\n",
      "   -8.36056937e-02  -1.47345634e-02   5.48970936e-02   8.70030347e-02\n",
      "    1.01772965e-03   7.57723609e-03   6.07856247e-03  -2.21716546e-02\n",
      "    3.88455615e-02   5.53569990e-02  -4.72469053e-02  -3.21490816e-02\n",
      "    1.72962519e-02  -5.15746574e-02   2.28739575e-02   8.95516387e-03\n",
      "    2.29090264e-03   9.22569422e-03   7.06024000e-02   3.07539532e-02\n",
      "   -6.27201557e-02  -3.26895584e-02   4.09849317e-04   3.91309292e-02\n",
      "   -1.11735798e-02  -2.83595929e-02   1.64667084e-02  -5.68638010e-02\n",
      "   -2.20902540e-02   5.98777629e-02   1.06577614e-01  -5.41166313e-02\n",
      "    1.27678094e-01  -4.61310144e-02   1.32968233e-01   1.02402310e-02\n",
      "    2.44849650e-02   2.64964311e-02  -3.72081500e-04   2.63046437e-02\n",
      "    4.54506157e-02   8.77657441e-02   4.67471151e-02   4.43811320e-02\n",
      "   -2.61717341e-01  -4.44428820e-03   6.10873588e-03  -1.01689350e-02\n",
      "    8.94795045e-03  -4.97968381e-02   5.38724386e-02   3.23592692e-02\n",
      "   -1.85525950e-02   1.65326498e-02   5.39673374e-02  -4.90030156e-02\n",
      "    2.73224989e-02  -3.57003929e-02   1.87666992e-02   9.85994313e-03\n",
      "   -1.41847210e-03  -3.47721096e-02   2.43582560e-02   2.71094923e-02\n",
      "   -2.07180873e-04  -2.47023754e-03   3.69101326e-02   4.41668355e-01\n",
      "    2.31607475e-01   2.74987125e-01   1.65516860e-01   5.52957007e-01\n",
      "    3.97715775e-01   5.13612421e-01   9.79951482e-01   8.83335317e-01\n",
      "    9.52114423e-01   2.65367810e-01   4.19531121e-01   1.34088159e-01\n",
      "    1.82347747e-01  -2.90021072e+00  -2.82304922e+00  -3.13926900e+00\n",
      "   -3.58847598e+00  -4.31512145e-01  -1.23632020e-01   8.84932709e-01\n",
      "    8.25577703e-01   3.85758005e-01   8.46101623e-01   4.97827971e-01\n",
      "    2.67375251e-01   6.99836215e-01   4.02549147e-01   8.33724319e-01\n",
      "    9.20253840e-01   4.28272331e-01   6.46543019e-01   5.67643223e-01\n",
      "    5.16659802e-01]\n",
      " [ -2.54937983e+00   1.32815246e-02   6.68616068e-02  -2.43141103e-02\n",
      "    2.55797205e-02  -4.92679993e-03   2.16639442e-02  -4.76120981e-02\n",
      "   -4.19467935e-02  -1.03882108e-01  -3.46779614e-02  -2.92608539e-04\n",
      "    6.06261858e-04   1.71463285e-02   2.10733732e-02   3.81530241e-02\n",
      "   -1.12527540e-01   5.32570387e-02   1.72059055e-02  -2.01869699e-02\n",
      "   -8.59553016e-03  -3.93304442e-02   6.88055674e-02  -6.72842977e-02\n",
      "   -3.68355257e-02   7.22768377e-03   3.48028659e-03   9.16742098e-02\n",
      "    2.94884801e-02  -2.96098425e-02   1.43382000e-01  -6.27803342e-02\n",
      "   -9.07776581e-02  -4.18655593e-02   1.69870975e-01  -2.40325418e-02\n",
      "    5.03812395e-02  -4.16790896e-02  -3.81167906e-02   1.05869840e-02\n",
      "   -3.07771704e-03  -2.49066931e-02  -9.60983493e-02  -5.69773186e-02\n",
      "    5.73180770e-02   3.71573836e-02  -2.04180469e-02  -2.42551085e-02\n",
      "    5.77506705e-03  -2.00771967e-02   7.57208919e-02   1.13562761e-02\n",
      "   -1.98652064e-02   7.02612997e-03  -2.83266150e-02   7.75332755e-03\n",
      "    1.61742129e-03  -1.08699181e-02  -8.36443303e-02  -2.50847330e-03\n",
      "    7.43106571e-02   4.58421980e-02  -5.14987234e-02   1.93153441e-02\n",
      "   -1.49688475e-02  -7.32400063e-03  -5.83380606e-02  -3.76301229e-02\n",
      "   -3.89777889e-02  -9.87117928e-03  -5.59985570e-03  -4.21148872e-02\n",
      "   -1.75903074e-02  -5.20296186e-02   1.67705325e-02  -4.83387756e-02\n",
      "    2.76433996e-02   3.94799633e-02  -2.00094246e-02   1.15008082e-02\n",
      "    1.37981467e-02  -5.63469432e-02   6.41585686e-02   5.67691243e-02\n",
      "   -5.51794416e-02  -9.86393312e-03  -1.02126204e-02  -1.22764033e-02\n",
      "    1.08323685e-02   4.32823544e-03  -2.17031234e-02  -5.65809804e-02\n",
      "    5.06657304e-03   4.48947938e-02   5.71624536e-02   3.30569332e-02\n",
      "   -1.93699741e-02   7.14910233e-03   1.29115035e-02   1.18298540e-03\n",
      "   -6.16715212e-02   3.71660121e-02  -1.51839235e-01  -1.21886951e-02\n",
      "    9.05680315e-03  -4.93699885e-02   5.54561101e-03  -8.14585692e-02\n",
      "    1.22292125e-01  -1.21084131e-02   5.47519959e-02   4.97225789e-02\n",
      "   -9.02362512e-02  -1.82640220e-03  -3.76843039e-02  -3.47455775e-02\n",
      "   -1.56710199e-01   3.55740884e-02   1.56227759e-02  -1.20209419e-01\n",
      "   -1.78214657e-02  -1.68765764e-02   3.70402773e-02   5.53464794e-03\n",
      "   -3.72035224e-02   9.04044191e-03  -7.04250737e-02   4.15781020e-02\n",
      "   -6.83894986e-02   7.07192999e-02   1.08748339e-02  -2.12103056e-02\n",
      "    2.79178711e-02  -3.67887120e-02  -3.72351677e-02  -5.58606727e-03\n",
      "    1.02957551e-02   1.39498810e-03  -5.02402053e-02  -5.90509792e-01\n",
      "   -3.63405267e-01  -2.30650019e-01  -1.94430434e-01  -8.52174060e-01\n",
      "   -7.23320123e-01  -8.48994250e-01   1.33395248e-01  -5.82556011e-02\n",
      "    7.75840695e-03  -4.76128901e-01  -2.56688543e-01  -3.67671328e-01\n",
      "   -3.57203364e-01   4.42042358e+00   4.07232070e+00   4.50856891e+00\n",
      "    4.97235334e+00  -2.52359601e-01  -5.47052043e-01   4.79068450e-01\n",
      "    5.51439191e-01  -1.70041216e+00  -2.36758589e+00  -8.96457433e-01\n",
      "   -1.75954202e+00  -1.59400282e+00  -1.73345356e+00   4.92176454e-01\n",
      "    3.39896326e-01   7.43720968e-01   8.01086186e-01   5.40126373e-01\n",
      "    3.82223704e-01]\n",
      " [  3.03517857e+00  -4.03484178e-02  -3.98354468e-02  -6.02793092e-02\n",
      "    7.05869311e-02   2.89589958e-02  -3.28385613e-02   1.53458069e-02\n",
      "   -6.16649884e-03   4.88981901e-02  -2.64722529e-02   7.68067614e-03\n",
      "    1.51270999e-02  -1.64874310e-01   2.07256532e-02  -3.48928715e-02\n",
      "    6.67175339e-02  -3.28908040e-02  -1.92689154e-02  -9.47726385e-03\n",
      "    1.14328901e-01  -3.08792121e-02   1.03591942e-01  -1.60260238e-02\n",
      "    2.54514548e-02  -4.29509232e-02  -1.83113397e-02  -1.56261927e-01\n",
      "    1.16220122e-01   1.02561403e-01  -2.65961372e-02   5.28967175e-02\n",
      "   -6.89840484e-03   1.75157885e-02  -8.78816111e-02   4.78004247e-02\n",
      "    1.34038790e-02  -2.06760726e-02  -3.68221192e-02  -2.81547538e-02\n",
      "    1.47751639e-02   2.41163212e-02   4.99817930e-02   5.51170194e-02\n",
      "   -2.25959814e-02  -2.82202044e-03   9.88003301e-03  -2.82958503e-02\n",
      "   -3.36505747e-02  -5.09474080e-02   3.91361529e-02   2.93396516e-02\n",
      "   -6.97443608e-03  -4.82897443e-02   4.10499363e-02  -2.39393236e-02\n",
      "   -2.51916187e-03   6.23903018e-02   2.20631744e-02  -5.26276583e-03\n",
      "   -1.09837934e-01   4.90824976e-02   3.85096505e-03  -1.58574835e-02\n",
      "    5.61024536e-02   6.85265111e-02   9.97009780e-03   2.83680161e-02\n",
      "   -4.69093709e-02   1.74840906e-02   1.93462556e-02  -1.91659447e-02\n",
      "    1.95868014e-02  -1.98988989e-02   1.82943867e-03   1.10029041e-02\n",
      "    1.66613906e-02  -5.58031150e-02   5.12576965e-02  -7.01999570e-03\n",
      "   -9.93570486e-02  -3.31480154e-02  -1.00690550e-02  -2.04159969e-02\n",
      "    2.51835101e-02  -2.39548664e-02  -3.01703118e-02  -2.67149250e-03\n",
      "    7.26968875e-02  -2.19631224e-02   4.02285455e-02   9.63404546e-03\n",
      "    1.95257778e-02  -4.19113215e-02  -1.81862831e-02  -9.06572020e-03\n",
      "    2.50831560e-02  -4.32053719e-02  -6.82043326e-02   4.19141368e-02\n",
      "    6.12226424e-02  -3.33606315e-02  -5.91620041e-02   9.33045958e-02\n",
      "   -1.49973073e-02   1.15502114e-01  -5.14195295e-02   2.57652864e-02\n",
      "   -7.26166849e-02   1.25815019e-02  -1.05286810e-01  -8.08215031e-02\n",
      "   -2.80229308e-02  -5.22823965e-02  -1.52562952e-03   4.83093611e-02\n",
      "    1.05014713e-01  -5.28268915e-02   2.49062259e-02   7.53696316e-02\n",
      "    7.60647679e-02   8.15200060e-02  -4.13374795e-02   5.80126330e-03\n",
      "    2.04574138e-02  -2.06262615e-02  -7.03190744e-02   7.36665102e-03\n",
      "    5.64494948e-02  -2.66482524e-02  -5.45241035e-03  -2.39455308e-02\n",
      "    9.84549906e-02  -2.81216664e-03  -3.03461978e-02  -3.81971569e-02\n",
      "   -6.70443785e-03   2.56097518e-03   3.05484350e-02   8.29874571e-01\n",
      "    7.53775518e-01   5.09383799e-01   7.36365985e-01  -3.44822493e-01\n",
      "   -3.18130461e-01  -2.29203568e-01   6.83444462e-01   9.30687216e-01\n",
      "    6.91345405e-01   6.79897869e-01   5.57480410e-01   7.76617539e-01\n",
      "    8.27521665e-01  -1.92479550e+00  -1.98267032e+00  -1.67974134e+00\n",
      "   -1.66095248e+00   1.18262811e+00   1.46740133e+00  -1.90721850e+00\n",
      "   -1.74001458e+00   1.77289656e-01   7.16362125e-01  -2.07918588e-01\n",
      "    2.13480461e-01  -1.01725680e-01   2.50688840e-01  -1.05187563e+00\n",
      "   -9.72511436e-01  -1.41193421e-01  -2.80955796e-01  -7.93950992e-01\n",
      "   -6.92683073e-01]\n",
      " [  1.80786555e+00  -5.94525158e-02  -3.91576944e-02   5.98201865e-02\n",
      "   -5.68210652e-02  -3.84542039e-02  -5.86491482e-02   4.19865223e-02\n",
      "    5.91894471e-02  -5.61238114e-02   8.88061335e-02   3.07371391e-02\n",
      "    5.08283499e-02   5.96277747e-02   2.09849513e-03   9.29644342e-02\n",
      "   -1.82339667e-02   1.67257959e-02   6.88103318e-03   1.18703672e-01\n",
      "   -5.01897404e-02  -3.04875070e-02  -5.78225413e-02  -7.83163975e-02\n",
      "   -3.80151330e-02   6.00257604e-02  -9.86794222e-04   5.54875894e-03\n",
      "   -9.00919247e-02   1.75640093e-02  -8.05234968e-02  -3.71122264e-02\n",
      "    2.48409417e-02  -2.51336687e-03  -2.95686480e-02  -2.77599310e-02\n",
      "   -1.89371164e-02   2.82234926e-02   2.98369251e-02  -2.33760537e-02\n",
      "    3.15438684e-02  -3.36971540e-02   1.22750456e-02   1.64728215e-02\n",
      "    6.08265451e-02  -7.90269020e-03  -3.50752425e-02   5.33246051e-02\n",
      "   -4.02857329e-02  -4.98135361e-03  -9.79753019e-02  -3.03785836e-02\n",
      "    3.11113657e-02   3.92453772e-02  -1.13735434e-02   1.59092373e-02\n",
      "    3.16934551e-02   3.36376604e-02  -3.67752198e-02   1.53857497e-02\n",
      "    1.54075792e-02  -5.33522048e-02   5.27192099e-02   3.97520820e-02\n",
      "   -1.79723834e-02  -4.67926314e-02   4.41833822e-02   2.95803351e-02\n",
      "    3.96035092e-02   3.87435974e-02   4.30968373e-02  -8.67758094e-03\n",
      "    5.21334411e-02   4.34265107e-02  -7.58053991e-02  -1.38666707e-02\n",
      "   -3.43641090e-02   9.60260935e-03  -4.95129900e-02   2.93792963e-02\n",
      "    6.50042026e-02   9.16392638e-03   3.39149480e-04   1.18682359e-03\n",
      "    9.89571055e-03   3.72136494e-02  -1.63313654e-02  -6.53630635e-03\n",
      "   -8.43220761e-02  -2.44403953e-03  -1.16524200e-01  -3.43506120e-02\n",
      "    1.81188590e-02  -1.07286573e-02  -1.75712285e-02   2.01956722e-02\n",
      "    1.43637732e-02   4.10242344e-02   4.20905883e-02   8.48423752e-02\n",
      "   -6.54910991e-02  -7.19309005e-02  -1.79579425e-02  -2.66447650e-02\n",
      "   -5.64668686e-02  -2.70311006e-02  -8.48367951e-02   5.92668184e-02\n",
      "   -6.38197174e-02   2.22544854e-02   1.02408377e-01   4.93004059e-02\n",
      "    7.89793530e-02   2.28908132e-02  -1.83118334e-02  -5.28745209e-02\n",
      "    1.45631124e-01   5.09716971e-02  -6.12091022e-02   2.66027961e-03\n",
      "   -1.22051552e-02   1.48712547e-02  -4.33875392e-02  -5.45545601e-02\n",
      "    2.68705431e-02  -3.22787361e-02   6.22005113e-02  -2.13034570e-02\n",
      "    5.56458969e-03   9.90534726e-03  -2.64364921e-02   3.89103103e-02\n",
      "   -4.58053698e-02   4.09754879e-02   6.02712890e-03  -9.21909604e-03\n",
      "   -4.84814712e-04  -3.78070178e-03  -3.47386769e-02  -5.70504085e-01\n",
      "   -5.20353546e-01  -4.84820021e-01  -6.05477284e-01   5.68506966e-01\n",
      "    7.59143894e-01   5.51468868e-01  -1.82901963e+00  -1.94893346e+00\n",
      "   -1.78131466e+00  -1.64192069e-01  -5.92451575e-01  -3.30471053e-01\n",
      "   -4.39082679e-01   9.66833978e-01   1.15142907e+00   8.82018523e-01\n",
      "    8.23271276e-02  -2.20631625e-01  -2.10045289e-01   1.91408041e-01\n",
      "    1.67209410e-02   1.13282745e+00   9.64663160e-01   9.56556632e-01\n",
      "    1.51596817e+00   9.65310267e-01   1.09833042e+00  -5.23117565e-01\n",
      "   -5.95106177e-01  -7.27217228e-01  -7.17376043e-01  -4.34505665e-01\n",
      "   -4.25756718e-01]\n",
      " [ -5.52312244e+00   9.27156184e-02  -3.02261945e-02   7.42526324e-02\n",
      "   -2.98044080e-02   6.84259778e-04   3.33443616e-02  -2.56283716e-02\n",
      "   -3.90496003e-02   4.89183865e-02  -1.38503025e-02   1.69682872e-02\n",
      "   -2.46913017e-02   5.23306213e-02  -2.48901554e-02  -6.75480420e-02\n",
      "    2.86238436e-02  -4.46790304e-03   2.71889749e-02  -2.99221083e-02\n",
      "   -5.97483266e-03   5.68943045e-02  -3.38648665e-02   4.38341315e-02\n",
      "    3.18319148e-02  -2.99255930e-02   1.74638912e-02   3.32437918e-02\n",
      "   -6.56830327e-03  -5.71054097e-02   1.87513126e-03   4.80014804e-02\n",
      "    5.69587492e-02  -3.37724691e-02  -2.22052548e-03   3.06474221e-02\n",
      "   -4.49486995e-02   2.41952690e-02   1.69685342e-02   5.83915697e-02\n",
      "   -1.77007454e-02   1.09668745e-02   3.71791023e-04   7.48428307e-03\n",
      "   -6.61273585e-02   1.71909653e-02  -5.04591331e-02  -1.59104531e-02\n",
      "    6.17987219e-02   7.57008877e-02   5.04011100e-02  -3.92349218e-02\n",
      "   -3.58840000e-02   2.78143382e-02  -8.69246982e-03   2.87018605e-03\n",
      "    3.92066641e-03  -3.23399908e-02   9.96740554e-03  -1.61839311e-02\n",
      "   -2.72618125e-02   7.64113584e-02   1.37674717e-02  -1.41312686e-03\n",
      "    3.23097729e-02  -3.66255730e-02   3.25547230e-02  -4.35841120e-03\n",
      "    1.33449373e-02   7.20410303e-03  -2.19866295e-02   4.36781351e-02\n",
      "    2.05689035e-02   3.81982932e-02   2.41212373e-02  -3.62359966e-02\n",
      "   -1.53731229e-02  -3.21685887e-03   1.05860428e-02  -8.27051203e-03\n",
      "   -2.91823825e-02   3.33468678e-02  -1.32638706e-02  -4.83241482e-03\n",
      "   -6.26697276e-03   4.11097510e-02   3.29524424e-02   8.93033137e-03\n",
      "   -8.10143238e-03   2.65236787e-02   2.39717790e-02   4.84019045e-02\n",
      "    2.66005560e-02   3.30810208e-02  -5.38302247e-03  -8.90942468e-02\n",
      "   -8.11947903e-03   2.46768015e-02   1.70793188e-03  -6.63882770e-02\n",
      "    9.59652275e-02   2.52865584e-03   1.51650616e-01  -5.01220631e-04\n",
      "   -7.80051457e-02   1.13016950e-03   1.03917158e-02  -2.28389425e-02\n",
      "    3.77518340e-03  -5.37597020e-02  -3.18268344e-02  -3.39019541e-02\n",
      "   -5.72291962e-03  -5.48058953e-02   1.23512554e-02  -1.09717439e-02\n",
      "    2.03414181e-01  -3.32496477e-02   1.46695404e-05   4.42244828e-02\n",
      "   -4.77459753e-02  -2.72588589e-02  -1.57394959e-03   1.04760391e-02\n",
      "   -1.66795063e-03   3.28264474e-02   2.76965212e-02   1.68195564e-02\n",
      "   -4.12008994e-02  -2.14764971e-02   2.14553118e-03  -7.85490148e-03\n",
      "   -8.07195834e-02   3.32541139e-02   3.60148355e-02   2.70982195e-02\n",
      "   -2.80718820e-03   1.60739228e-03   3.29238461e-03   3.05685298e-02\n",
      "    3.98410406e-02   8.41407319e-02   3.30618121e-02   3.23092158e-01\n",
      "    9.50092018e-02   2.50636247e-01   5.44670698e-01   6.72261777e-01\n",
      "    6.44679897e-01  -3.72366166e-01  -1.23470237e-01  -2.84664363e-01\n",
      "   -2.71964592e-01   1.09282167e-01   2.48759572e-01   2.35236727e-02\n",
      "    7.51717231e-01  -1.86046163e-01  -5.29517915e-01   7.49013166e-01\n",
      "    7.91237662e-01   1.46739596e-01  -2.09062409e-01  -6.46201321e-02\n",
      "   -8.85908779e-02   2.08114164e-01   1.44621101e-01   5.10326734e-01\n",
      "    5.39226155e-01  -1.89989618e-01  -3.27108738e-01   3.11119048e-01\n",
      "    3.85594353e-01]]\n",
      "Accuracy of:  0.612871287129\n",
      "CPU times: user 2.94 s, sys: 121 ms, total: 3.06 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear boundaries visualization from sklearn documentation\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_decision_boundaries(lr,Xin,y,title=''):\n",
    "    Xb = copy.deepcopy(Xin)\n",
    "    lr.fit(Xb[:,:2],y) # train only on two features\n",
    "\n",
    "    h=0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "    y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb[:, 0], Xb[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Fear of Public Speaking')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  3.,  2., ...,  1.,  1.,  0.],\n",
       "       [ 4.,  4.,  2., ...,  0.,  1.,  0.],\n",
       "       [ 5.,  5.,  2., ...,  0.,  1.,  0.],\n",
       "       ..., \n",
       "       [ 4.,  3.,  1., ...,  0.,  1.,  0.],\n",
       "       [ 5.,  3.,  3., ...,  0.,  1.,  0.],\n",
       "       [ 5.,  5.,  4., ...,  1.,  0.,  1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.lr_explor>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import widgets as wd\n",
    "\n",
    "cost_vals = np.logspace(-3,-2,15)\n",
    "def lr_explor(cost_idx):\n",
    "    C = cost_vals[cost_idx]\n",
    "    lr_clf = MultiClassLogisticRegression(eta=0.1,\n",
    "                                           iterations=2500,\n",
    "                                           C=C) # get object\n",
    "    \n",
    "    plot_decision_boundaries(lr_clf,X,y,title=\"C=%.5f\"%(C))\n",
    "\n",
    "wd.interact(lr_explor,cost_idx=(0,15,1),__manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 152.98 MiB, increment: 0.02 MiB\n",
      "Accuracy of:  0.117821782178\n",
      "CPU times: user 473 ms, sys: 42.8 ms, total: 516 ms\n",
      "Wall time: 652 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression() # all params default\n",
    "lr_sk.fit(X,y)\n",
    "# print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "# yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-75cdf50c7591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.logspace(0.1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
