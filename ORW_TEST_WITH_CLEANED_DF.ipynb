{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "df = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change NaN number values to the mean\n",
    "df_imputed = df.fillna(df.mean())\n",
    "# get categorical features\n",
    "object_features = list(df.select_dtypes(include=['object']).columns)\n",
    "# one hot encode categorical features\n",
    "one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in object_features], axis=1)\n",
    "# drop object features from imputed dataframe\n",
    "df_imputed_dropped = df_imputed.drop(object_features, 1)\n",
    "frames = [df_imputed_dropped, one_hot_df]\n",
    "# concatenate both frames by columns\n",
    "df_fixed = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Fear of public speaking' in df_fixed:\n",
    "    y = df_fixed['Fear of public speaking'].values # get the labels we want\n",
    "    del df_fixed['Fear of public speaking'] # get rid of the class label\n",
    "    X = df_fixed.values # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47 µs, sys: 1 µs, total: 48 µs\n",
      "Wall time: 53.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "# blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "# blr.fit(X,y)\n",
    "# print(blr)\n",
    "\n",
    "# yhat = blr.predict(X)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54 µs, sys: 1 µs, total: 55 µs\n",
      "Wall time: 59.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X + 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "# hlr = HessianBinaryLogisticRegression(eta=0.1,iterations=20,C=0.1) # note that we need only a few iterations here\n",
    "\n",
    "# hlr.fit(X,y)\n",
    "# yhat = hlr.predict(X)\n",
    "# print(hlr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 µs, sys: 1 µs, total: 53 µs\n",
      "Wall time: 57.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += 2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "# bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "# bfgslr.fit(X,y)\n",
    "# yhat = bfgslr.predict(X)\n",
    "# print(bfgslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            hblr = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.237623762376\n",
      "confusion matrix\n",
      " [[10  9  0  5  2  1]\n",
      " [10 14  0 18  4  2]\n",
      " [ 9 15  0 27 15  4]\n",
      " [ 4  4  0 17  5  6]\n",
      " [ 1  0  0 10  7  3]\n",
      " [ 0  0  0  0  0  0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't handle mix of continuous and multiclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-91c37fd2a708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# now let's get the accuracy and confusion matrix for this iterations of training/testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"====Iteration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" ====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Omar/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Omar/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         raise ValueError(\"Can't handle mix of {0} and {1}\"\n\u001b[0;32m---> 82\u001b[0;31m                          \"\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't handle mix of continuous and multiclass"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = MultiClassLogisticRegression(eta=0.1,iterations=10, C=0.0001) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear boundaries visualization from sklearn documentation\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_decision_boundaries(lr,Xin,y,title=''):\n",
    "    Xb = copy.deepcopy(Xin)\n",
    "    lr.fit(Xb[:,:2],y) # train only on two features\n",
    "\n",
    "    h=0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "    y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb[:, 0], Xb[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Fear of Public Speaking')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  3.,  2., ...,  1.,  1.,  0.],\n",
       "       [ 4.,  4.,  2., ...,  0.,  1.,  0.],\n",
       "       [ 5.,  5.,  2., ...,  0.,  1.,  0.],\n",
       "       ..., \n",
       "       [ 4.,  3.,  1., ...,  0.,  1.,  0.],\n",
       "       [ 5.,  3.,  3., ...,  0.,  1.,  0.],\n",
       "       [ 5.,  5.,  4., ...,  1.,  0.,  1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEFCAYAAADDkQ0WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b5bd6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets as wd\n",
    "\n",
    "cost_vals = np.logspace(-3,-2,15)\n",
    "def lr_explor(cost_idx):\n",
    "    C = cost_vals[cost_idx]\n",
    "    lr_clf = MultiClassLogisticRegression(eta=0.1,\n",
    "                                           iterations=2500,\n",
    "                                           C=C) # get object\n",
    "    \n",
    "    plot_decision_boundaries(lr_clf,X,y,title=\"C=%.5f\"%(C))\n",
    "\n",
    "wd.interact(lr_explor,cost_idx=(0,15,1),__manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0cc10eef3cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from sklearn.linear_model import LogisticRegression as SKLogisticRegression\\n\\nlr_sk = SKLogisticRegression() # all params default\\nlr_sk.fit(X,y)\\n# print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\\n# yhat = lr_sk.predict(X)\\nprint('Accuracy of: ',accuracy_score(y,yhat))\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Omar/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/Omar/anaconda/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Omar/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/Omar/anaconda/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n\u001b[1;32m   1173\u001b[0m                          order=\"C\")\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Omar/anaconda/lib/python3.5/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m             'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression() # all params default\n",
    "lr_sk.fit(X,y)\n",
    "# print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "# yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "f2fd0a3c04d44d8c83dc4725b2ff0e05": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
