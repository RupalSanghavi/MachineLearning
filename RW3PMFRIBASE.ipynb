{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "import time\n",
    "\n",
    "\n",
    "df = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change NaN number values to the mean\n",
    "df_imputed = df.fillna(df.median())\n",
    "# get categorical features\n",
    "object_features = list(df.select_dtypes(include=['object']).columns)\n",
    "# one hot encode categorical features\n",
    "one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in object_features], axis=1)\n",
    "# drop object features from imputed dataframe\n",
    "df_imputed_dropped = df_imputed.drop(object_features, 1)\n",
    "frames = [df_imputed_dropped, one_hot_df]\n",
    "# concatenate both frames by columns\n",
    "df_fixed = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Fear of public speaking' in df_fixed:\n",
    "    y = df_fixed['Fear of public speaking'].values # get the labels we want\n",
    "    del df_fixed['Fear of public speaking'] # get rid of the class label\n",
    "    X = expit(df_fixed.values) # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 µs, sys: 0 ns, total: 32 µs\n",
      "Wall time: 36 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "# blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "# blr.fit(X,y)\n",
    "# print(blr)\n",
    "\n",
    "# yhat = blr.predict(X)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37 µs, sys: 0 ns, total: 37 µs\n",
      "Wall time: 40.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X + 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "# hlr = HessianBinaryLogisticRegression(eta=0.1,iterations=20,C=0.1) # note that we need only a few iterations here\n",
    "\n",
    "# hlr.fit(X,y)\n",
    "# yhat = hlr.predict(X)\n",
    "# print(hlr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61 µs, sys: 4 µs, total: 65 µs\n",
      "Wall time: 76.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.optimize import OptimizeResult\n",
    "\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    @staticmethod\n",
    "    def line_search_function(eta,X,y,w,grad):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)+np.sum(wnew**2)\n",
    "    @staticmethod\n",
    "    def line_search_function_l1(eta,X,y,w,grad):\n",
    "        if(math.sin(w) < 0 ):\n",
    "            w -=1\n",
    "        elif(math.sin(w) > 0):\n",
    "            w += 1\n",
    "        else:\n",
    "            w = w\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)+np.sum(math.fabs(wnew))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ += gradient*eta # set new function values\n",
    "                \n",
    "            \n",
    "\n",
    "# lslr = LineSearchLogisticRegression(eta=0.1,iterations=110, C=0.001)\n",
    "\n",
    "# lslr.fit(X,y)\n",
    "\n",
    "# yhat = lslr.predict(X)\n",
    "# print(lslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43 µs, sys: 1 µs, total: 44 µs\n",
      "Wall time: 47.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "# slr = StochasticLogisticRegression(0.1,1000, C=0.001) # take a lot more steps!!\n",
    "\n",
    "# slr.fit(X,y)\n",
    "\n",
    "# yhat = slr.predict(X)\n",
    "# print(slr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38 µs, sys: 1 µs, total: 39 µs\n",
      "Wall time: 41 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += 2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "# bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "# bfgslr.fit(X,y)\n",
    "# yhat = bfgslr.predict(X)\n",
    "# print(bfgslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, optimization=None):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.optimization = optimization\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            #hblr = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            if(self.optimization == \"BFGSBinaryLogisticRegression\"):\n",
    "                hblr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            elif(self.optimization == \"StochasticLogisticRegression\"):\n",
    "                hblr = StochasticLogisticRegression(self.eta,self.iters,self.C)\n",
    "            else:\n",
    "                hblr = LineSearchLogisticRegression(self.eta,self.iters,self.C)\n",
    "\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.336633663366\n",
      "confusion matrix\n",
      " [[ 0  0 39  0  0]\n",
      " [ 0  0 44  0  0]\n",
      " [ 0  0 68  0  0]\n",
      " [ 0  0 34  0  0]\n",
      " [ 0  0 17  0  0]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.311881188119\n",
      "confusion matrix\n",
      " [[ 0  0 37  0  0]\n",
      " [ 0  0 42  0  0]\n",
      " [ 0  0 63  0  0]\n",
      " [ 0  0 44  0  0]\n",
      " [ 0  0 16  0  0]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.336633663366\n",
      "confusion matrix\n",
      " [[ 0  0 30  0  0]\n",
      " [ 0  0 44  0  0]\n",
      " [ 0  0 68  0  0]\n",
      " [ 0  0 43  0  0]\n",
      " [ 0  0 17  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "\n",
    "lr_clf = MultiClassLogisticRegression(eta=0.1,iterations=2500, C=0.006, optimization=\"BFGSBinaryLogisticRegression\") # get object\n",
    "lr_clf_accuracies = []\n",
    "lr_clf_times = []\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = expit(X[train_indices])\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = expit(X[test_indices])\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    st = time.time()\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    t = (time.time() -st)\n",
    "    lr_clf_times.append(t)\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    lr_clf_accuracies.append(acc)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-0.05879671 -0.05804428 -0.05526859 -0.06054565 -0.052267   -0.05351393\n",
      "  -0.05683023 -0.06197697 -0.06155897 -0.05704704 -0.05106109 -0.05795078\n",
      "  -0.0529062  -0.0477312  -0.05412005 -0.05605494 -0.04988172 -0.05426526\n",
      "  -0.05548312 -0.05012395 -0.05915867 -0.0500789  -0.06567154 -0.05777443\n",
      "  -0.06066835 -0.06173793 -0.0503009  -0.06066656 -0.06476976 -0.06023164\n",
      "  -0.0471049  -0.05563847 -0.04584592 -0.04778198 -0.03651198 -0.05318108\n",
      "  -0.03835143 -0.0574508  -0.05096968 -0.03521597 -0.06329312 -0.0588248\n",
      "  -0.05537848 -0.05057318 -0.05775464 -0.06147044 -0.02760299 -0.03847083\n",
      "  -0.04607932 -0.04512763 -0.06506676 -0.04865159 -0.04216681 -0.04707778\n",
      "  -0.05371216 -0.04146041 -0.05815593 -0.07010551 -0.05526274 -0.05211043\n",
      "  -0.0522725  -0.05581161 -0.03786508 -0.06547102 -0.07566846 -0.06516685\n",
      "  -0.07262119 -0.07434775 -0.07708026 -0.07876693 -0.07366707 -0.07082593\n",
      "  -0.0825835  -0.05968502 -0.04427827 -0.04620992 -0.05437043 -0.05226108\n",
      "  -0.05801022 -0.0603631  -0.05470031 -0.05569918 -0.05487919 -0.06275186\n",
      "  -0.04917643 -0.06839585 -0.04830414 -0.07134816 -0.04438466 -0.06227983\n",
      "  -0.0521066  -0.06690601 -0.05963322 -0.06646413 -0.05390415 -0.05966169\n",
      "  -0.06362004 -0.08053355 -0.05400927 -0.07325883 -0.08005271 -0.05756825\n",
      "  -0.05287265 -0.05048133 -0.03710252 -0.05259278 -0.03553954 -0.06868195\n",
      "  -0.05686067 -0.03394349 -0.05267417 -0.06674586 -0.05173495 -0.0418132\n",
      "  -0.06048459 -0.04355511 -0.11732981 -0.06442473 -0.07458266 -0.05052147\n",
      "  -0.04333048 -0.04742128 -0.04946231 -0.06274066 -0.06223774 -0.04085736\n",
      "  -0.05601083 -0.06494752 -0.06178182 -0.05640469 -0.045028   -0.04581194\n",
      "  -0.04795058 -0.05383225 -0.05003777 -0.05890077 -0.05890077 -0.05890077\n",
      "  -0.03716372 -0.01578893 -0.02926978 -0.03818676 -0.0476185  -0.02363649\n",
      "  -0.030826   -0.04756494 -0.02059148 -0.04616691 -0.03479304 -0.02823939\n",
      "  -0.02897797 -0.03344038 -0.04143391 -0.03554517 -0.02625859 -0.0395436\n",
      "  -0.03006371 -0.05692244 -0.01545189 -0.02721764 -0.04467892 -0.02812128\n",
      "  -0.02822519 -0.02938373 -0.03075502 -0.0294534  -0.04416859 -0.04411967\n",
      "  -0.02798115 -0.04650014 -0.02519215 -0.03776498 -0.03392773]\n",
      " [-0.02166937 -0.02143956 -0.01848562 -0.01737623 -0.00986273 -0.01014985\n",
      "  -0.01064434 -0.01460738 -0.0206159  -0.02432155 -0.02769474 -0.03288509\n",
      "  -0.01753593 -0.0152354  -0.0103363  -0.02007107 -0.02481882 -0.00570659\n",
      "  -0.01122159 -0.00865671 -0.02180164 -0.02152357 -0.01866395 -0.02196429\n",
      "  -0.01895943 -0.0172307  -0.01098536 -0.01413354 -0.01840392 -0.01482268\n",
      "  -0.00427417 -0.01833248 -0.01796205 -0.0166786   0.00173011 -0.01093504\n",
      "  -0.00867966 -0.02367588 -0.02447793 -0.01428428 -0.01864551 -0.01258279\n",
      "  -0.02519507 -0.02053862 -0.01431205 -0.01026241 -0.0084677  -0.01964411\n",
      "  -0.01493024 -0.01061859 -0.01225895 -0.0066532  -0.01242224 -0.01466346\n",
      "  -0.02182392 -0.01068489 -0.0142436  -0.02294634 -0.02213373 -0.01891562\n",
      "  -0.01061406 -0.02136061 -0.01852133 -0.01979055 -0.0254966  -0.02358899\n",
      "  -0.03235207 -0.02852314 -0.02755829 -0.0286812  -0.02695418 -0.03038479\n",
      "  -0.02645482 -0.01911322 -0.01461257 -0.01927306 -0.01444566 -0.01643402\n",
      "  -0.02056784 -0.01607114 -0.02092918 -0.02257154 -0.01108811 -0.01564177\n",
      "  -0.0224425  -0.02048103 -0.02761923 -0.02375859 -0.00795504 -0.02065094\n",
      "  -0.0197656  -0.02751163 -0.01860648 -0.01244691 -0.01763532 -0.01885973\n",
      "  -0.02386644 -0.02384479 -0.02114968 -0.02508005 -0.02636872 -0.01122853\n",
      "  -0.02032378 -0.01612224 -0.0178203  -0.00993876 -0.00915107 -0.02500354\n",
      "  -0.01606098 -0.01100687 -0.01463865 -0.01110049 -0.02344189 -0.01297954\n",
      "  -0.02342957 -0.0186305  -0.02777815 -0.01602993 -0.01896017 -0.01866232\n",
      "  -0.01496039 -0.01774071 -0.01712911 -0.01503849 -0.02606441 -0.0158744\n",
      "  -0.02287653 -0.01718548 -0.02399313 -0.01729089 -0.02260441 -0.01583755\n",
      "  -0.01992889 -0.02428174 -0.02230548 -0.02175425 -0.02175426 -0.02175426\n",
      "  -0.0222197  -0.02517841 -0.01505883 -0.00993512 -0.00043839 -0.01613182\n",
      "  -0.0052143  -0.01810012 -0.00505006 -0.02275505 -0.01037165 -0.02025182\n",
      "  -0.01188588 -0.00856498 -0.00835097 -0.0053733  -0.01850908 -0.01406344\n",
      "  -0.0105892  -0.01655038 -0.0098234  -0.01069526 -0.01539091 -0.01184298\n",
      "  -0.01319085 -0.01006942 -0.00998512 -0.0087794  -0.01717286 -0.01162075\n",
      "  -0.01469691 -0.01155953 -0.01429464 -0.00625512 -0.01959847]\n",
      " [-0.02809887 -0.0294391  -0.0273715  -0.03235776 -0.02413334 -0.02470813\n",
      "  -0.03024361 -0.02900182 -0.02624604 -0.02570162 -0.02314622 -0.01922518\n",
      "  -0.03173743 -0.04538569 -0.03246373 -0.0287214  -0.02525066 -0.0395745\n",
      "  -0.03413529 -0.0330076  -0.02474216 -0.02605493 -0.01926202 -0.0279461\n",
      "  -0.02699205 -0.02406052 -0.02607964 -0.02957405 -0.024585   -0.02457821\n",
      "  -0.02911761 -0.02379238 -0.03208132 -0.03616564 -0.03635631 -0.02316206\n",
      "  -0.02874702 -0.02458463 -0.02650125 -0.03311728 -0.02175286 -0.01780606\n",
      "  -0.02501653 -0.02307473 -0.03071804 -0.02282736 -0.03278335 -0.0244303\n",
      "  -0.03494218 -0.03245757 -0.02446625 -0.03131357 -0.0309984  -0.03164657\n",
      "  -0.01218328 -0.03378185 -0.01825976 -0.01234445 -0.02001724 -0.0221551\n",
      "  -0.03560413 -0.02777403 -0.02772279 -0.02308301 -0.01239214 -0.01066299\n",
      "  -0.0143901  -0.01248064 -0.01713551 -0.01368355 -0.01627546 -0.02266876\n",
      "  -0.01898744 -0.02663181 -0.02836781 -0.02092298 -0.0295367  -0.03039881\n",
      "  -0.02022277 -0.02214677 -0.02894588 -0.02571208 -0.02549134 -0.02527667\n",
      "  -0.02262089 -0.02206329 -0.03121781 -0.01725441 -0.01999363 -0.02521617\n",
      "  -0.02761887 -0.0183679  -0.02370959 -0.02829421 -0.0264083  -0.0282347\n",
      "  -0.0222452  -0.02318399 -0.03032969 -0.01933696 -0.0117181  -0.02972791\n",
      "  -0.02855564 -0.01860937 -0.02541534 -0.01788976 -0.03144249 -0.02022857\n",
      "  -0.02756312 -0.02982444 -0.02950754 -0.03137958 -0.02718468 -0.03428001\n",
      "  -0.02355125 -0.02563592  0.00134228 -0.02668497 -0.02128778 -0.02796506\n",
      "  -0.02683173 -0.02252333 -0.02833143 -0.02144359 -0.019404   -0.026531\n",
      "  -0.02612927 -0.02435726 -0.01355359 -0.02151765 -0.02017713 -0.02608251\n",
      "  -0.02124299 -0.02076369 -0.02846338 -0.02816785 -0.02816785 -0.02816785\n",
      "  -0.01885528 -0.01611071 -0.0177063  -0.01899604 -0.0085575  -0.02427657\n",
      "  -0.01874987 -0.00518116 -0.02458082  0.00242901 -0.02599438 -0.01617351\n",
      "  -0.01697007 -0.01739778 -0.01168879 -0.02144898 -0.01844326 -0.00892941\n",
      "  -0.01402248 -0.01930029 -0.01353403 -0.02053068 -0.01617167 -0.01334699\n",
      "  -0.01322399 -0.01463629 -0.01930147 -0.02390819 -0.00628818 -0.02353814\n",
      "  -0.01150655 -0.01697058 -0.01746007 -0.02679415 -0.00960103]\n",
      " [-0.0343511  -0.03437001 -0.03383023 -0.0240803  -0.03670832 -0.03370922\n",
      "  -0.03386617 -0.02464163 -0.02470911 -0.03081736 -0.02113485 -0.01886529\n",
      "  -0.02405536 -0.02159542 -0.02889059 -0.02489401 -0.03178294 -0.02465333\n",
      "  -0.02612261 -0.02525513 -0.03566522 -0.02963849 -0.0326168  -0.03431529\n",
      "  -0.03134009 -0.02705505 -0.03755821 -0.03620344 -0.03527807 -0.03585706\n",
      "  -0.03525574 -0.03398158 -0.036182   -0.03173348 -0.04600363 -0.03588445\n",
      "  -0.03239943 -0.0333739  -0.02520206 -0.04110298 -0.02436003 -0.028066\n",
      "  -0.03026364 -0.03553944 -0.03444664 -0.02595255 -0.04396337 -0.03111916\n",
      "  -0.03531963 -0.03897959 -0.03999135 -0.03625431 -0.03075241 -0.02598644\n",
      "  -0.04315429 -0.03549917 -0.03222125 -0.02344541 -0.03795785 -0.03088329\n",
      "  -0.03234163 -0.035747   -0.03568891 -0.01675789 -0.01660594 -0.02196292\n",
      "  -0.01132594 -0.01738454 -0.01191522 -0.01034547 -0.01254277 -0.00927865\n",
      "  -0.01362957 -0.02991805 -0.04306437 -0.03269153 -0.03483724 -0.03078632\n",
      "  -0.03724415 -0.03101199 -0.03422733 -0.03508628 -0.03065715 -0.03463299\n",
      "  -0.03534389 -0.01742461 -0.02232502 -0.02719975 -0.05083918 -0.03275064\n",
      "  -0.03920001 -0.01308359 -0.03723826 -0.01720377 -0.02657423 -0.02736619\n",
      "  -0.02976867 -0.01685156 -0.0284504  -0.01957467 -0.0307487  -0.0377972\n",
      "  -0.03659249 -0.03548275 -0.0425746  -0.04171958 -0.04501038 -0.02442297\n",
      "  -0.03713863 -0.04264479 -0.02960622 -0.02330403 -0.03103906 -0.03321784\n",
      "  -0.02880453 -0.03861774 -0.01085271 -0.02865384 -0.02667933 -0.03591686\n",
      "  -0.0406808  -0.03379448 -0.03767406 -0.03287609 -0.03007484 -0.04138831\n",
      "  -0.0319091  -0.02973934 -0.03193443 -0.03526451 -0.03302804 -0.0373759\n",
      "  -0.03589673 -0.02570519 -0.03227307 -0.03446619 -0.03446619 -0.03446619\n",
      "  -0.02603239 -0.01448603 -0.01691242 -0.01641593 -0.02953288 -0.01398691\n",
      "  -0.01569827 -0.02889684 -0.02500855 -0.02361068 -0.01170182 -0.00999742\n",
      "  -0.01953733 -0.01892289 -0.02800612 -0.02874135 -0.01348939 -0.01678331\n",
      "  -0.01788203 -0.01572423 -0.02758892 -0.01290363 -0.02887751 -0.02491262\n",
      "  -0.01612531 -0.01722434 -0.01185331 -0.01490481 -0.02612537 -0.02108846\n",
      "  -0.02199932 -0.0229134  -0.0208324  -0.02159043 -0.02106525]\n",
      " [-0.07971104 -0.07665037 -0.07800084 -0.07243921 -0.0718871  -0.06964276\n",
      "  -0.07314262 -0.07092289 -0.07795302 -0.07497241 -0.06803511 -0.06494141\n",
      "  -0.07489767 -0.07004602 -0.07529376 -0.07665144 -0.06867375 -0.07727447\n",
      "  -0.06480312 -0.07275801 -0.07889784 -0.07174847 -0.07453659 -0.07727046\n",
      "  -0.0730278  -0.07592353 -0.08160972 -0.07312983 -0.07054551 -0.07828387\n",
      "  -0.07534298 -0.07970595 -0.07481507 -0.0733751  -0.08157198 -0.06934495\n",
      "  -0.07959487 -0.07859321 -0.07796807 -0.07368603 -0.07022383 -0.07179477\n",
      "  -0.0670408  -0.07601046 -0.07679636 -0.07575961 -0.07892728 -0.08348363\n",
      "  -0.06525529 -0.06352599 -0.07182422 -0.07046591 -0.07243957 -0.06208018\n",
      "  -0.07791593 -0.08346146 -0.061981   -0.06537748 -0.07285342 -0.08373978\n",
      "  -0.073235   -0.07903461 -0.08164062 -0.07990767 -0.05772821 -0.06541966\n",
      "  -0.06036596 -0.06569556 -0.06442737 -0.07034144 -0.06296188 -0.06186153\n",
      "  -0.06213993 -0.072967   -0.07753422 -0.08091545 -0.07068821 -0.07439064\n",
      "  -0.07563376 -0.0683119  -0.07700704 -0.07776453 -0.077645   -0.0763947\n",
      "  -0.08027661 -0.06324816 -0.06366719 -0.06911135 -0.0838145  -0.07097152\n",
      "  -0.07790757 -0.0578896  -0.07572284 -0.06725704 -0.07925576 -0.08049644\n",
      "  -0.07656076 -0.06023532 -0.07844453 -0.07265441 -0.05570118 -0.0694775\n",
      "  -0.0748216  -0.07090942 -0.0884974  -0.08248157 -0.09071221 -0.07235915\n",
      "  -0.07693014 -0.09177146 -0.08141284 -0.07429905 -0.07919051 -0.08986203\n",
      "  -0.06946748 -0.08560809 -0.05809037 -0.07607818 -0.06257517 -0.08264589\n",
      "  -0.08869297 -0.08364883 -0.08114653 -0.07169331 -0.07303594 -0.08765419\n",
      "  -0.0744806  -0.06699164 -0.07574682 -0.07698163 -0.08384026 -0.08341997\n",
      "  -0.08211892 -0.07793558 -0.07904187 -0.07977384 -0.07977384 -0.07977384\n",
      "  -0.06203728 -0.0480657  -0.04088537 -0.03845373 -0.04958274 -0.04343185\n",
      "  -0.0476271  -0.04641315 -0.05659302 -0.03906289 -0.0421885  -0.04287409\n",
      "  -0.03641292 -0.04717322 -0.05127295 -0.05905472 -0.04079299 -0.0389635\n",
      "  -0.03916889 -0.0328887  -0.06675141 -0.04477267 -0.053061   -0.04436342\n",
      "  -0.04112182 -0.04050237 -0.04337658 -0.03784717 -0.05042018 -0.04945849\n",
      "  -0.04850054 -0.05019278 -0.04860678 -0.0493964  -0.04831023]]\n",
      "Accuracy of:  0.230693069307\n",
      "CPU times: user 1.1 s, sys: 24 ms, total: 1.13 s\n",
      "Wall time: 285 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear boundaries visualization from sklearn documentation\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_decision_boundaries(lr,Xin,y,title=''):\n",
    "    Xb = copy.deepcopy(Xin)\n",
    "    lr.fit(Xb[:,:2],y) # train only on two features\n",
    "\n",
    "    h=0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "    y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb[:, 0], Xb[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Fear of Public Speaking')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 1 µs, total: 13 µs\n",
      "Wall time: 6.91 µs\n",
      "For  BFGSBinaryLogisticRegression  and cost  0.0  Accuracy of:  0.453465346535\n",
      "CPU times: user 14 µs, sys: 1 µs, total: 15 µs\n",
      "Wall time: 7.15 µs\n",
      "For  BFGSBinaryLogisticRegression  and cost  0.002  Accuracy of:  0.444554455446\n",
      "CPU times: user 9 µs, sys: 6 µs, total: 15 µs\n",
      "Wall time: 8.11 µs\n",
      "For  BFGSBinaryLogisticRegression  and cost  0.004  Accuracy of:  0.414851485149\n",
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 8.11 µs\n",
      "For  BFGSBinaryLogisticRegression  and cost  0.006  Accuracy of:  0.372277227723\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n",
      "For  BFGSBinaryLogisticRegression  and cost  0.008  Accuracy of:  0.372277227723\n",
      "CPU times: user 8 µs, sys: 1 µs, total: 9 µs\n",
      "Wall time: 6.91 µs\n",
      "For  StochasticLogisticRegression  and cost  0.0  Accuracy of:  0.305940594059\n",
      "CPU times: user 14 µs, sys: 2 µs, total: 16 µs\n",
      "Wall time: 7.15 µs\n",
      "For  StochasticLogisticRegression  and cost  0.002  Accuracy of:  0.191089108911\n",
      "CPU times: user 16 µs, sys: 1 µs, total: 17 µs\n",
      "Wall time: 4.05 µs\n",
      "For  StochasticLogisticRegression  and cost  0.004  Accuracy of:  0.191089108911\n",
      "CPU times: user 23 µs, sys: 2 µs, total: 25 µs\n",
      "Wall time: 11 µs\n",
      "For  StochasticLogisticRegression  and cost  0.006  Accuracy of:  0.305940594059\n",
      "CPU times: user 12 µs, sys: 1 µs, total: 13 µs\n",
      "Wall time: 5.96 µs\n",
      "For  StochasticLogisticRegression  and cost  0.008  Accuracy of:  0.230693069307\n",
      "CPU times: user 11 µs, sys: 1 µs, total: 12 µs\n",
      "Wall time: 5.01 µs\n",
      "For  LineSearchLogisticRegression  and cost  0.0  Accuracy of:  0.230693069307\n",
      "CPU times: user 11 µs, sys: 0 ns, total: 11 µs\n",
      "Wall time: 5.01 µs\n",
      "For  LineSearchLogisticRegression  and cost  0.002  Accuracy of:  0.230693069307\n",
      "CPU times: user 12 µs, sys: 1e+03 ns, total: 13 µs\n",
      "Wall time: 5.96 µs\n",
      "For  LineSearchLogisticRegression  and cost  0.004  Accuracy of:  0.0970297029703\n",
      "CPU times: user 8 µs, sys: 1 µs, total: 9 µs\n",
      "Wall time: 5.01 µs\n",
      "For  LineSearchLogisticRegression  and cost  0.006  Accuracy of:  0.0970297029703\n",
      "CPU times: user 19 µs, sys: 0 ns, total: 19 µs\n",
      "Wall time: 5.96 µs\n",
      "For  LineSearchLogisticRegression  and cost  0.008  Accuracy of:  0.230693069307\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "costs = [n for n in np.arange(0,0.01,0.002)]\n",
    "optimizations = [\"BFGSBinaryLogisticRegression\",\"StochasticLogisticRegression\",\"LineSearchLogisticRegression\"]\n",
    "\n",
    "for optimization in optimizations:\n",
    "    for cost in costs:\n",
    "        %%time\n",
    "        lr = MultiClassLogisticRegression(eta=0.1,\n",
    "                                           iterations=10,\n",
    "                                           C=cost,optimization=optimization) # get object\n",
    "        lr.fit(X,y)\n",
    "#         print(lr)\n",
    "        yhat = lr.predict(X)\n",
    "        print('For ',optimization,' and cost ', cost,' Accuracy of: ',accuracy_score(y,yhat+1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.548514851485\n",
      "CPU times: user 614 ms, sys: 23.9 ms, total: 638 ms\n",
      "Wall time: 164 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='lbfgs')#,max_iter=100,C=0.005) \n",
    "lr_sk.fit(X,y)\n",
    "#print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations  [100 100  91 100  37]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.237623762376\n",
      "confusion matrix\n",
      " [[ 0 30  0  0  0]\n",
      " [ 0 48  0  0  0]\n",
      " [ 0 68  0  0  0]\n",
      " [ 0 40  0  0  0]\n",
      " [ 0 16  0  0  0]]\n",
      "Iterations  [100 100 100 100 100]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.222772277228\n",
      "confusion matrix\n",
      " [[ 0 30  0  0  0]\n",
      " [ 0 45  0  0  0]\n",
      " [ 0 65  0  0  0]\n",
      " [ 0 41  0  0  0]\n",
      " [ 0 21  0  0  0]]\n",
      "Iterations  [100 100 100 100  61]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.237623762376\n",
      "confusion matrix\n",
      " [[ 0 43  0  0  0]\n",
      " [ 0 48  0  0  0]\n",
      " [ 0 54  0  0  0]\n",
      " [ 0 39  0  0  0]\n",
      " [ 0 18  0  0  0]]\n",
      "[0.1221010684967041, 0.14481019973754883, 0.12372183799743652]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='lbfgs')#,max_iter=100,C=0.005) \n",
    "\n",
    "lr_sk_accuracies = []\n",
    "lr_sk_times = []\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    st = time.time()\n",
    "    lr_sk.fit(X_train,y_train)\n",
    "    t = (time.time() -st)\n",
    "    lr_sk_times.append(t)\n",
    "    #print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "    yhat = lr_sk.predict(X_test)\n",
    " \n",
    "    print(\"Iterations \",lr_sk.n_iter_)\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    lr_sk_accuracies.append(acc)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "\n",
    "print(lr_sk_times)\n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.856000900268555\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAGHCAYAAAA6MMHNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120e96780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ff85d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot([lr_sk_accuracies,lr_clf_accuracies])\n",
    "plt.title(\"Comparing Accuracies\")\n",
    "plt.xlabel('Implementation of Logistic Regression')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()\n",
    "print((time.time() -st)*100)\n",
    "# ax = fig.add_subplot(111)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1207cfbe0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGHCAYAAABrpPKuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1207cf5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1207cfbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot([lr_sk_times,lr_clf_times])\n",
    "plt.title(\"Comparing Training Times\")\n",
    "plt.xlabel('Implementation of Logistic Regression')\n",
    "plt.ylabel('Training Time (seconds) ')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1221010684967041, 0.14481019973754883, 0.12372183799743652]\n",
      "[0.15733003616333008, 0.13342595100402832, 0.12798094749450684]\n"
     ]
    }
   ],
   "source": [
    "print(lr_sk_times)\n",
    "print(lr_clf_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = [time/100 for time in lr_clf_times ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
