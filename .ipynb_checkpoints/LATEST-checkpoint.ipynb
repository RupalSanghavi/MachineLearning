{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Four: Extending Logistic Regression \n",
    "## Rupal Sanghavi, Omar Roa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset represents the responses from students and their friends(ages 15-30, henceforth stated as \"young people\") of a Statistics class from the Faculty of Social and Economic Sciences at The Comenius University in Bratislava, Slovakia. Their survey was a mix of various topics.\n",
    "\n",
    "* Music preferences (19 items)\n",
    "* Movie preferences (12 items)\n",
    "* Hobbies & interests (32 items)\n",
    "* Phobias (10 items)\n",
    "* Health habits (3 items)\n",
    "* Personality traits, views on life, & opinions (57 items)\n",
    "* Spending habits (7 items)\n",
    "* Demographics (10 items)\n",
    "\n",
    "The dataset can be found here. https://www.kaggle.com/miroslavsabo/young-people-survey\n",
    "\n",
    "Our target is to predict how likely a \"young person\" would be interested in shopping at a large shopping center. We were not given details about what a \"large\" shopping center, but searching online for malls led us to the Avion Shopping Park in Ružinov, Slovakia. It has an area of 103,000m<sup>2</sup> and is the largest shopping mall in Slovakia (https://www.avion.sk/sk-sk/about-the-centre/fakty-a-cisla). \n",
    "\n",
    "Slovakia is in the lower half of European nations by size (28/48 - https://en.wikipedia.org/wiki/List_of_European_countries_by_area) and is very mountainous, making real estate space a precious commodity. This information would be of great interest to any commercial devlopment firm deciding on where to build their next shopping center or place of business. This could also help other parties trying to purchase real estate for youth-orientated construction (parks, recreation centers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "import time\n",
    "import math\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "target_classifier = 'Shopping centres'\n",
    "df = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Prepare Class Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove rows whose target classfier value is NaN\n",
    "df_cleaned_classifier = df[np.isfinite(df[target_classifier])]\n",
    "# change NaN number values to the mean\n",
    "df_imputed = df_cleaned_classifier.fillna(df.mean())\n",
    "# get categorical features\n",
    "object_features = list(df_cleaned_classifier.select_dtypes(include=['object']).columns)\n",
    "# one hot encode categorical features\n",
    "one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in object_features], axis=1)\n",
    "# drop object features from imputed dataframe\n",
    "df_imputed_dropped = df_imputed.drop(object_features, 1)\n",
    "frames = [df_imputed_dropped, one_hot_df]\n",
    "# concatenate both frames by columns\n",
    "df_fixed = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if target_classifier in df_fixed:\n",
    "    y = df_fixed[target_classifier].values # get the labels we want\n",
    "    del df_fixed[target_classifier] # get rid of the class label\n",
    "    X = df_fixed.values # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,test_size = 0.2)\n",
    "\n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating One-Versus-All Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53 µs, sys: 15 µs, total: 68 µs\n",
      "Wall time: 78 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001,reg=0):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.iterations = 0\n",
    "        self.reg = reg\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if(self.reg == 0):\n",
    "            gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        elif(self.reg == 1):\n",
    "            gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        else:\n",
    "            gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "            gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        #gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    def _get_l1_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        return gradient\n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "# blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "# blr.fit(X,y)\n",
    "# print(blr)\n",
    "\n",
    "# yhat = blr.predict(X)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47 µs, sys: 11 µs, total: 58 µs\n",
      "Wall time: 62 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.optimize import OptimizeResult\n",
    "\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    @staticmethod\n",
    "    def line_search_function(eta,X,y,w,grad,C):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = expit(X @ wnew)>0.5\n",
    "        return np.sum((y-yhat)**2) + C*np.sum(wnew**2)\n",
    "     \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient,self.C), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ += gradient*eta # set new function values\n",
    "                \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 µs, sys: 16 µs, total: 56 µs\n",
      "Wall time: 58.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if(self.reg == 0):\n",
    "            gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        elif(self.reg == 1):\n",
    "            gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        else:\n",
    "            gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "            gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        #gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        #gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "# slr = StochasticLogisticRegression(0.1,1000, C=0.001) # take a lot more steps!!\n",
    "\n",
    "# slr.fit(X,y)\n",
    "\n",
    "# yhat = slr.predict(X)\n",
    "# print(slr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44 µs, sys: 3 µs, total: 47 µs\n",
      "Wall time: 51 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C,reg):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C,reg):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        if(reg == 0):\n",
    "            gradient[1:] += 2 * w[1:] * C\n",
    "        elif(reg == 1):\n",
    "            gradient[1:] += np.sign(w[1:]) * C\n",
    "        else:\n",
    "            gradient[1:] += 2 * w[1:] * C\n",
    "            gradient[1:] += np.sign(w[1:]) * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C,self.reg), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        result = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C,self.reg), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False,\n",
    "                            retall=True)\n",
    "        self.iterations = len(result)\n",
    "        #print(\"Iterations: \", self.iterations)\n",
    "        #print(\"iterations: \", self.iterations)\n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "    def getIterations(self):\n",
    "        return self.iterations\n",
    "# bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "# bfgslr.fit(X,y)\n",
    "# yhat = bfgslr.predict(X)\n",
    "# print(bfgslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, optimization=None,reg=0):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.optimization = optimization\n",
    "        self.reg = reg\n",
    "        self.params = {}\n",
    "\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            #hblr = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            if(self.optimization == \"BFGSBinaryLogisticRegression\"):\n",
    "                #self.iters = 10\n",
    "                hblr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.C,self.reg)\n",
    "                #print(\"Iterations: \",hblr.getIterations())\n",
    "\n",
    "            elif(self.optimization == \"StochasticLogisticRegression\"):\n",
    "                #self.iters = 2000 #1000\n",
    "                hblr = StochasticLogisticRegression(self.eta,self.iters,self.C,self.reg)\n",
    "            else:\n",
    "                #self.iters = 100\n",
    "                #self.C = 0.001\n",
    "                hblr = LineSearchLogisticRegression(self.eta,self.iters,self.C,self.reg)\n",
    "\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "    def get_params(self,deep=False):\n",
    "        #return self.params\n",
    "        return dict(C=self.C,eta=self.eta,iterations=self.iters, optimization=self.optimization)\n",
    "\n",
    "    def set_params(self,**kwds):\n",
    "        print(kwds)\n",
    "        self.C = kwds['C']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Optimization Techniques, Etas, Iterations, and Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.386138613861\n",
      "confusion matrix\n",
      " [[ 7 10  7  1  1]\n",
      " [ 9 13  8 10  1]\n",
      " [ 4  8 16 19  4]\n",
      " [ 3  4  6 24  5]\n",
      " [ 4  1  4 15 18]]\n",
      "====Iteration 1  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.326732673267\n",
      "confusion matrix\n",
      " [[ 7  7  6  3  3]\n",
      " [ 8 11 11  7  3]\n",
      " [ 2 14 11 17 11]\n",
      " [ 1  4  7 11 16]\n",
      " [ 1  2  3 10 26]]\n",
      "====Iteration 2  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.341584158416\n",
      "confusion matrix\n",
      " [[ 9  8  3  3  3]\n",
      " [ 6 10  6  3  0]\n",
      " [ 5 15 10 18  9]\n",
      " [ 1  6 15 13 15]\n",
      " [ 0  4  8  5 27]]\n",
      "====Iteration 0  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.168316831683\n",
      "confusion matrix\n",
      " [[ 0 26  0  0  0]\n",
      " [ 0 34  0  0  0]\n",
      " [ 0 50  0  0  0]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0 46  0  0  0]]\n",
      "====Iteration 1  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.257425742574\n",
      "confusion matrix\n",
      " [[ 0  1 19  0  0]\n",
      " [ 0  2 38  0  0]\n",
      " [ 0  3 50  0  0]\n",
      " [ 0  0 49  0  0]\n",
      " [ 0  1 39  0  0]]\n",
      "====Iteration 2  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.168316831683\n",
      "confusion matrix\n",
      " [[31  0  2  0  0]\n",
      " [36  0  2  0  0]\n",
      " [51  0  3  0  0]\n",
      " [47  0  0  0  0]\n",
      " [30  0  0  0  0]]\n",
      "====Iteration 0  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.30198019802\n",
      "confusion matrix\n",
      " [[ 0  1 17 14  0]\n",
      " [ 0  0 14 15  0]\n",
      " [ 0  0 22 24  1]\n",
      " [ 0  0 18 37  1]\n",
      " [ 0  0  8 28  2]]\n",
      "====Iteration 1  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.217821782178\n",
      "confusion matrix\n",
      " [[ 0  0 12 13  0]\n",
      " [ 0  0 25 18  0]\n",
      " [ 0  0 16 26  0]\n",
      " [ 0  0 16 28  0]\n",
      " [ 0  0 10 38  0]]\n",
      "====Iteration 2  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.29702970297\n",
      "confusion matrix\n",
      " [[ 0  1 10 17  0]\n",
      " [ 0  3  5 26  0]\n",
      " [ 0  2  7 41  0]\n",
      " [ 0  0  5 46  0]\n",
      " [ 0  0  2 33  4]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "with np.errstate(all='ignore'):\n",
    "    # first we create a reusable logisitic regression object\n",
    "    #   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "    optimizations = [\"BFGSBinaryLogisticRegression\",\"StochasticLogisticRegression\",\"LineSearchLogisticRegression\"]\n",
    "    #optimizations = [\"BFGSBinaryLogisticRegression\",\"BFGSBinaryLogisticRegression\",\"BFGSBinaryLogisticRegression\"]\n",
    "    etas = [0.1, 0.1, 0.001]\n",
    "    iters = [10, 5000, 150]\n",
    "    regs = [0,1,2]\n",
    "\n",
    "    for optimization,eta,iter_,reg in zip(optimizations,etas,iters,regs):\n",
    "        lr_clf = MultiClassLogisticRegression(eta=eta,iterations=iter_, C=0.02, optimization=optimization,reg=reg) # get object\n",
    "\n",
    "\n",
    "        # now we can use the cv_object that we setup before to iterate through the \n",
    "        #    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "        #    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "        iter_num=0\n",
    "        # the indices are the rows used for training and testing in each iteration\n",
    "        for train_indices, test_indices in cv_object.split(X,y): \n",
    "            # I will create new variables here so that it is more obvious what \n",
    "            # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "            # but it makes this code less readable)\n",
    "            X_train = (X[train_indices])\n",
    "            y_train = y[train_indices]\n",
    "\n",
    "        #     print(X_train)\n",
    "        #     print(y_train)\n",
    "\n",
    "            X_test = (X[test_indices])\n",
    "            y_test = y[test_indices]\n",
    "\n",
    "    #         st = time.time()\n",
    "\n",
    "            lr_clf.fit(X_train,y_train)  # train object\n",
    "    #         t = (time.time() -st)\n",
    "    #         lr_clf_times.append(t)\n",
    "\n",
    "            lr_clf.fit(X_train,y_train)\n",
    "\n",
    "            # train the reusable logisitc regression model on the training data\n",
    "            y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "            # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "            acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    #         lr_clf_accuracies.append(acc)\n",
    "    #         cost_accuracies.append([acc])\n",
    "\n",
    "            conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "            print(\"====Iteration\",iter_num,\" ====\")\n",
    "            if(reg == 0):\n",
    "                label = \"L1\"\n",
    "            elif(reg == 1):\n",
    "                label = \"L2\"\n",
    "            else:\n",
    "                label = \"L1 and L2\"\n",
    "            print('For ',optimization,' eta: ',eta, \"Iterations: \",iter_,\"Regularization: \",label,' : Accuracy of: ',acc)\n",
    "\n",
    "            #print(\"accuracy\", acc )\n",
    "            print(\"confusion matrix\\n\",conf)\n",
    "            iter_num+=1\n",
    "\n",
    "        \n",
    "    # Also note that every time you run the above code\n",
    "    #   it randomly creates a new training and testing set, \n",
    "    #   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Pipelining PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.366336633663\n",
      "confusion matrix\n",
      " [[10  4  5  6  2]\n",
      " [ 6  5  8  3  4]\n",
      " [ 2 11 15 11  9]\n",
      " [ 0  5 15 17 12]\n",
      " [ 2  1  8 14 27]]\n",
      "====Iteration 1  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.356435643564\n",
      "confusion matrix\n",
      " [[ 4  6  7  5  1]\n",
      " [ 9 12  8  9  5]\n",
      " [ 6 13 22 15  3]\n",
      " [ 4  4  8  9 12]\n",
      " [ 0  1  2 12 25]]\n",
      "====Iteration 2  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.376237623762\n",
      "confusion matrix\n",
      " [[11 10 11  1  1]\n",
      " [ 9 10  7  8  2]\n",
      " [ 3  8 17 11  5]\n",
      " [ 2  3 12 13 12]\n",
      " [ 0  2  9 10 25]]\n",
      "====Iteration 0  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.336633663366\n",
      "confusion matrix\n",
      " [[ 6  5  8  1  1]\n",
      " [13  5 12  3  1]\n",
      " [10  7 20  5 12]\n",
      " [ 5  7  8 15 18]\n",
      " [ 3  3  7  5 22]]\n",
      "====Iteration 1  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.282178217822\n",
      "confusion matrix\n",
      " [[12  7  6  2  2]\n",
      " [ 9  9  6  2  6]\n",
      " [10 17 13  3 12]\n",
      " [ 1 19  6 11 14]\n",
      " [ 2  8  4  9 12]]\n",
      "====Iteration 2  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.341584158416\n",
      "confusion matrix\n",
      " [[ 7  7  4  2  2]\n",
      " [10  8  6  3  4]\n",
      " [ 8 16 19  9 12]\n",
      " [ 4  5 12 16  9]\n",
      " [ 1  4  0 15 19]]\n",
      "====Iteration 0  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.351485148515\n",
      "confusion matrix\n",
      " [[ 9  6  1  0  2]\n",
      " [ 9  8  3  6  6]\n",
      " [11 17 11  7 19]\n",
      " [ 4  2  8 10 18]\n",
      " [ 1  4  2  5 33]]\n",
      "====Iteration 1  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.331683168317\n",
      "confusion matrix\n",
      " [[12  7  5  1  0]\n",
      " [17  3  9  4  4]\n",
      " [14  6 10  5  9]\n",
      " [ 7  2 10 11 18]\n",
      " [ 7  3  3  4 31]]\n",
      "====Iteration 2  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.361386138614\n",
      "confusion matrix\n",
      " [[14 13  1  2  2]\n",
      " [10  9  1  5  2]\n",
      " [14 15  6  7 11]\n",
      " [ 5  6  3 12 21]\n",
      " [ 1  2  0  8 32]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "optimizations = [\"BFGSBinaryLogisticRegression\",\"StochasticLogisticRegression\",\"LineSearchLogisticRegression\"]\n",
    "#optimizations = [\"BFGSBinaryLogisticRegression\",\"BFGSBinaryLogisticRegression\",\"BFGSBinaryLogisticRegression\"]\n",
    "etas = [0.1, 0.1, 0.001]\n",
    "iters = [10, 5000, 150]\n",
    "regs = [0,1,2]\n",
    "components = 90\n",
    "pca = PCA(n_components=components)\n",
    "\n",
    "with np.errstate(all='ignore'):\n",
    "    for optimization,eta,iter_,reg in zip(optimizations,etas,iters,regs):\n",
    "        mglr = MultiClassLogisticRegression(eta=eta,iterations=iter_, C=0.02, optimization=optimization,reg=reg) # get object\n",
    "        lr_clf = Pipeline([ ('pca', pca), (\"multiclasslogregression\", mglr)]) # get object\n",
    "\n",
    "\n",
    "        # now we can use the cv_object that we setup before to iterate through the \n",
    "        #    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "        #    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "        iter_num=0\n",
    "        # the indices are the rows used for training and testing in each iteration\n",
    "        for train_indices, test_indices in cv_object.split(X,y): \n",
    "            # I will create new variables here so that it is more obvious what \n",
    "            # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "            # but it makes this code less readable)\n",
    "            X_train = (X[train_indices])\n",
    "            y_train = y[train_indices]\n",
    "\n",
    "        #     print(X_train)\n",
    "        #     print(y_train)\n",
    "\n",
    "            X_test = (X[test_indices])\n",
    "            y_test = y[test_indices]\n",
    "\n",
    "    #         st = time.time()\n",
    "\n",
    "            lr_clf.fit(X_train,y_train)  # train object\n",
    "    #         t = (time.time() -st)\n",
    "    #         lr_clf_times.append(t)\n",
    "\n",
    "            lr_clf.fit(X_train,y_train)\n",
    "\n",
    "            # train the reusable logisitc regression model on the training data\n",
    "            y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "            # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "            acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    #         lr_clf_accuracies.append(acc)\n",
    "    #         cost_accuracies.append([acc])\n",
    "\n",
    "            conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "            print(\"====Iteration\",iter_num,\" ====\")\n",
    "            if(reg == 0):\n",
    "                label = \"L1\"\n",
    "            elif(reg == 1):\n",
    "                label = \"L2\"\n",
    "            else:\n",
    "                label = \"L1 and L2\"\n",
    "            print('For ',optimization,' eta: ',eta, \"Iterations: \",iter_,\"Regularization: \",label,' : Accuracy of: ',acc)\n",
    "\n",
    "            #print(\"accuracy\", acc )\n",
    "            print(\"confusion matrix\\n\",conf)\n",
    "            iter_num+=1\n",
    "\n",
    "        \n",
    "    # Also note that every time you run the above code\n",
    "    #   it randomly creates a new training and testing set, \n",
    "    #   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best: eta: 0.1, reg = 0, iters = 10, acc = 35%\n",
    "eta: eta .001, iters 150, reg 2, acc 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Values of C to Achieve Best Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.391089108911\n",
      "confusion matrix\n",
      " [[ 6 13  7  4  1]\n",
      " [ 6 11 15  4  2]\n",
      " [ 3  6 16 15  4]\n",
      " [ 0  3 15 18  9]\n",
      " [ 1  1  7  7 28]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.331683168317\n",
      "confusion matrix\n",
      " [[ 9  1 10  4  3]\n",
      " [11  9 10 10  1]\n",
      " [ 9  6 12 16  5]\n",
      " [ 3  5 16 13 14]\n",
      " [ 1  0  4  6 24]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.346534653465\n",
      "confusion matrix\n",
      " [[ 9  8  5  3  1]\n",
      " [ 5  5 12  4  4]\n",
      " [ 7  9 18 16  5]\n",
      " [ 2  6  8 17 21]\n",
      " [ 3  0  4  9 21]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.321782178218\n",
      "confusion matrix\n",
      " [[12 12  2  0  0]\n",
      " [ 8 25  4  2  2]\n",
      " [ 6 31  5  4  1]\n",
      " [ 1 23  5  3  8]\n",
      " [ 0 20  3  5 20]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.40099009901\n",
      "confusion matrix\n",
      " [[ 8  3  4  5  0]\n",
      " [ 7  8  8  3  1]\n",
      " [ 6  7 12  7 11]\n",
      " [ 1  8 15 17  9]\n",
      " [ 0  3 10 13 36]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.361386138614\n",
      "confusion matrix\n",
      " [[ 8 12  6  4  1]\n",
      " [ 3  9 10  5  2]\n",
      " [ 4 10 21  9  7]\n",
      " [ 3  4 13 11 15]\n",
      " [ 1  2  4 14 24]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.386138613861\n",
      "confusion matrix\n",
      " [[ 9  6  8  5  1]\n",
      " [ 8  5 15  8  3]\n",
      " [ 3  5 22 19  6]\n",
      " [ 0  1 13 15  6]\n",
      " [ 1  0  3 13 27]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.391089108911\n",
      "confusion matrix\n",
      " [[12  7  6  1  0]\n",
      " [14 10  5  5  0]\n",
      " [ 7 10 21 12  4]\n",
      " [ 0  3 15 10 19]\n",
      " [ 1  0  4 10 26]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.371287128713\n",
      "confusion matrix\n",
      " [[ 7  4 13  0  1]\n",
      " [ 5  0 31  1  2]\n",
      " [ 4  2 44  4  3]\n",
      " [ 0  1 32  2  5]\n",
      " [ 0  2 10  7 22]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.356435643564\n",
      "confusion matrix\n",
      " [[13 14  7  1  1]\n",
      " [ 7  8 10  8  1]\n",
      " [ 3 13 17 10  6]\n",
      " [ 1  5 12 12 17]\n",
      " [ 0  3  3  8 22]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.420792079208\n",
      "confusion matrix\n",
      " [[12  8  3  1  2]\n",
      " [ 8 11 11  3  1]\n",
      " [ 2  9 17  7  7]\n",
      " [ 1  3 15 17 14]\n",
      " [ 1  2  7 12 28]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.356435643564\n",
      "confusion matrix\n",
      " [[11 10  4  1  1]\n",
      " [ 7  9  9  2  2]\n",
      " [ 3 14 16  9  6]\n",
      " [ 2 12 13 14 18]\n",
      " [ 2  1  4 10 22]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.331683168317\n",
      "confusion matrix\n",
      " [[10  3  5  5  2]\n",
      " [ 6  7 12  7  2]\n",
      " [ 7  9 12 11  5]\n",
      " [ 3  4 16 10 10]\n",
      " [ 3  4 10 11 28]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.361386138614\n",
      "confusion matrix\n",
      " [[ 9  6 10  1  1]\n",
      " [ 8 11 12  4  1]\n",
      " [ 4  7 16 10 10]\n",
      " [ 3  5 12 15 18]\n",
      " [ 0  2  3 12 22]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.371287128713\n",
      "confusion matrix\n",
      " [[ 6 15 10  1  2]\n",
      " [ 6 13 12  6  1]\n",
      " [ 1  9 20  9 10]\n",
      " [ 1  2 12 11 16]\n",
      " [ 1  6  1  6 25]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.341584158416\n",
      "confusion matrix\n",
      " [[ 8 10  3  0  1]\n",
      " [ 7 11 15  4  1]\n",
      " [ 9  7 14  8  8]\n",
      " [ 4  8 20 14  9]\n",
      " [ 1  1  3 14 22]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.356435643564\n",
      "confusion matrix\n",
      " [[10  7  8  1  0]\n",
      " [ 6 10  7  7  2]\n",
      " [ 6 10 19 19  7]\n",
      " [ 3  6  9 12 14]\n",
      " [ 1  1  3 13 21]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.29702970297\n",
      "confusion matrix\n",
      " [[ 6  6 14  0  1]\n",
      " [10  6 16  3  0]\n",
      " [ 2  5 16 10 10]\n",
      " [ 2  5 17 12 20]\n",
      " [ 0  4  5 12 20]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.39603960396\n",
      "confusion matrix\n",
      " [[13  4  3  1  0]\n",
      " [11 10 11  6  3]\n",
      " [ 8  5 21 20 11]\n",
      " [ 3  3  6 18 10]\n",
      " [ 1  0  3 13 18]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.410891089109\n",
      "confusion matrix\n",
      " [[ 9  4  7  2  1]\n",
      " [10 13  9 10  3]\n",
      " [ 5  3 15  9  9]\n",
      " [ 2  6 10 18  7]\n",
      " [ 2  2  4 14 28]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.425742574257\n",
      "confusion matrix\n",
      " [[ 7  5  7  1  0]\n",
      " [ 8  8 12  6  1]\n",
      " [ 5  6 14 10 10]\n",
      " [ 1  3 10 20 11]\n",
      " [ 0  3  6 11 37]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.435643564356\n",
      "confusion matrix\n",
      " [[13  1  5  2  0]\n",
      " [ 8  9 10  8  2]\n",
      " [ 8  5 27 13  4]\n",
      " [ 4  3 12 14 16]\n",
      " [ 1  0  3  9 25]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.39603960396\n",
      "confusion matrix\n",
      " [[ 6  8  6  1  0]\n",
      " [11 11  7  5  0]\n",
      " [ 1  8 19  9  4]\n",
      " [ 3  7 13 16 12]\n",
      " [ 1  1  8 17 28]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.336633663366\n",
      "confusion matrix\n",
      " [[ 8  7  6  1  1]\n",
      " [ 8  6 15  6  3]\n",
      " [ 2  9 21 10  9]\n",
      " [ 2  5 15 13 22]\n",
      " [ 0  1  5  7 20]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.376237623762\n",
      "confusion matrix\n",
      " [[ 9 10  4  0  0]\n",
      " [ 5  9 12  5  6]\n",
      " [ 4 11 20  8  3]\n",
      " [ 1  5 17 12 14]\n",
      " [ 1  1  9 10 26]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.381188118812\n",
      "confusion matrix\n",
      " [[11 12  7  3  1]\n",
      " [ 7 10  9  6  1]\n",
      " [ 5  8 16 10  9]\n",
      " [ 1  3 15 17 11]\n",
      " [ 1  1  5 10 23]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.30198019802\n",
      "confusion matrix\n",
      " [[ 9 13  8  2  0]\n",
      " [ 8 10  9  7  3]\n",
      " [ 5 12 12  8  5]\n",
      " [ 0  8 19 12 12]\n",
      " [ 1  3  5 13 18]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.351485148515\n",
      "confusion matrix\n",
      " [[11  5  9  3  2]\n",
      " [11  9 10  5  3]\n",
      " [ 2  9 14 11  5]\n",
      " [ 2  4 18  7 15]\n",
      " [ 2  3  4  8 30]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.376237623762\n",
      "confusion matrix\n",
      " [[ 8 10  6  3  1]\n",
      " [ 7  7  9  3  2]\n",
      " [ 3  5 21 15 10]\n",
      " [ 1  5 11 14 13]\n",
      " [ 2  5  3 12 26]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.356435643564\n",
      "confusion matrix\n",
      " [[ 9 15  2  0  0]\n",
      " [ 4 30  3  2  0]\n",
      " [ 5 30  7  5  2]\n",
      " [ 4 29  5  4  6]\n",
      " [ 0  9  1  8 22]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.371287128713\n",
      "confusion matrix\n",
      " [[10  6  8  3  1]\n",
      " [ 7 10 18  8  2]\n",
      " [ 3  5 15 10  8]\n",
      " [ 3  6 16 12 13]\n",
      " [ 0  1  5  4 28]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.346534653465\n",
      "confusion matrix\n",
      " [[ 2  8  9  3  0]\n",
      " [ 9  4 11  6  2]\n",
      " [11  6 13  4  5]\n",
      " [ 2  9 15 18  9]\n",
      " [ 0  3 10 10 33]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.346534653465\n",
      "confusion matrix\n",
      " [[ 7 10  8  2  2]\n",
      " [ 6  8 14  5  0]\n",
      " [ 4 10 23 11  5]\n",
      " [ 2  9 13 16 13]\n",
      " [ 1  2  4 11 16]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.371287128713\n",
      "confusion matrix\n",
      " [[ 6  7  7  0  1]\n",
      " [ 5  9 10  8  0]\n",
      " [ 3  6 21 15  5]\n",
      " [ 0  6 15 14 10]\n",
      " [ 1  1  3 24 25]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.39603960396\n",
      "confusion matrix\n",
      " [[ 7  5  5  5  1]\n",
      " [ 7 12  7 12  2]\n",
      " [ 4  5 24 11  6]\n",
      " [ 0  4 12 11 15]\n",
      " [ 1  3  6 11 26]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.316831683168\n",
      "confusion matrix\n",
      " [[ 5 14  2  0  1]\n",
      " [ 8 23  3  1  0]\n",
      " [ 3 30  8  3  5]\n",
      " [ 0 33  3  6  6]\n",
      " [ 1 19  2  4 22]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.386138613861\n",
      "confusion matrix\n",
      " [[11  7  8  1  1]\n",
      " [11  9 13  3  3]\n",
      " [ 5  3 18 12  4]\n",
      " [ 2  5 14 14 10]\n",
      " [ 1  1  7 13 26]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.381188118812\n",
      "confusion matrix\n",
      " [[ 9  7  9  4  1]\n",
      " [ 4 10  9  7  1]\n",
      " [ 2 12 23 14 10]\n",
      " [ 1  7  9 13 15]\n",
      " [ 0  2  5  6 22]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.391089108911\n",
      "confusion matrix\n",
      " [[11  9  6  2  2]\n",
      " [ 4  9 14  5  2]\n",
      " [ 5  6 20 16  4]\n",
      " [ 1  3 13 13 15]\n",
      " [ 2  2  5  7 26]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.316831683168\n",
      "confusion matrix\n",
      " [[ 7  8  6  1  0]\n",
      " [ 5  8 11  6  2]\n",
      " [ 4 13 13 10  6]\n",
      " [ 2 10 16 12 18]\n",
      " [ 1  4  6  9 24]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.311881188119\n",
      "confusion matrix\n",
      " [[ 9  2  8  1  1]\n",
      " [10  5 11  5  3]\n",
      " [ 3  8 19 17 13]\n",
      " [ 4  4 14 11 15]\n",
      " [ 0  2  8 10 19]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.346534653465\n",
      "confusion matrix\n",
      " [[10  7  8  3  1]\n",
      " [ 4 10 14 11  1]\n",
      " [ 2 10 18  8  9]\n",
      " [ 4  5 16 12 13]\n",
      " [ 2  3  6  5 20]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.40099009901\n",
      "confusion matrix\n",
      " [[ 5 10  8  4  0]\n",
      " [ 7 10 13  5  1]\n",
      " [ 4  6 19  9 10]\n",
      " [ 2  6  6 16 10]\n",
      " [ 2  1  2 15 31]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.331683168317\n",
      "confusion matrix\n",
      " [[ 8 10  4  3  1]\n",
      " [ 4  5  9  6  3]\n",
      " [ 7  8 19 14 13]\n",
      " [ 3  6 12 16 16]\n",
      " [ 0  2  7  7 19]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.346534653465\n",
      "confusion matrix\n",
      " [[ 3 12  4  1  0]\n",
      " [ 8  8 17  4  1]\n",
      " [ 5  9 16 14  5]\n",
      " [ 2  4 13 13 13]\n",
      " [ 1  4  6  9 30]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.30198019802\n",
      "confusion matrix\n",
      " [[14 10  4  0  0]\n",
      " [ 7 23  5  2  1]\n",
      " [ 2 35  4  6  3]\n",
      " [ 1 28  4  5  9]\n",
      " [ 0 14  1  9 15]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.415841584158\n",
      "confusion matrix\n",
      " [[ 8  6 12  1  1]\n",
      " [ 7 11 10  2  1]\n",
      " [ 5  5 25  9  2]\n",
      " [ 0  1 19 17 12]\n",
      " [ 1  2  7 15 23]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.346534653465\n",
      "confusion matrix\n",
      " [[ 8 11  8  6  1]\n",
      " [11  9  7  6  2]\n",
      " [ 5  9 16 13  5]\n",
      " [ 2  4  7 13 16]\n",
      " [ 0  3  4 12 24]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.351485148515\n",
      "confusion matrix\n",
      " [[ 7  5 10  4  2]\n",
      " [13  8  6  6  5]\n",
      " [ 3  5 22 13 12]\n",
      " [ 5  4  6 16 12]\n",
      " [ 1  1  8 10 18]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.40099009901\n",
      "confusion matrix\n",
      " [[ 6  7 10  2  2]\n",
      " [10  9 10  7  2]\n",
      " [ 4  7 21 11  9]\n",
      " [ 4  0  8 19 15]\n",
      " [ 1  0  3  9 26]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.371287128713\n",
      "confusion matrix\n",
      " [[ 8  2  4  2  1]\n",
      " [10 11 11  5  2]\n",
      " [ 8 12 15 11  4]\n",
      " [ 4  3 19 11 14]\n",
      " [ 2  0  8  5 30]]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.40099009901\n",
      "confusion matrix\n",
      " [[11  4  8  2  1]\n",
      " [10  7 11  5  6]\n",
      " [ 2 10 14 18  5]\n",
      " [ 1  5 11 20 15]\n",
      " [ 1  1  3  2 29]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.331683168317\n",
      "confusion matrix\n",
      " [[ 7  8  7  3  3]\n",
      " [ 3 12  8  6  4]\n",
      " [ 6 11  9 17  7]\n",
      " [ 2  4 11 14 21]\n",
      " [ 1  1  3  9 25]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.331683168317\n",
      "confusion matrix\n",
      " [[12  6  6  2  2]\n",
      " [ 7 11 15  4  3]\n",
      " [ 3  8 17 15  7]\n",
      " [ 3  1 12  9 17]\n",
      " [ 1  1  7 15 18]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf_accuracies = []\n",
    "lr_clf_times = []\n",
    "lr_clf_mem = []\n",
    "\n",
    "costs = np.logspace(-3,1)\n",
    "costs.sort()\n",
    "\n",
    "cost_accuracies = []\n",
    "with np.errstate(all='ignore'):\n",
    "    for cost in costs:\n",
    "        mglr = MultiClassLogisticRegression(eta=0.1,iterations=5000, C=cost, optimization=\"BFGSBinaryLogisticRegression\",reg=0) # get object\n",
    "\n",
    "        lr_clf = Pipeline([ ('pca', pca), (\"multiclasslogregression\", mglr)])\n",
    "        # now we can use the cv_object that we setup before to iterate through the \n",
    "        #    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "        #    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "        iter_num=0\n",
    "        # the indices are the rows used for training and testing in each iteration\n",
    "        for train_indices, test_indices in cv_object.split(X,y): \n",
    "            # I will create new variables here so that it is more obvious what \n",
    "            # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "            # but it makes this code less readable)\n",
    "            X_train = (X[train_indices])\n",
    "            y_train = y[train_indices]\n",
    "\n",
    "        #     print(X_train)\n",
    "        #     print(y_train)\n",
    "\n",
    "            X_test = (X[test_indices])\n",
    "            y_test = y[test_indices]\n",
    "\n",
    "            st = time.time()\n",
    "\n",
    "            mem = memory_usage((lr_clf.fit,(X_train,y_train))) # train object\n",
    "            t = (time.time() -st)\n",
    "            lr_clf_times.append(t)\n",
    "            lr_clf_mem.append(mem[0])\n",
    "\n",
    "            # train the reusable logisitc regression model on the training data\n",
    "            y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "            # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "            acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "            lr_clf_accuracies.append(acc)\n",
    "            cost_accuracies.append([acc])\n",
    "\n",
    "            conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "            print(\"====Iteration\",iter_num,\" ====\")\n",
    "            print(\"accuracy\", acc )\n",
    "            print(\"confusion matrix\\n\",conf)\n",
    "            iter_num+=1\n",
    "\n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "c2 = [x for pair in zip(costs,costs,costs) for x in pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "li = [np.arange(1,151)]\n",
    "p = plt.plot(cost_accuracies)\n",
    "plt.title(\"Determining Optimal Value of Regularization Term C\")\n",
    "plt.xlabel('Value of C ')\n",
    "plt.ylabel('Accuracy (%) ')\n",
    "costs_plot = np.around(c2,decimals=2)\n",
    "plt.xticks(li[0],costs_plot, rotation=90)\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_y = max(p[0].get_ydata())\n",
    "d = {}\n",
    "for pair in zip(c2,p[0].get_ydata()):\n",
    "    d[pair[1]] = pair[0]\n",
    "max_x = d[max_y]\n",
    "print(max_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing our Best Logistic Regression Optimization Procedure to that of Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "lr_clf_accuracies = []\n",
    "lr_clf_times = []\n",
    "lr_clf_mem = []\n",
    "lr_clf_iterations = []\n",
    "\n",
    "#For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.381188118812\n",
    "\n",
    "mglr = MultiClassLogisticRegression(eta=0.1,iterations=10, C=max_x, optimization=\"BFGSBinaryLogisticRegression\",reg=0) # get object\n",
    "\n",
    "lr_clf = Pipeline([ ('pca', pca), (\"multiclasslogregression\", mglr)])\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "with np.errstate(all='ignore'):\n",
    "    for train_indices, test_indices in cv_object.split(X,y): \n",
    "        # I will create new variables here so that it is more obvious what \n",
    "        # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "        # but it makes this code less readable)\n",
    "        X_train = (X[train_indices])\n",
    "        y_train = y[train_indices]\n",
    "\n",
    "    #     print(X_train)\n",
    "    #     print(y_train)\n",
    "\n",
    "        X_test = (X[test_indices])\n",
    "        y_test = y[test_indices]\n",
    "\n",
    "        st = time.time()\n",
    "\n",
    "        mem = memory_usage((lr_clf.fit,(X_train,y_train))) # train object\n",
    "        t = (time.time() -st)\n",
    "        lr_clf_times.append(t)\n",
    "        lr_clf_mem.append(mem[0])\n",
    "\n",
    "        # train the reusable logisitc regression model on the training data\n",
    "        y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "        # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "        acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "        lr_clf_accuracies.append(acc)\n",
    "        cost_accuracies.append([acc])\n",
    "\n",
    "        conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "        print(\"====Iteration\",iter_num,\" ====\")\n",
    "        print(\"accuracy\", acc )\n",
    "        print(\"confusion matrix\\n\",conf)\n",
    "        iter_num+=1\n",
    "for x in range(0,15):\n",
    "    lr_clf_iterations.append(2) \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='lbfgs',class_weight='balanced',max_iter=500,C=0.002) \n",
    "\n",
    "lr_sk_accuracies = []\n",
    "lr_sk_times = []\n",
    "lr_sk_mem = []\n",
    "lr_sk_iterations = []\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    st = time.time()\n",
    "    mem = memory_usage((lr_sk.fit,(X_train,y_train)))\n",
    "    #lr_sk.fit(X_train,y_train)\n",
    "    t = (time.time() -st)\n",
    "    lr_sk_times.append(t)\n",
    "    lr_sk_mem.append(mem[0])\n",
    "    #print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "    yhat = lr_sk.predict(X_test)\n",
    "    print(\"Iterations \",lr_sk.n_iter_)\n",
    "    lr_sk_iterations.append(lr_sk.n_iter_)\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    lr_sk_accuracies.append(acc)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "\n",
    "print(lr_sk_times)\n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot([lr_sk_accuracies,lr_clf_accuracies])\n",
    "plt.title(\"Comparing Accuracies\")\n",
    "plt.xlabel('Implementation of Logistic Regression')\n",
    "plt.ylabel('Accuracy Percentage ')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()\n",
    "print((time.time() -st)*100)\n",
    "# ax = fig.add_subplot(111)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot([lr_sk_times,lr_clf_times])\n",
    "plt.title(\"Comparing Training Times\")\n",
    "plt.xlabel('Implementation of Logistic Regression')\n",
    "plt.ylabel('Training Time (seconds) ')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot([lr_sk_mem,lr_clf_mem])\n",
    "plt.title(\"Comparing Memory \")\n",
    "plt.xlabel('Implementation of Logistic Regression')\n",
    "plt.ylabel('Memory Usage (mb) ')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot([lr_sk_iterations,lr_clf_iterations])\n",
    "plt.title(\"Comparing Number of Iterations \")\n",
    "plt.xlabel('Implementation of Logistic Regression')\n",
    "plt.ylabel('Number of Iterations')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing which implementation of Logistic Regression would be best for our case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One idea: Make your implementation of logistic regression compatible with the GridSearchCV function that is part of scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid_input = {'C': costs[:3] }\n",
    "mglr = MultiClassLogisticRegression(eta=eta,iterations=iter_, C=0.02, optimization=\"BFGSBinaryLogisticRegression\")\n",
    "gscv = GridSearchCV(cv= cv_object, estimator=mglr, param_grid= param_grid_input, scoring= \"accuracy\",refit=False)\n",
    "gscv.fit(X,y)\n",
    "print(gscv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
