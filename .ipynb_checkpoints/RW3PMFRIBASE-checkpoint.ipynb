{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "training_classifier = 'Fear of public speaking'\n",
    "\n",
    "df = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cleaned_classifier = df[np.isfinite(df[training_classifier])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change NaN number values to the mean\n",
    "df_imputed = df_cleaned_classifier.fillna(df.median())\n",
    "# get categorical features\n",
    "object_features = list(df_cleaned_classifier.select_dtypes(include=['object']).columns)\n",
    "# one hot encode categorical features\n",
    "one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in object_features], axis=1)\n",
    "# drop object features from imputed dataframe\n",
    "df_imputed_dropped = df_imputed.drop(object_features, 1)\n",
    "frames = [df_imputed_dropped, one_hot_df]\n",
    "# concatenate both frames by columns\n",
    "df_fixed = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if training_classifier in df_fixed:\n",
    "    y = df_fixed[training_classifier].values # get the labels we want\n",
    "    del df_fixed[training_classifier] # get rid of the class label\n",
    "    X = df_fixed.values # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 µs, sys: 0 ns, total: 30 µs\n",
      "Wall time: 32.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "# blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "# blr.fit(X,y)\n",
    "# print(blr)\n",
    "\n",
    "# yhat = blr.predict(X)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 µs, sys: 0 ns, total: 30 µs\n",
      "Wall time: 34.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X + 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "# hlr = HessianBinaryLogisticRegression(eta=0.1,iterations=20,C=0.1) # note that we need only a few iterations here\n",
    "\n",
    "# hlr.fit(X,y)\n",
    "# yhat = hlr.predict(X)\n",
    "# print(hlr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38 µs, sys: 0 ns, total: 38 µs\n",
      "Wall time: 42 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    @staticmethod\n",
    "    def line_search_function(eta,X,y,w,grad):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)+np.sum(wnew**2)\n",
    "    @staticmethod\n",
    "    def line_search_function_l1(eta,X,y,w,grad):\n",
    "        if(math.sin(w) < 0 ):\n",
    "            w -=1\n",
    "        elif(math.sin(w) > 0):\n",
    "            w += 1\n",
    "        else:\n",
    "            w = w\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)+np.sum(math.fabs(wnew))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ += gradient*eta # set new function values\n",
    "                \n",
    "            \n",
    "\n",
    "# lslr = LineSearchLogisticRegression(eta=0.1,iterations=110, C=0.001)\n",
    "\n",
    "# lslr.fit(X,y)\n",
    "\n",
    "# yhat = lslr.predict(X)\n",
    "# print(lslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 µs, sys: 0 ns, total: 26 µs\n",
      "Wall time: 29.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "# slr = StochasticLogisticRegression(0.1,1000, C=0.001) # take a lot more steps!!\n",
    "\n",
    "# slr.fit(X,y)\n",
    "\n",
    "# yhat = slr.predict(X)\n",
    "# print(slr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 µs, sys: 0 ns, total: 36 µs\n",
      "Wall time: 40.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += 2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "# bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "# bfgslr.fit(X,y)\n",
    "# yhat = bfgslr.predict(X)\n",
    "# print(bfgslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, optimization=None):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.optimization = optimization\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            #hblr = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            if(self.optimization == \"BFGSBinaryLogisticRegression\"):\n",
    "                hblr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            elif(self.optimization == \"StochasticLogisticRegression\"):\n",
    "                hblr = StochasticLogisticRegression(self.eta,self.iters,self.C)\n",
    "            else:\n",
    "                hblr = LineSearchLogisticRegression(self.eta,self.iters,self.C)\n",
    "\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.321782178218\n",
      "confusion matrix\n",
      " [[12  5  6  2  0]\n",
      " [11 12 12  2  1]\n",
      " [13 15 29 12  8]\n",
      " [ 3  9 15 10 10]\n",
      " [ 1  1  7  4  2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 1  ====\n",
      "accuracy 0.306930693069\n",
      "confusion matrix\n",
      " [[13  5  7  4  1]\n",
      " [15 10 14  3  0]\n",
      " [11 19 24  8  6]\n",
      " [ 4  9 12 11  8]\n",
      " [ 0  1  9  4  4]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.346534653465\n",
      "confusion matrix\n",
      " [[17 12  8  5  0]\n",
      " [12 13 17  4  2]\n",
      " [ 9 15 28 10  1]\n",
      " [ 2  3 17  8  6]\n",
      " [ 1  1  3  4  4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "\n",
    "lr_clf = MultiClassLogisticRegression(eta=0.1,iterations=2500, C=0.006, optimization=\"BFGSBinaryLogisticRegression\") # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ -7.87442891e-02  -3.51886375e-01  -1.73335952e-01  -2.89984814e-01\n",
      "   -1.29972292e-01  -1.46277457e-01  -1.65262840e-01  -1.97763054e-01\n",
      "   -3.37182584e-01  -2.77579727e-01  -1.40547198e-01  -2.23583764e-01\n",
      "   -1.66319863e-01  -1.07214092e-01  -1.52238082e-01  -2.02552642e-01\n",
      "   -1.53129150e-01  -1.76281332e-01  -2.26810671e-01  -8.80635330e-02\n",
      "   -4.13605616e-01  -1.33928208e-01  -3.70908114e-01  -2.84264685e-01\n",
      "   -3.39353247e-01  -2.75125199e-01  -1.12727234e-01  -3.54437772e-01\n",
      "   -4.45821965e-01  -2.50141277e-01  -1.04536991e-01  -1.70833379e-01\n",
      "    1.52995106e-03  -5.60713151e-02   8.10092592e-02  -1.39767805e-01\n",
      "   -4.58771435e-02  -2.94724798e-01  -7.89450872e-02   6.22534620e-02\n",
      "   -3.98450446e-01  -2.89085953e-01  -2.59208560e-01  -1.51684379e-01\n",
      "   -2.43185701e-01  -3.11962842e-01   2.31222371e-01   6.14182318e-02\n",
      "   -8.87909838e-02  -4.14823171e-02  -3.89888821e-01  -1.12275855e-01\n",
      "   -1.14296181e-02  -1.07929853e-01  -1.95653683e-01   3.10258300e-02\n",
      "   -2.46397023e-01  -3.84200372e-01  -2.08341856e-01  -1.02320621e-01\n",
      "   -9.18558321e-02  -2.55659095e-01   6.52004003e-02  -4.10644650e-01\n",
      "   -4.04622733e-01  -3.30323878e-01  -4.31783552e-01  -4.86772907e-01\n",
      "   -5.16827944e-01  -5.99921128e-01  -4.78200372e-01  -3.20868067e-01\n",
      "   -6.04206616e-01  -1.87636409e-01  -4.72419256e-04  -4.83867303e-02\n",
      "   -2.02162556e-01  -7.92777766e-02  -2.00217582e-01  -2.70112259e-01\n",
      "   -1.76179007e-01  -1.89258824e-01  -2.41729241e-01  -3.13476699e-01\n",
      "   -6.99305244e-02  -3.47021467e-01  -7.23175255e-02  -5.48720955e-01\n",
      "   -5.54059364e-02  -3.56105895e-01  -7.03819030e-02  -2.88638471e-01\n",
      "   -3.09915544e-01  -3.36670692e-01  -1.79232166e-01  -3.47482976e-01\n",
      "   -3.77091310e-01  -5.54728845e-01  -2.35783423e-01  -4.85888074e-01\n",
      "   -5.92407174e-01  -1.48736661e-01  -1.29146413e-01  -1.43160667e-01\n",
      "    2.12735633e-01  -1.52237302e-01   3.13802104e-01  -4.23166089e-01\n",
      "   -1.99773996e-01   1.87526022e-01  -1.37213291e-01  -3.63225087e-01\n",
      "   -1.72948983e-01   1.23576383e-01  -2.80522769e-01   5.07812538e-02\n",
      "   -1.22158057e+00  -3.59555374e-01  -5.19501148e-01  -1.56158995e-02\n",
      "    1.61475725e-01  -1.16955326e-01  -7.39376029e-02  -3.02783104e-01\n",
      "   -3.78311950e-01   1.81511897e-01  -2.32606899e-01  -3.29106355e-01\n",
      "   -3.53429574e-01  -2.22143669e-01  -4.21570472e-02   1.71131206e-02\n",
      "   -9.51401355e-02  -9.78094430e-02  -1.31263576e-01  -1.38468497e+00\n",
      "   -1.26005958e+01  -3.72422831e+00  -5.13162861e-03   5.89359199e-02\n",
      "   -2.11638390e-03  -4.32449710e-02  -8.98293040e-02   2.27605378e-02\n",
      "   -8.34918416e-03  -9.35044724e-02   3.36115160e-02  -8.15359577e-02\n",
      "   -2.90538517e-02   3.17106907e-03   1.27959520e-03  -2.26535378e-02\n",
      "   -6.37286249e-02  -4.01886948e-02   1.21313526e-02  -4.79748246e-02\n",
      "   -2.83837973e-03  -1.35077425e-01   5.69304325e-02   8.45852204e-03\n",
      "   -8.44906889e-02   2.50257326e-03   5.39834941e-03   2.22572178e-04\n",
      "   -7.29138185e-03  -1.39205021e-03  -7.73644828e-02  -7.93691909e-02\n",
      "    2.39089752e-03  -8.93864975e-02   1.43004572e-02  -4.78119867e-02\n",
      "   -2.72740535e-02]\n",
      " [  3.51497553e-02   1.41658259e-01   1.40664325e-01   1.16381536e-01\n",
      "    1.95407404e-01   1.52635297e-01   1.93585874e-01   1.15423497e-01\n",
      "    6.11123085e-02  -5.87821355e-02  -5.74167182e-02  -1.24144194e-01\n",
      "    1.09560282e-01   1.19663325e-01   2.26908611e-01   9.41429355e-02\n",
      "   -8.07061567e-02   2.92935431e-01   1.35349145e-01   1.40803522e-01\n",
      "    1.47007581e-01   1.28735678e-02   9.09895465e-02   1.12680113e-01\n",
      "    1.12106981e-01   1.12130035e-01   1.94742350e-01   2.42618945e-01\n",
      "    1.54703998e-01   1.59142263e-01   2.43330105e-01   1.18659471e-01\n",
      "    1.20037437e-01   9.69811608e-02   3.64591718e-01   1.16929863e-01\n",
      "    1.61687210e-01   5.35793100e-02   1.80947819e-02   1.68018209e-01\n",
      "    8.86296112e-02   7.98342059e-02   9.08955946e-03   7.76858397e-02\n",
      "    2.02428262e-01   1.31810264e-01   1.43957203e-01   5.03310792e-02\n",
      "    1.09513299e-01   1.79838020e-01   2.37503077e-01   2.33916337e-01\n",
      "    1.56151124e-01   8.86873909e-02   5.88696845e-02   1.97872686e-01\n",
      "    7.97055139e-02  -1.15962764e-02   5.80636821e-02   1.29451497e-01\n",
      "    1.75073210e-01   1.62670347e-01   1.24627447e-01   8.44370573e-02\n",
      "   -1.82891304e-02  -3.18729828e-02  -1.13959442e-01  -5.30248519e-02\n",
      "   -9.06224361e-02  -4.45915932e-02  -4.75588025e-02  -9.31338249e-02\n",
      "   -5.87827195e-02   9.61751584e-02   2.04296483e-01   8.34162879e-03\n",
      "    1.32009284e-01   1.42988366e-01   8.90623782e-02   1.10844867e-01\n",
      "    1.30047682e-01   9.72855277e-02   1.99577488e-01   2.48248704e-01\n",
      "    3.44234777e-02   1.75216502e-02  -7.37918129e-02  -2.78073690e-02\n",
      "    2.17074625e-01   6.85291637e-02   1.35549041e-01  -8.94201449e-02\n",
      "    1.17293177e-01   1.66195804e-01   1.49155363e-01   1.37931738e-01\n",
      "    5.94541969e-02   8.97254017e-03   9.00437634e-02  -1.93744023e-02\n",
      "   -7.47259232e-02   2.24783299e-01   9.77171267e-02   7.70776898e-02\n",
      "    1.46784631e-01   1.92067251e-01   2.36837147e-01  -8.99092177e-03\n",
      "    2.21559873e-01   2.14107708e-01   1.78460346e-01   1.69817270e-01\n",
      "    8.40635299e-02   1.99461345e-01   1.02223982e-02   1.32897288e-01\n",
      "   -2.49784541e-01   1.66944318e-01   4.94086416e-02   1.31192193e-01\n",
      "    1.86555095e-01   1.30736081e-01   1.54411596e-01   1.54960214e-01\n",
      "   -2.02968399e-02   2.10755883e-01   6.84705081e-02   9.79619620e-02\n",
      "    1.70772244e-03   1.58265932e-01   9.52057379e-02   1.08707881e-01\n",
      "    8.04890467e-02   2.08043151e-02   1.11546398e-01   7.65905865e-01\n",
      "    6.77494330e+00   2.88251340e+00   4.12896707e-02  -5.12631457e-02\n",
      "   -8.25758374e-03   1.57914582e-02   6.93558836e-02  -1.02080976e-02\n",
      "    3.14892427e-02   5.75874362e-03   4.76661930e-02  -3.30189542e-02\n",
      "    1.80574278e-02  -3.27738269e-02  -1.48331264e-03   2.51588273e-02\n",
      "    4.18029788e-02   6.56419116e-02  -2.52168989e-02  -6.77427110e-03\n",
      "    1.41327627e-03   8.67867799e-03   2.78102889e-02   6.48245286e-03\n",
      "    3.14197903e-02   7.75565637e-03  -9.46075087e-03   3.77262762e-03\n",
      "    8.39794849e-03   1.35955251e-02   7.69758471e-03   3.91751546e-02\n",
      "   -2.21898661e-03   3.68741669e-02   1.97415127e-03   5.35530446e-02\n",
      "   -1.47047264e-02]\n",
      " [ -2.38880757e-02  -1.44816915e-01  -1.14860964e-01  -1.67580855e-01\n",
      "   -7.06162426e-02  -1.01600278e-01  -1.75148278e-01  -1.66399562e-01\n",
      "   -6.72158558e-02  -8.86472883e-03  -7.93421627e-02  -5.37111369e-02\n",
      "   -1.86500253e-01  -3.56882452e-01  -2.16093156e-01  -1.36640731e-01\n",
      "   -2.68793819e-02  -2.66499412e-01  -1.58095002e-01  -2.04894033e-01\n",
      "   -1.82952723e-02  -7.69846160e-02   8.17429075e-02  -1.00375431e-01\n",
      "   -1.03810011e-01  -6.55448179e-02  -1.09740590e-01  -1.99970778e-01\n",
      "   -4.75512811e-02  -5.24031472e-02  -1.09825055e-01  -6.82734521e-02\n",
      "   -2.08371202e-01  -2.04942982e-01  -2.18919332e-01  -6.90916961e-02\n",
      "   -1.12740733e-01  -8.79088713e-02  -1.30018606e-01  -2.16070360e-01\n",
      "    2.82128119e-02   4.81703692e-02  -7.54503509e-02  -5.84996543e-02\n",
      "   -2.04367092e-01  -2.52864490e-02  -1.74110119e-01  -9.68281960e-02\n",
      "   -1.98496504e-01  -2.16558427e-01  -6.68384082e-02  -1.62091072e-01\n",
      "   -1.70637987e-01  -1.59684576e-01   5.88827484e-02  -2.38844375e-01\n",
      "   -3.02586525e-02   5.53814612e-02  -6.29968683e-02  -1.06758204e-01\n",
      "   -2.45276097e-01  -1.34120614e-01  -2.19822398e-01  -6.00811895e-02\n",
      "    2.30290891e-02   8.98848984e-02   5.91660125e-02   7.90307087e-02\n",
      "   -4.41636145e-02   4.83557346e-02   5.88798942e-03  -6.30049076e-02\n",
      "   -9.77415463e-03  -1.22750525e-01  -1.41437288e-01  -1.17545089e-05\n",
      "   -1.23596602e-01  -1.81358762e-01  -4.16954489e-02  -3.50699653e-02\n",
      "   -1.58591464e-01  -1.33873223e-01  -1.34468550e-01  -1.63176502e-01\n",
      "   -8.41876687e-02  -3.99201522e-02  -1.78774940e-01   8.54721494e-02\n",
      "    2.10308754e-02  -9.25361412e-02  -1.12615715e-01  -4.08540527e-02\n",
      "   -7.78129353e-02  -1.15768710e-01  -1.25109751e-01  -9.14249146e-02\n",
      "   -1.33733406e-02  -9.51930305e-02  -1.84845650e-01   4.79652100e-02\n",
      "    9.37425169e-02  -1.65455943e-01  -1.16318803e-01   1.70968312e-02\n",
      "   -1.44554767e-01   2.08227146e-02  -2.75261204e-01  -9.31023126e-02\n",
      "   -1.75896813e-01  -2.14425476e-01  -2.09928262e-01  -1.82906104e-01\n",
      "   -1.18401482e-01  -2.78266386e-01  -9.17322966e-02  -1.32524323e-01\n",
      "    2.27940523e-01  -1.56285869e-01  -3.34689931e-02  -1.37444576e-01\n",
      "   -1.73882521e-01  -5.73639530e-02  -1.16875256e-01  -2.43086368e-02\n",
      "   -5.02326452e-02  -2.24187876e-01  -1.03697713e-01  -8.11580126e-02\n",
      "    8.76654469e-02  -1.12334130e-01  -6.71360717e-02  -1.45155132e-01\n",
      "   -2.22618305e-02  -5.56717989e-02  -1.98122414e-01  -6.59841387e-01\n",
      "   -4.39750181e+00  -1.57449653e+00  -5.00777578e-02  -7.98074337e-03\n",
      "   -1.49478021e-02  -2.03903143e-02   2.57074515e-02  -4.31837663e-02\n",
      "   -1.96776516e-02   4.12767345e-02  -4.37744124e-02   7.28416974e-02\n",
      "   -5.03737420e-02  -8.45909134e-03  -1.22761566e-02  -1.32231340e-02\n",
      "    1.26519250e-02  -2.87843777e-02  -1.82865194e-02   2.28373637e-02\n",
      "    2.78226441e-04  -2.01137671e-02   4.10501023e-03  -2.74793897e-02\n",
      "   -5.25714683e-03   4.07249625e-03   3.76175582e-03  -2.37062362e-03\n",
      "   -2.22483968e-02  -4.21814794e-02   3.63353657e-02  -3.77771265e-02\n",
      "    1.22191680e-02  -9.54116234e-03  -1.33679461e-02  -5.25231279e-02\n",
      "    2.11110163e-02]\n",
      " [  4.71354828e-04  -1.50693922e-02  -4.45532797e-02   7.15985558e-02\n",
      "   -1.20066311e-01  -4.22605374e-02  -5.13396785e-02   1.09335081e-01\n",
      "    1.23464808e-01   5.92374938e-02   1.17019019e-01   1.62765743e-01\n",
      "    1.35825110e-01   1.45398870e-01   1.31076759e-03   1.49124338e-01\n",
      "    1.03260239e-02   2.18463063e-02   4.56151847e-02   8.52982518e-02\n",
      "   -5.69757852e-02  -1.50518013e-02  -3.90440027e-02  -7.62309157e-02\n",
      "   -2.53607101e-03   5.67390219e-02  -1.17680950e-01  -4.73398574e-02\n",
      "   -5.45195771e-02  -6.68733835e-02  -9.95841239e-02  -8.16763092e-02\n",
      "   -9.75347149e-02  -2.13994656e-02  -2.95434715e-01  -8.03297094e-02\n",
      "   -2.11114241e-02   2.46665434e-03   4.48845667e-02  -1.85407230e-01\n",
      "    8.20099330e-02   3.28719584e-02   4.29363181e-02  -6.25272319e-02\n",
      "    9.42700871e-03   6.94334747e-02  -1.94753713e-01  -7.92704422e-03\n",
      "   -7.88850841e-02  -1.50453212e-01  -1.52135701e-01  -9.24635006e-02\n",
      "   -3.12428296e-02   1.09467382e-02  -1.11449860e-01  -5.81324732e-02\n",
      "    1.61224590e-02   1.11119556e-01  -5.47932405e-02   1.69613314e-03\n",
      "   -3.24180362e-02  -7.73446158e-02  -3.88173782e-02   1.98598855e-01\n",
      "    1.40229377e-01   9.06980899e-02   2.12132569e-01   2.01722058e-01\n",
      "    3.10356582e-01   2.96608274e-01   2.79639379e-01   1.94264681e-01\n",
      "    2.47743957e-01   2.53507945e-02  -2.13460772e-01  -4.34340081e-02\n",
      "   -4.36606084e-02  -3.46994429e-02  -1.06107313e-01  -4.22653513e-02\n",
      "   -2.26303806e-02  -5.05698291e-02   1.58368720e-02  -5.59107945e-02\n",
      "   -4.51650838e-02   1.43345417e-01   1.25810128e-01   9.51950412e-02\n",
      "   -3.52598769e-01  -2.42418377e-02  -1.94212796e-01   1.76962389e-01\n",
      "   -5.23496737e-02   7.44449146e-02  -2.07301663e-02   7.84918882e-02\n",
      "    2.41175273e-02   2.32830161e-01   9.60805576e-02   2.07559955e-01\n",
      "    4.65556397e-02  -2.23035388e-01  -8.45681869e-02  -7.87693745e-02\n",
      "   -2.13240614e-01  -1.96816632e-01  -2.80583113e-01   1.61311361e-01\n",
      "   -7.59445784e-02  -1.83964921e-01   5.59611206e-02   1.32094782e-01\n",
      "   -4.54073134e-03  -6.46460117e-02   1.05826584e-01  -1.57254019e-01\n",
      "    4.67461589e-01   6.64195642e-02   8.47891443e-02  -9.98210782e-02\n",
      "   -1.90202466e-01  -1.29385066e-02  -1.03657480e-01  -1.04655484e-01\n",
      "    1.07101440e-01  -2.29604743e-01   2.00059180e-02   2.75238828e-02\n",
      "    3.29197915e-02  -1.41097590e-02  -8.67363988e-02  -5.71854826e-02\n",
      "   -6.54215677e-02   1.11605731e-02   2.72383027e-02   7.16170758e-03\n",
      "   -2.32472936e-01  -5.93813050e-01  -3.98043281e-02   1.93102283e-02\n",
      "    7.79000080e-03   1.12402407e-02  -4.02973301e-02   2.27604886e-02\n",
      "    1.14636171e-02  -2.91129356e-02  -2.08738086e-02  -1.71443703e-02\n",
      "    3.53377637e-02   3.78434604e-02  -8.62682111e-03   2.11245070e-03\n",
      "   -2.90563003e-02  -2.53292351e-02   2.20377404e-02   6.51041125e-03\n",
      "   -2.83837994e-03   2.83781529e-02  -3.22272657e-02   2.33181203e-02\n",
      "   -2.00992036e-02  -2.72167100e-02   5.39834500e-03   2.22572174e-04\n",
      "    2.73810450e-02   1.34675413e-02  -1.79261301e-02   9.78834521e-03\n",
      "   -1.24687604e-02  -2.28959495e-04  -5.51240746e-03   1.71995796e-03\n",
      "   -2.50811872e-03]\n",
      " [ -6.49415926e-02  -2.44719454e-01  -2.48289730e-01  -1.25090400e-01\n",
      "   -1.96244505e-01  -1.48684113e-01  -1.91001541e-01  -2.14590008e-01\n",
      "   -2.33192044e-01  -1.92054471e-01  -1.24140413e-01  -4.72620013e-02\n",
      "   -2.63440856e-01  -1.21144041e-01  -2.24396534e-01  -3.12162342e-01\n",
      "   -9.75824058e-02  -2.50770296e-01  -9.49531594e-02  -2.05019643e-01\n",
      "   -2.81468837e-01  -1.39389262e-01  -2.33966718e-01  -2.38708056e-01\n",
      "   -1.18920346e-01  -2.44394337e-01  -2.84368434e-01  -1.32802677e-01\n",
      "   -1.13447429e-01  -2.85862046e-01  -2.33381275e-01  -2.70094943e-01\n",
      "   -2.20522534e-01  -2.06716563e-01  -3.00191400e-01  -1.39934083e-01\n",
      "   -2.65614017e-01  -2.10374400e-01  -2.47077914e-01  -1.65506535e-01\n",
      "   -1.67054756e-01  -1.77148342e-01  -1.14896590e-01  -2.11232958e-01\n",
      "   -2.58196542e-01  -2.12139845e-01  -3.03135564e-01  -3.58745944e-01\n",
      "   -6.16610847e-02  -5.80502258e-02  -1.33912576e-01  -2.02230455e-01\n",
      "   -2.48373537e-01  -6.53559029e-02  -2.74328301e-01  -3.61580825e-01\n",
      "   -7.33818075e-02  -8.32112687e-02  -1.56522003e-01  -3.54587697e-01\n",
      "   -1.88678425e-01  -2.95791736e-01  -3.05763290e-01  -2.53967133e-01\n",
      "   -4.55454228e-03  -8.37039734e-02  -4.06300921e-03  -8.27449536e-02\n",
      "    2.67607671e-03  -9.18264053e-02  -6.32108087e-02  -2.19449013e-02\n",
      "    5.00842059e-02  -2.03026805e-01  -2.63749531e-01  -2.68036096e-01\n",
      "   -1.65116706e-01  -2.34459443e-01  -1.93372828e-01  -1.19880444e-01\n",
      "   -2.73060684e-01  -2.37369314e-01  -2.05248205e-01  -2.22454413e-01\n",
      "   -2.58973812e-01  -4.42962283e-02  -9.55452080e-02  -2.29217053e-02\n",
      "   -3.20134751e-01  -5.12805808e-02  -2.84053426e-01   2.06317765e-02\n",
      "   -1.84879546e-01  -8.49409932e-02  -2.17917566e-01  -3.06092498e-01\n",
      "   -2.23846489e-01   5.62252004e-02  -2.38893515e-01  -1.78070571e-01\n",
      "    1.53619908e-01  -1.24598760e-01  -1.94379970e-01  -1.63832509e-01\n",
      "   -4.49297170e-01  -2.51674528e-01  -4.52754334e-01  -3.68052120e-02\n",
      "   -2.48236449e-01  -4.25556334e-01  -2.72782627e-01  -1.52549000e-01\n",
      "   -2.56674883e-01  -4.36237964e-01  -1.20755677e-01  -3.55257052e-01\n",
      "    3.45314857e-01  -1.68753034e-01   3.03936341e-02  -3.67178340e-01\n",
      "   -4.71540512e-01  -3.51079564e-01  -2.99903175e-01  -1.16807963e-01\n",
      "   -1.10532438e-01  -4.08289053e-01  -1.71025236e-01  -7.23440405e-02\n",
      "   -1.77736608e-01  -2.41038884e-01  -3.07182083e-01  -3.41607081e-01\n",
      "   -3.18227198e-01  -2.50537806e-01  -2.61268979e-01  -1.41884497e+00\n",
      "   -1.26626731e+01  -5.94729476e+00  -1.15527815e-01  -3.46828735e-02\n",
      "   -7.98868841e-04   1.15884361e-02  -3.66426128e-02  -1.19212248e-02\n",
      "   -3.40897242e-02  -1.62281863e-02  -7.09140007e-02   1.14602275e-02\n",
      "   -4.48857887e-03  -1.09153023e-02   1.77294740e-02  -2.87984283e-02\n",
      "   -4.19580955e-02  -7.48602608e-02  -1.10878223e-03   7.40730151e-03\n",
      "    3.48391189e-03   4.71781715e-02  -1.19223825e-01  -2.10317433e-02\n",
      "   -4.23428698e-02  -1.66180754e-02  -5.67739145e-03  -2.83869401e-03\n",
      "   -1.48640797e-02   1.16392752e-02  -3.61511253e-02  -2.86663512e-02\n",
      "   -3.52760008e-02  -3.31054044e-02  -3.48885983e-02  -3.17641278e-02\n",
      "   -3.10427453e-02]]\n",
      "Accuracy of:  0.230693069307\n",
      "CPU times: user 1.18 s, sys: 54.5 ms, total: 1.24 s\n",
      "Wall time: 323 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear boundaries visualization from sklearn documentation\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_decision_boundaries(lr,Xin,y,title=''):\n",
    "    Xb = copy.deepcopy(Xin)\n",
    "    lr.fit(Xb[:,:2],y) # train only on two features\n",
    "\n",
    "    h=0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "    y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb[:, 0], Xb[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Fear of Public Speaking')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 µs, sys: 0 ns, total: 13 µs\n",
      "Wall time: 7.87 µs\n",
      "For  BFGSBinaryLogisticRegression  and cost  0.0  Accuracy of:  0.615841584158\n",
      "CPU times: user 17 µs, sys: 1e+03 ns, total: 18 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  BFGSBinaryLogisticRegression  and cost  0.002  Accuracy of:  0.616831683168\n",
      "CPU times: user 10 µs, sys: 1e+03 ns, total: 11 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  BFGSBinaryLogisticRegression  and cost  0.004  Accuracy of:  0.611881188119\n",
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  BFGSBinaryLogisticRegression  and cost  0.006  Accuracy of:  0.609900990099\n",
      "CPU times: user 12 µs, sys: 1 µs, total: 13 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  BFGSBinaryLogisticRegression  and cost  0.008  Accuracy of:  0.60297029703\n",
      "CPU times: user 8 µs, sys: 1e+03 ns, total: 9 µs\n",
      "Wall time: 5.01 µs\n",
      "For  StochasticLogisticRegression  and cost  0.0  Accuracy of:  0.175247524752\n",
      "CPU times: user 13 µs, sys: 0 ns, total: 13 µs\n",
      "Wall time: 5.01 µs\n",
      "For  StochasticLogisticRegression  and cost  0.002  Accuracy of:  0.175247524752\n",
      "CPU times: user 8 µs, sys: 1 µs, total: 9 µs\n",
      "Wall time: 4.05 µs\n",
      "For  StochasticLogisticRegression  and cost  0.004  Accuracy of:  0.305940594059\n",
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 5.01 µs\n",
      "For  StochasticLogisticRegression  and cost  0.006  Accuracy of:  0.230693069307\n",
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 4.05 µs\n",
      "For  StochasticLogisticRegression  and cost  0.008  Accuracy of:  0.305940594059\n",
      "CPU times: user 12 µs, sys: 1 µs, total: 13 µs\n",
      "Wall time: 5.01 µs\n",
      "For  LineSearchLogisticRegression  and cost  0.0  Accuracy of:  0.230693069307\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  LineSearchLogisticRegression  and cost  0.002  Accuracy of:  0.230693069307\n",
      "CPU times: user 15 µs, sys: 1 µs, total: 16 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  LineSearchLogisticRegression  and cost  0.004  Accuracy of:  0.230693069307\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  LineSearchLogisticRegression  and cost  0.006  Accuracy of:  0.230693069307\n",
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  LineSearchLogisticRegression  and cost  0.008  Accuracy of:  0.230693069307\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "costs = [n for n in np.arange(0,0.01,0.002)]\n",
    "optimizations = [\"BFGSBinaryLogisticRegression\",\"StochasticLogisticRegression\",\"LineSearchLogisticRegression\"]\n",
    "\n",
    "for optimization in optimizations:\n",
    "    for cost in costs:\n",
    "        %%time\n",
    "        lr = MultiClassLogisticRegression(eta=0.1,\n",
    "                                           iterations=10,\n",
    "                                           C=cost,optimization=optimization) # get object\n",
    "        lr.fit(X,y)\n",
    "#         print(lr)\n",
    "        yhat = lr.predict(X)\n",
    "        print('For ',optimization,' and cost ', cost,' Accuracy of: ',accuracy_score(y,yhat+1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.610891089109\n",
      "CPU times: user 756 ms, sys: 46.7 ms, total: 802 ms\n",
      "Wall time: 213 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='lbfgs')#,max_iter=100,C=0.005) \n",
    "lr_sk.fit(X,y)\n",
    "#print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.212871287129\n",
      "confusion matrix\n",
      " [[ 0  0  0  0  0  0]\n",
      " [ 5 10 15  8  1  0]\n",
      " [14 11 20  4  4  0]\n",
      " [12 11 23 10  3  0]\n",
      " [ 5  6 11  7  3  0]\n",
      " [ 5  6  4  2  2  0]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.193069306931\n",
      "confusion matrix\n",
      " [[ 0  0  0  0  0  0]\n",
      " [ 5  9 15  5  3  0]\n",
      " [12 10 19  5  1  0]\n",
      " [10 17 23  9  6  0]\n",
      " [10  7 11  9  2  0]\n",
      " [ 4  1  5  3  1  0]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.168316831683\n",
      "confusion matrix\n",
      " [[ 0  0  0  0  0  0]\n",
      " [11  6  9  5  2  0]\n",
      " [ 7 12 18  8  1  0]\n",
      " [14 15 24  8  8  0]\n",
      " [ 7  6 13  7  2  0]\n",
      " [ 2  5  9  3  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='lbfgs')#,max_iter=100,C=0.005) \n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_sk.fit(X_train,y_train)\n",
    "    #print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "    yhat = lr_sk.predict(X_test)\n",
    " \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
