{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "target_classifier = 'Fear of public speaking'\n",
    "\n",
    "df = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove rows whose target classfier value is NaN\n",
    "df_cleaned_classifier = df[np.isfinite(df[target_classifier])]\n",
    "# change NaN number values to the mean\n",
    "df_imputed = df_cleaned_classifier.fillna(df.mean())\n",
    "# get categorical features\n",
    "object_features = list(df_cleaned_classifier.select_dtypes(include=['object']).columns)\n",
    "# one hot encode categorical features\n",
    "one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in object_features], axis=1)\n",
    "# drop object features from imputed dataframe\n",
    "df_imputed_dropped = df_imputed.drop(object_features, 1)\n",
    "frames = [df_imputed_dropped, one_hot_df]\n",
    "# concatenate both frames by columns\n",
    "df_fixed = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "We are trying to predict the likeliness of a youth having a fear of public speaking. We first removed any rows that contained NaN values for our target classifier. Afterwards, we imputed any NaN numerical values by using the feature's median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if target_classifier in df_fixed:\n",
    "    y = df_fixed[target_classifier].values # get the labels we want\n",
    "    del df_fixed[target_classifier] # get rid of the class label\n",
    "    X = df_fixed.values # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46 µs, sys: 1 µs, total: 47 µs\n",
      "Wall time: 52 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate\n",
    "\n",
    "# blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "# blr.fit(X,y)\n",
    "# print(blr)\n",
    "\n",
    "# yhat = blr.predict(X)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 µs, sys: 1 µs, total: 53 µs\n",
      "Wall time: 57.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X + 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "# hlr = HessianBinaryLogisticRegression(eta=0.1,iterations=20,C=0.1) # note that we need only a few iterations here\n",
    "\n",
    "# hlr.fit(X,y)\n",
    "# yhat = hlr.predict(X)\n",
    "# print(hlr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 83 µs, sys: 1e+03 ns, total: 84 µs\n",
      "Wall time: 89.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    @staticmethod\n",
    "    def line_search_function(eta,X,y,w,grad):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)+np.sum(wnew**2)\n",
    "    @staticmethod\n",
    "    def line_search_function_l1(eta,X,y,w,grad):\n",
    "        if(math.sin(w) < 0 ):\n",
    "            w -=1\n",
    "        elif(math.sin(w) > 0):\n",
    "            w += 1\n",
    "        else:\n",
    "            w = w\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)+np.sum(math.fabs(wnew))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ += gradient*eta # set new function values\n",
    "                \n",
    "            \n",
    "\n",
    "# lslr = LineSearchLogisticRegression(eta=0.1,iterations=110, C=0.001)\n",
    "\n",
    "# lslr.fit(X,y)\n",
    "\n",
    "# yhat = lslr.predict(X)\n",
    "# print(lslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42 µs, sys: 0 ns, total: 42 µs\n",
      "Wall time: 47 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "# slr = StochasticLogisticRegression(0.1,1000, C=0.001) # take a lot more steps!!\n",
    "\n",
    "# slr.fit(X,y)\n",
    "\n",
    "# yhat = slr.predict(X)\n",
    "# print(slr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55 µs, sys: 0 ns, total: 55 µs\n",
      "Wall time: 61 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += 2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        test = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False, retall=True)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "        \n",
    "        print(test)\n",
    "            \n",
    "# bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "# bfgslr.fit(X,y)\n",
    "# yhat = bfgslr.predict(X)\n",
    "# print(bfgslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, optimization=None):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.optimization = optimization\n",
    "\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            #hblr = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            if(self.optimization == \"BFGSBinaryLogisticRegression\"):\n",
    "                hblr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            elif(self.optimization == \"StochasticLogisticRegression\"):\n",
    "                hblr = StochasticLogisticRegression(self.eta,self.iters,self.C)\n",
    "            else:\n",
    "                hblr = LineSearchLogisticRegression(self.eta,self.iters,self.C)\n",
    "\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.351485148515\n",
      "confusion matrix\n",
      " [[18 11  5  5  1]\n",
      " [11 15 13  3  0]\n",
      " [ 5 20 24 16  5]\n",
      " [ 2  7 13 10  1]\n",
      " [ 2  1  7  3  4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 1  ====\n",
      "accuracy 0.287128712871\n",
      "confusion matrix\n",
      " [[ 8  7 11  4  0]\n",
      " [ 9 14 19  7  0]\n",
      " [10 14 22 15  2]\n",
      " [ 5  4 15 13  3]\n",
      " [ 1  0 10  8  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 2  ====\n",
      "accuracy 0.336633663366\n",
      "confusion matrix\n",
      " [[11 11 13  5  0]\n",
      " [11 15 20  5  1]\n",
      " [ 6 10 26 15  4]\n",
      " [ 3  4 11 13  6]\n",
      " [ 0  1  4  4  3]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "\n",
    "lr_clf = MultiClassLogisticRegression(eta=0.1,iterations=2500, C=0.006, optimization=\"BFGSBinaryLogisticRegression\") # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ -7.78757804e-02  -3.46738012e-01  -1.70607483e-01  -2.89754433e-01\n",
      "   -1.21009396e-01  -1.42942001e-01  -1.62947126e-01  -1.94666877e-01\n",
      "   -3.35068687e-01  -2.76735355e-01  -1.37182016e-01  -2.21791174e-01\n",
      "   -1.66085863e-01  -1.07130260e-01  -1.51268043e-01  -2.01340997e-01\n",
      "   -1.52897484e-01  -1.73201966e-01  -2.26654949e-01  -8.28549763e-02\n",
      "   -4.10916397e-01  -1.33746128e-01  -3.65956572e-01  -2.78374978e-01\n",
      "   -3.33504031e-01  -2.69755287e-01  -1.08398652e-01  -3.50583483e-01\n",
      "   -4.45764515e-01  -2.44720963e-01  -1.01651717e-01  -1.67686892e-01\n",
      "    5.87405767e-03  -5.51490079e-02   8.33666300e-02  -1.39910088e-01\n",
      "   -4.51589410e-02  -2.89203391e-01  -7.88516492e-02   6.17704804e-02\n",
      "   -3.98380164e-01  -2.87248545e-01  -2.55792622e-01  -1.46161689e-01\n",
      "   -2.40634961e-01  -3.09315775e-01   2.34049622e-01   6.74006074e-02\n",
      "   -8.45707219e-02  -3.48559981e-02  -3.86260234e-01  -1.06674283e-01\n",
      "   -7.01255215e-03  -9.94851370e-02  -1.90852980e-01   3.45143304e-02\n",
      "   -2.43422267e-01  -3.80630474e-01  -2.04337286e-01  -9.78562332e-02\n",
      "   -8.71531712e-02  -2.50448496e-01   6.63587984e-02  -4.12285601e-01\n",
      "   -4.03946346e-01  -3.25890591e-01  -4.27951574e-01  -4.78163987e-01\n",
      "   -5.12643636e-01  -5.95780377e-01  -4.73072877e-01  -3.17000781e-01\n",
      "   -5.99899176e-01  -1.85071465e-01   2.98482271e-03  -4.47253480e-02\n",
      "   -1.99345137e-01  -7.84101697e-02  -2.00645491e-01  -2.65058090e-01\n",
      "   -1.71083569e-01  -1.84698872e-01  -2.36130154e-01  -3.09689723e-01\n",
      "   -6.68155888e-02  -3.43701033e-01  -7.05578052e-02  -5.46158460e-01\n",
      "   -5.00107613e-02  -3.52764340e-01  -6.67425886e-02  -2.89540221e-01\n",
      "   -3.04818304e-01  -3.34163167e-01  -1.75485709e-01  -3.43043715e-01\n",
      "   -3.73710605e-01  -5.52330431e-01  -2.33794342e-01  -4.81871776e-01\n",
      "   -5.89115843e-01  -1.44721874e-01  -1.26433215e-01  -1.38863481e-01\n",
      "    2.15787664e-01  -1.48615913e-01   3.17948342e-01  -4.20441939e-01\n",
      "   -1.97982189e-01   1.91522568e-01  -1.34438561e-01  -3.60795098e-01\n",
      "   -1.68844479e-01   1.25065811e-01  -2.78016081e-01   5.46461465e-02\n",
      "   -1.21904434e+00  -3.58382500e-01  -5.17265109e-01  -1.16755470e-02\n",
      "    1.66210882e-01  -1.14261443e-01  -7.22767931e-02  -2.99756223e-01\n",
      "   -3.73913872e-01   1.83582786e-01  -2.30499836e-01  -3.25638620e-01\n",
      "   -3.50770037e-01  -2.19019041e-01  -3.85064050e-02   1.75043925e-02\n",
      "   -9.31153624e-02  -9.50651873e-02  -1.28014040e-01  -1.36000817e+00\n",
      "   -1.24444114e+01  -3.67575402e+00  -5.35240887e-03   5.89943302e-02\n",
      "   -1.17141714e-03  -4.32878303e-02  -8.99183321e-02   2.27830954e-02\n",
      "   -8.35745887e-03  -9.26500786e-02   3.45918920e-02  -8.16167664e-02\n",
      "   -2.90826464e-02   3.17421186e-03   1.28086338e-03  -2.26759893e-02\n",
      "   -6.28447209e-02  -3.92814608e-02   1.21433758e-02  -4.80223715e-02\n",
      "   -2.84119279e-03  -1.35211297e-01   5.79339194e-02   8.46690511e-03\n",
      "   -8.36273617e-02   2.50505351e-03   5.40369961e-03   2.22792765e-04\n",
      "   -7.29860820e-03  -1.39342985e-03  -7.64940929e-02  -7.94478522e-02\n",
      "    3.34033136e-03  -8.85280224e-02   1.43146301e-02  -4.78593723e-02\n",
      "   -2.63540200e-02]\n",
      " [  3.61311421e-02   1.46154410e-01   1.43473275e-01   1.17496888e-01\n",
      "    1.98969795e-01   1.55043382e-01   1.96722888e-01   1.18829887e-01\n",
      "    6.13725120e-02  -5.65407748e-02  -5.75535593e-02  -1.24834606e-01\n",
      "    1.10954769e-01   1.20295414e-01   2.29444434e-01   9.42447363e-02\n",
      "   -8.01619246e-02   2.95919305e-01   1.37068381e-01   1.45545795e-01\n",
      "    1.52439564e-01   1.42241699e-02   9.63985759e-02   1.18962888e-01\n",
      "    1.16231063e-01   1.16762023e-01   2.00038262e-01   2.46175457e-01\n",
      "    1.55505103e-01   1.62185546e-01   2.46471342e-01   1.24389625e-01\n",
      "    1.24498757e-01   9.85465930e-02   3.67230018e-01   1.18466194e-01\n",
      "    1.62611084e-01   5.77009889e-02   2.00265153e-02   1.67640079e-01\n",
      "    9.43784332e-02   8.11032326e-02   1.26593103e-02   8.17911844e-02\n",
      "    2.05576286e-01   1.34534393e-01   1.47790224e-01   5.63021443e-02\n",
      "    1.16025204e-01   1.85137649e-01   2.41973050e-01   2.37571331e-01\n",
      "    1.60734035e-01   9.22246367e-02   6.31052177e-02   2.00082372e-01\n",
      "    8.62273791e-02  -9.45256372e-03   6.23320523e-02   1.33980998e-01\n",
      "    1.79792761e-01   1.66412748e-01   1.25624924e-01   8.98808709e-02\n",
      "   -1.75351636e-02  -2.71441666e-02  -1.09812706e-01  -5.00915394e-02\n",
      "   -8.66347628e-02  -3.99007234e-02  -4.40332446e-02  -8.90410638e-02\n",
      "   -5.41462726e-02   9.92917051e-02   2.07480749e-01   1.23077460e-02\n",
      "    1.35099161e-01   1.44095938e-01   8.89211813e-02   1.13535653e-01\n",
      "    1.34845621e-01   1.02129210e-01   2.03134653e-01   2.50710163e-01\n",
      "    3.74357704e-02   2.12031758e-02  -7.50291207e-02  -2.58483927e-02\n",
      "    2.22739554e-01   7.25868882e-02   1.39465389e-01  -8.62409741e-02\n",
      "    1.22212217e-01   1.69201555e-01   1.52921875e-01   1.42872125e-01\n",
      "    6.32673498e-02   1.14486432e-02   9.48887777e-02  -1.38289053e-02\n",
      "   -7.09217401e-02   2.29168048e-01   1.00655010e-01   8.20366521e-02\n",
      "    1.49771145e-01   1.93240422e-01   2.40906898e-01  -6.03792683e-03\n",
      "    2.24051534e-01   2.17234933e-01   1.81547775e-01   1.73026694e-01\n",
      "    8.68107645e-02   2.01363737e-01   1.30170857e-02   1.36843357e-01\n",
      "   -2.50355334e-01   1.68638912e-01   5.22083665e-02   1.35277829e-01\n",
      "    1.88198235e-01   1.33694137e-01   1.57542341e-01   1.57349148e-01\n",
      "   -1.55442000e-02   2.13170808e-01   7.08758358e-02   9.97122007e-02\n",
      "    4.48274216e-03   1.61767395e-01   9.92076356e-02   1.09189910e-01\n",
      "    8.26131615e-02   2.36659720e-02   1.17230045e-01   7.90257941e-01\n",
      "    6.96272749e+00   2.97509618e+00   4.09058092e-02  -5.13139516e-02\n",
      "   -7.31875495e-03   1.58071088e-02   6.94246209e-02  -1.02182146e-02\n",
      "    3.15204510e-02   6.71146372e-03   4.86604467e-02  -3.30516787e-02\n",
      "    1.80753241e-02  -3.28063084e-02  -1.48478273e-03   2.51837617e-02\n",
      "    4.27914216e-02   6.66539808e-02  -2.52418909e-02  -6.78098494e-03\n",
      "    1.41467694e-03   8.68727925e-03   2.87848638e-02   6.48887749e-03\n",
      "    3.23979425e-02   7.76334284e-03  -9.47012724e-03   3.77636660e-03\n",
      "    8.40627153e-03   1.36089994e-02   8.65222636e-03   3.92139804e-02\n",
      "   -1.27417308e-03   3.78577249e-02   1.97610782e-03   5.36061200e-02\n",
      "   -1.37722873e-02]\n",
      " [ -2.68406600e-02  -1.59685000e-01  -1.21705428e-01  -1.70313333e-01\n",
      "   -8.72488721e-02  -1.10783689e-01  -1.83891382e-01  -1.75736372e-01\n",
      "   -7.03595828e-02  -1.19360856e-02  -8.22501045e-02  -5.37676494e-02\n",
      "   -1.89521378e-01  -3.59008953e-01  -2.18349461e-01  -1.40417468e-01\n",
      "   -2.89743787e-02  -2.74552140e-01  -1.62884867e-01  -2.19932248e-01\n",
      "   -2.98892706e-02  -7.94437616e-02   6.53678437e-02  -1.15265571e-01\n",
      "   -1.16532705e-01  -8.05595347e-02  -1.24249372e-01  -2.13823562e-01\n",
      "   -4.96871282e-02  -6.63587367e-02  -1.19393290e-01  -8.37342381e-02\n",
      "   -2.22894977e-01  -2.08404277e-01  -2.26181021e-01  -7.19969461e-02\n",
      "   -1.15489771e-01  -1.02081370e-01  -1.33582147e-01  -2.17339151e-01\n",
      "    2.28499004e-02   4.24960135e-02  -8.64780709e-02  -7.34911132e-02\n",
      "   -2.12833028e-01  -3.31291606e-02  -1.83412912e-01  -1.12578691e-01\n",
      "   -2.13014684e-01  -2.32509072e-01  -7.87989366e-02  -1.78738153e-01\n",
      "   -1.84507434e-01  -1.81662799e-01   4.48314289e-02  -2.46550420e-01\n",
      "   -4.94971009e-02   4.56863087e-02  -7.84430810e-02  -1.20387414e-01\n",
      "   -2.60008969e-01  -1.47392706e-01  -2.22984618e-01  -6.82342452e-02\n",
      "    2.01390824e-02   7.53595698e-02   4.60455656e-02   6.46306182e-02\n",
      "   -5.91851121e-02   3.37537716e-02  -8.64211716e-03  -7.84871905e-02\n",
      "   -2.44902272e-02  -1.31653263e-01  -1.50424878e-01  -9.38360339e-03\n",
      "   -1.32840792e-01  -1.84475974e-01  -4.63137189e-02  -4.79912939e-02\n",
      "   -1.72650001e-01  -1.48638708e-01  -1.48946696e-01  -1.73192322e-01\n",
      "   -9.21201815e-02  -5.18530626e-02  -1.88206512e-01   7.86479110e-02\n",
      "    9.16891135e-03  -1.05140199e-01  -1.24378709e-01  -5.45235489e-02\n",
      "   -9.34072658e-02  -1.24673379e-01  -1.36763509e-01  -1.06266231e-01\n",
      "   -2.51538522e-02  -1.03927521e-01  -1.97568418e-01   3.30304402e-02\n",
      "    8.18364679e-02  -1.81072093e-01  -1.25224017e-01   2.49292764e-03\n",
      "   -1.53487965e-01   1.19621523e-02  -2.87399256e-01  -1.02255174e-01\n",
      "   -1.84973083e-01  -2.24266896e-01  -2.18985136e-01  -1.92439826e-01\n",
      "   -1.25300386e-01  -2.86056343e-01  -1.00501848e-01  -1.40083034e-01\n",
      "    2.20644366e-01  -1.61171787e-01  -4.20127734e-02  -1.46062594e-01\n",
      "   -1.83534187e-01  -6.60684120e-02  -1.26087072e-01  -3.29883733e-02\n",
      "   -5.98404460e-02  -2.31414793e-01  -1.12163894e-01  -8.86947345e-02\n",
      "    7.89690356e-02  -1.24786154e-01  -7.88413519e-02  -1.47314922e-01\n",
      "   -2.81142561e-02  -6.45169066e-02  -2.13678469e-01  -7.33316758e-01\n",
      "   -4.94511418e+00  -1.81718849e+00  -4.99615081e-02  -7.98865293e-03\n",
      "   -1.78925941e-02  -2.04105228e-02   2.57329296e-02  -4.32265649e-02\n",
      "   -1.96971538e-02   3.83876656e-02  -4.67477738e-02   7.29138894e-02\n",
      "   -5.04236665e-02  -8.46747498e-03  -1.22883233e-02  -1.32362392e-02\n",
      "    9.73448660e-03  -3.17428828e-02  -1.83046428e-02   2.28599973e-02\n",
      "    2.78502186e-04  -2.01337015e-02   1.17910115e-03  -2.75066240e-02\n",
      "   -8.19233456e-03   4.07653242e-03   3.76548403e-03  -2.37297310e-03\n",
      "   -2.22704467e-02  -4.22232846e-02   3.34413995e-02  -3.78145667e-02\n",
      "    9.30130066e-03  -1.24805959e-02  -1.33811948e-02  -5.25751825e-02\n",
      "    1.82019616e-02]\n",
      " [  1.21060087e-03  -1.19200290e-02  -4.40023903e-02   7.15307752e-02\n",
      "   -1.17274887e-01  -3.98573329e-02  -4.90789153e-02   1.12109857e-01\n",
      "    1.25265498e-01   5.84405898e-02   1.16584639e-01   1.62137248e-01\n",
      "    1.36212950e-01   1.46303652e-01   1.88105996e-03   1.50729129e-01\n",
      "    1.01425044e-02   2.37270718e-02   4.55855858e-02   8.95684780e-02\n",
      "   -5.68185794e-02  -1.43019387e-02  -3.44468343e-02  -7.35814476e-02\n",
      "    2.97778337e-04   6.07246208e-02  -1.13914681e-01  -4.27866555e-02\n",
      "   -5.49176953e-02  -6.37382772e-02  -9.70686712e-02  -7.68040195e-02\n",
      "   -9.28593111e-02  -2.17977016e-02  -2.93893813e-01  -7.91683646e-02\n",
      "   -2.03931743e-02   5.66466839e-03   4.59030720e-02  -1.86636263e-01\n",
      "    7.86778947e-02   3.53809574e-02   4.51514873e-02  -5.79015434e-02\n",
      "    1.03373198e-02   6.93604074e-02  -1.92759081e-01  -4.00591325e-03\n",
      "   -7.51915377e-02  -1.47077548e-01  -1.47472121e-01  -8.50867900e-02\n",
      "   -2.72918777e-02   1.46662406e-02  -1.11069815e-01  -5.67778307e-02\n",
      "    1.89526917e-02   1.12814661e-01  -5.00433670e-02   5.68410258e-03\n",
      "   -2.84410842e-02  -7.50878038e-02  -3.82838545e-02   2.00098822e-01\n",
      "    1.40646518e-01   9.50528173e-02   2.16022645e-01   2.04317128e-01\n",
      "    3.15466643e-01   3.00876598e-01   2.82859948e-01   2.00911128e-01\n",
      "    2.51941026e-01   2.75027829e-02  -2.11913857e-01  -4.38276693e-02\n",
      "   -4.13629943e-02  -3.43803538e-02  -1.00975472e-01  -3.57955530e-02\n",
      "   -1.88698946e-02  -4.66773584e-02   1.96949964e-02  -5.26647444e-02\n",
      "   -4.41239053e-02   1.46706024e-01   1.27238101e-01   9.66885812e-02\n",
      "   -3.53903446e-01  -1.92430967e-02  -1.91442759e-01   1.85121198e-01\n",
      "   -4.85301012e-02   7.68932469e-02  -1.78029034e-02   8.23312093e-02\n",
      "    2.72151774e-02   2.35352500e-01   1.00139941e-01   2.11535763e-01\n",
      "    4.98226822e-02  -2.16776043e-01  -8.24671383e-02  -7.42918156e-02\n",
      "   -2.11260007e-01  -1.93741850e-01  -2.77674053e-01   1.62580357e-01\n",
      "   -7.27951008e-02  -1.81713379e-01   5.81339181e-02   1.35067454e-01\n",
      "   -4.77964820e-03  -6.16512194e-02   1.08089520e-01  -1.56826038e-01\n",
      "    4.70826593e-01   6.72672000e-02   8.68490127e-02  -1.01079271e-01\n",
      "   -1.88330337e-01  -1.06812668e-02  -9.97840676e-02  -1.02659993e-01\n",
      "    1.07148267e-01  -2.26373896e-01   2.30008080e-02   2.82538132e-02\n",
      "    3.50701187e-02  -1.03338619e-02  -8.36948870e-02  -5.64586062e-02\n",
      "   -6.45650000e-02   1.34794701e-02   3.21278986e-02   2.47753181e-02\n",
      "   -9.08708967e-02  -5.17672299e-01  -3.88486178e-02   1.92930125e-02\n",
      "    8.71246013e-03   1.12036836e-02  -4.04280869e-02   2.27484574e-02\n",
      "    1.14539546e-02  -2.83461361e-02  -2.00290041e-02  -1.72268575e-02\n",
      "    3.53120893e-02   3.78542998e-02  -8.64333533e-03   2.06715913e-03\n",
      "   -2.82639229e-02  -2.45604511e-02   2.20301079e-02   6.49188727e-03\n",
      "   -2.84148813e-03   2.82149431e-02  -3.13270201e-02   2.33208962e-02\n",
      "   -1.93590837e-02  -2.72823654e-02   5.39856813e-03   2.22776625e-04\n",
      "    2.73968775e-02   1.34547360e-02  -1.71231432e-02   9.64013322e-03\n",
      "   -1.15837856e-02   5.66247752e-04  -5.57364649e-03   1.59618255e-03\n",
      "   -1.64449238e-03]\n",
      " [ -6.45911571e-02  -2.42415811e-01  -2.48259320e-01  -1.24796931e-01\n",
      "   -1.93254205e-01  -1.47209570e-01  -1.90022186e-01  -2.14515598e-01\n",
      "   -2.35270941e-01  -1.92733427e-01  -1.24672213e-01  -4.92440212e-02\n",
      "   -2.63333364e-01  -1.21318798e-01  -2.27410254e-01  -3.12217960e-01\n",
      "   -9.68768222e-02  -2.50658023e-01  -9.25955849e-02  -2.02738519e-01\n",
      "   -2.78336015e-01  -1.40202908e-01  -2.31511024e-01  -2.38589471e-01\n",
      "   -1.18614425e-01  -2.42381679e-01  -2.82411556e-01  -1.30372667e-01\n",
      "   -1.13319045e-01  -2.82183643e-01  -2.31844685e-01  -2.67493652e-01\n",
      "   -2.18570669e-01  -2.06532456e-01  -2.99411079e-01  -1.40344190e-01\n",
      "   -2.65651287e-01  -2.08791771e-01  -2.47583771e-01  -1.63030733e-01\n",
      "   -1.65823887e-01  -1.77303331e-01  -1.12999572e-01  -2.09428008e-01\n",
      "   -2.56670326e-01  -2.09765307e-01  -3.02137662e-01  -3.57436661e-01\n",
      "   -6.12703407e-02  -5.60769073e-02  -1.34540533e-01  -2.00710524e-01\n",
      "   -2.46256080e-01  -5.65626828e-02  -2.69539429e-01  -3.61108576e-01\n",
      "   -6.51146360e-02  -8.05855145e-02  -1.53352430e-01  -3.53313121e-01\n",
      "   -1.86471165e-01  -2.93470773e-01  -3.05988128e-01  -2.51514524e-01\n",
      "   -4.12755578e-03  -8.14066008e-02  -1.91364309e-03  -8.14377383e-02\n",
      "    5.42303459e-03  -8.96689686e-02  -5.94832294e-02  -1.99137227e-02\n",
      "    5.23456687e-02  -2.02004835e-01  -2.63047320e-01  -2.65380348e-01\n",
      "   -1.64234251e-01  -2.34522541e-01  -1.94256794e-01  -1.19821945e-01\n",
      "   -2.72221040e-01  -2.35420848e-01  -2.02299501e-01  -2.21939651e-01\n",
      "   -2.58625635e-01  -4.17916213e-02  -8.73560899e-02  -2.28815780e-02\n",
      "   -3.17659976e-01  -5.06549128e-02  -2.82619272e-01   2.47142180e-02\n",
      "   -1.82443358e-01  -8.36580236e-02  -2.16204964e-01  -3.04098459e-01\n",
      "   -2.22451003e-01   5.74990778e-02  -2.36878107e-01  -1.76106511e-01\n",
      "    1.55608179e-01  -1.22698543e-01  -1.93368842e-01  -1.61596683e-01\n",
      "   -4.48451603e-01  -2.49996079e-01  -4.51287516e-01  -3.49801035e-02\n",
      "   -2.46672298e-01  -4.25027713e-01  -2.71779893e-01  -1.51756255e-01\n",
      "   -2.56818388e-01  -4.34898483e-01  -1.19767527e-01  -3.56460785e-01\n",
      "    3.47207037e-01  -1.68583783e-01   3.14313487e-02  -3.65708293e-01\n",
      "   -4.69673607e-01  -3.50130616e-01  -2.99632065e-01  -1.15336557e-01\n",
      "   -1.10494028e-01  -4.08956824e-01  -1.70324587e-01  -7.05687293e-02\n",
      "   -1.76802885e-01  -2.38649893e-01  -3.05646366e-01  -3.42093922e-01\n",
      "   -3.18124299e-01  -2.49464874e-01  -2.58712151e-01  -1.41036500e+00\n",
      "   -1.25963142e+01  -5.91826673e+00  -1.16849548e-01  -3.47468563e-02\n",
      "   -2.46073112e-04   1.15376705e-02  -3.67253547e-02  -1.19592997e-02\n",
      "   -3.41323747e-02  -1.57938110e-02  -7.04728621e-02   1.14292312e-02\n",
      "   -4.54700860e-03  -1.09388288e-02   1.77449949e-02  -2.88548227e-02\n",
      "   -4.15419860e-02  -7.44964161e-02  -1.12312093e-03   7.40495668e-03\n",
      "    3.48733486e-03   4.70723952e-02  -1.18773964e-01  -2.10735565e-02\n",
      "   -4.19486515e-02  -1.66493217e-02  -5.68601011e-03  -2.84150738e-03\n",
      "   -1.48855334e-02   1.16273609e-02  -3.57237698e-02  -2.88196082e-02\n",
      "   -3.47710317e-02  -3.26959136e-02  -3.49503160e-02  -3.18810002e-02\n",
      "   -3.05719849e-02]]\n",
      "Accuracy of:  0.230921704658\n",
      "CPU times: user 894 ms, sys: 15.4 ms, total: 909 ms\n",
      "Wall time: 489 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear boundaries visualization from sklearn documentation\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_decision_boundaries(lr,Xin,y,title=''):\n",
    "    Xb = copy.deepcopy(Xin)\n",
    "    lr.fit(Xb[:,:2],y) # train only on two features\n",
    "\n",
    "    h=0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "    y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb[:, 0], Xb[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Fear of Public Speaking')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  BFGSBinaryLogisticRegression  and cost  0.0  Accuracy of:  0.616451932607\n",
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  BFGSBinaryLogisticRegression  and cost  0.002  Accuracy of:  0.616451932607\n",
      "CPU times: user 10 µs, sys: 1 µs, total: 11 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  BFGSBinaryLogisticRegression  and cost  0.004  Accuracy of:  0.613478691774\n",
      "CPU times: user 14 µs, sys: 2 µs, total: 16 µs\n",
      "Wall time: 11 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  BFGSBinaryLogisticRegression  and cost  0.006  Accuracy of:  0.610505450942\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  BFGSBinaryLogisticRegression  and cost  0.008  Accuracy of:  0.602576808722\n",
      "CPU times: user 10 µs, sys: 1e+03 ns, total: 11 µs\n",
      "Wall time: 6.91 µs\n",
      "For  StochasticLogisticRegression  and cost  0.0  Accuracy of:  0.176412289395\n",
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 10 µs\n",
      "For  StochasticLogisticRegression  and cost  0.002  Accuracy of:  0.230921704658\n",
      "CPU times: user 10 µs, sys: 0 ns, total: 10 µs\n",
      "Wall time: 6.91 µs\n",
      "For  StochasticLogisticRegression  and cost  0.004  Accuracy of:  0.175421209118\n",
      "CPU times: user 10 µs, sys: 2 µs, total: 12 µs\n",
      "Wall time: 7.15 µs\n",
      "For  StochasticLogisticRegression  and cost  0.006  Accuracy of:  0.263627353816\n",
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 5.96 µs\n",
      "For  StochasticLogisticRegression  and cost  0.008  Accuracy of:  0.230921704658\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  LineSearchLogisticRegression  and cost  0.0  Accuracy of:  0.230921704658\n",
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  LineSearchLogisticRegression  and cost  0.002  Accuracy of:  0.230921704658\n",
      "CPU times: user 10 µs, sys: 1 µs, total: 11 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  LineSearchLogisticRegression  and cost  0.004  Accuracy of:  0.230921704658\n",
      "CPU times: user 5 µs, sys: 8 µs, total: 13 µs\n",
      "Wall time: 12.2 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  LineSearchLogisticRegression  and cost  0.006  Accuracy of:  0.230921704658\n",
      "CPU times: user 11 µs, sys: 1e+03 ns, total: 12 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  LineSearchLogisticRegression  and cost  0.008  Accuracy of:  0.230921704658\n"
     ]
    }
   ],
   "source": [
    "costs = [n for n in np.arange(0,0.01,0.002)]\n",
    "optimizations = [\"BFGSBinaryLogisticRegression\",\"StochasticLogisticRegression\",\"LineSearchLogisticRegression\"]\n",
    "\n",
    "for optimization in optimizations:\n",
    "    for cost in costs:\n",
    "        %%time\n",
    "        lr = MultiClassLogisticRegression(eta=0.1,\n",
    "                                           iterations=10,\n",
    "                                           C=cost,optimization=optimization) # get object\n",
    "        lr.fit(X,y)\n",
    "#         print(lr)\n",
    "        \n",
    "        yhat = lr.predict(X)\n",
    "        print('For ',optimization,' and cost ', cost,' Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/Omar/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "dog = MultiClassLogisticRegression(eta=0.1,\n",
    "                                           iterations=10,\n",
    "                                           C=0.2,optimization='BFGSBinaryLogisticRegression') # get object\n",
    "dog.fit(X,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.614469772052\n",
      "CPU times: user 787 ms, sys: 33.2 ms, total: 820 ms\n",
      "Wall time: 642 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='lbfgs')#,max_iter=100,C=0.005) \n",
    "lr_sk.fit(X,y)\n",
    "#print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.222772277228\n",
      "confusion matrix\n",
      " [[ 0  0  0  0  0  0]\n",
      " [ 5  9 11  6  3  0]\n",
      " [10 14 17  6  3  0]\n",
      " [ 9 13 16 12  4  0]\n",
      " [ 6  9 15  8  7  0]\n",
      " [ 3  4  7  3  2  0]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.173267326733\n",
      "confusion matrix\n",
      " [[ 0  0  0  0  0  0]\n",
      " [ 9  3 11  7  6  0]\n",
      " [ 9 10 17  5  2  0]\n",
      " [ 9 14 17 11  6  0]\n",
      " [ 3 13 16  6  4  0]\n",
      " [ 3  9  5  6  1  0]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.188118811881\n",
      "confusion matrix\n",
      " [[ 0  0  0  0  0  0]\n",
      " [ 3  8 10  6  4  0]\n",
      " [11  9 11  9  7  0]\n",
      " [12 13 17 16  4  0]\n",
      " [ 5 14 18  4  3  0]\n",
      " [ 2  5 10  0  1  0]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='lbfgs')#,max_iter=100,C=0.005) \n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_sk.fit(X_train,y_train)\n",
    "    #print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "    yhat = lr_sk.predict(X_test)\n",
    " \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
