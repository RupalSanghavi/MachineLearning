{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "df_imputed = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_imputed = df_imputed.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in ['Smoking', 'Alcohol', 'Punctuality', 'Lying', 'Internet usage',\n",
    "        'Gender', 'Left - right handed', 'Education', 'Only child',\n",
    "        'Village - town', 'House - block of flats']:\n",
    "    df_imputed = df_imputed.drop(col,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Fear of public speaking' in df_imputed:\n",
    "    y = df_imputed['Fear of public speaking'].values # get the labels we want\n",
    "    del df_imputed['Fear of public speaking'] # get rid of the class label\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 µs, sys: 0 ns, total: 33 µs\n",
      "Wall time: 37 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "# blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "# blr.fit(X,y)\n",
    "# print(blr)\n",
    "\n",
    "# yhat = blr.predict(X)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 µs, sys: 1e+03 ns, total: 37 µs\n",
      "Wall time: 40.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X + 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "# hlr = HessianBinaryLogisticRegression(eta=0.1,iterations=20,C=0.1) # note that we need only a few iterations here\n",
    "\n",
    "# hlr.fit(X,y)\n",
    "# yhat = hlr.predict(X)\n",
    "# print(hlr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44 µs, sys: 1e+03 ns, total: 45 µs\n",
      "Wall time: 47 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    @staticmethod\n",
    "    def line_search_function(eta,X,y,w,grad):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)+np.sum(wnew**2)\n",
    "    @staticmethod\n",
    "    def line_search_function_l1(eta,X,y,w,grad):\n",
    "        if(math.sin(w) < 0 ):\n",
    "            w -=1\n",
    "        elif(math.sin(w) > 0):\n",
    "            w += 1\n",
    "        else:\n",
    "            w = w\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)+np.sum(math.fabs(wnew))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ += gradient*eta # set new function values\n",
    "                \n",
    "            \n",
    "\n",
    "# lslr = LineSearchLogisticRegression(eta=0.1,iterations=110, C=0.001)\n",
    "\n",
    "# lslr.fit(X,y)\n",
    "\n",
    "# yhat = lslr.predict(X)\n",
    "# print(lslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27 µs, sys: 0 ns, total: 27 µs\n",
      "Wall time: 31 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "# slr = StochasticLogisticRegression(0.1,1000, C=0.001) # take a lot more steps!!\n",
    "\n",
    "# slr.fit(X,y)\n",
    "\n",
    "# yhat = slr.predict(X)\n",
    "# print(slr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39 µs, sys: 1 µs, total: 40 µs\n",
      "Wall time: 42.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += 2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "# bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "# bfgslr.fit(X,y)\n",
    "# yhat = bfgslr.predict(X)\n",
    "# print(bfgslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, optimization=None):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.optimization = optimization\n",
    "        print(self.optimization)\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        print(optimization)\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            #hblr = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            if(self.optimization == \"BFGSBinaryLogisticRegression\"):\n",
    "                hblr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            elif(self.optimization == \"StochasticLogisticRegression\"):\n",
    "                hblr = StochasticLogisticRegression(self.eta,self.iters,self.C)\n",
    "            else:\n",
    "                hblr = LineSearchLogisticRegression(self.eta,self.iters,self.C)\n",
    "\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.lr_explor>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_vals = np.logspace(-3,-2,15)\n",
    "def lr_explor(cost_idx):\n",
    "    C = cost_vals[cost_idx]\n",
    "    lr_clf = MultiClassLogisticRegression(eta=0.1,\n",
    "                                           iterations=2500,\n",
    "                                           C=C) # get object\n",
    "    \n",
    "    plot_decision_boundaries(lr_clf,X,y,title=\"C=%.5f\"%(C))\n",
    "wd.interact(lr_explor,cost_idx=(0,15,1),__manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFGSBinaryLogisticRegression\n",
      "LineSearchLogisticRegression\n",
      "====Iteration 0  ====\n",
      "accuracy 0.266666666667\n",
      "confusion matrix\n",
      " [[ 8  7  5  3  0]\n",
      " [ 7  9  9  6  0]\n",
      " [ 1  9 16  5  3]\n",
      " [ 4  9  9  3  5]\n",
      " [ 2  1  7  7  0]]\n",
      "LineSearchLogisticRegression\n",
      "====Iteration 1  ====\n",
      "accuracy 0.296296296296\n",
      "confusion matrix\n",
      " [[ 9  6  8  2  0]\n",
      " [ 2  7  9  8  1]\n",
      " [ 4 10 17 12  4]\n",
      " [ 0  2 11  4  4]\n",
      " [ 0  0  6  6  3]]\n",
      "LineSearchLogisticRegression\n",
      "====Iteration 2  ====\n",
      "accuracy 0.333333333333\n",
      "confusion matrix\n",
      " [[13  3  4  1  0]\n",
      " [13  9 14  1  0]\n",
      " [ 7  6 15  9  7]\n",
      " [ 1  2  9  5  1]\n",
      " [ 0  2  7  3  3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "\n",
    "lr_clf = MultiClassLogisticRegression(eta=0.1,iterations=10, C=0.0001, optimization=\"BFGSBinaryLogisticRegression\") # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LineSearchLogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ -1.63349907e-01  -7.53109219e-01  -4.18890314e-01  -5.37218753e-01\n",
      "   -3.74173244e-01  -3.18686580e-01  -4.16782071e-01  -4.37504622e-01\n",
      "   -6.38465654e-01  -5.43033776e-01  -3.47500475e-01  -4.29742118e-01\n",
      "   -4.66071886e-01  -3.39931617e-01  -4.23360747e-01  -4.12940095e-01\n",
      "   -4.03825684e-01  -4.10749643e-01  -4.63371730e-01  -3.17367557e-01\n",
      "   -7.98096680e-01  -3.66735922e-01  -7.03349904e-01  -6.64868032e-01\n",
      "   -6.03155294e-01  -5.82584381e-01  -3.73393149e-01  -6.83038405e-01\n",
      "   -7.22700445e-01  -5.59667719e-01  -2.52170908e-01  -4.56243148e-01\n",
      "   -2.84375526e-01  -3.61969532e-01  -1.24642188e-01  -3.19311378e-01\n",
      "   -2.45098971e-01  -6.62726906e-01  -3.36484250e-01  -2.02482825e-01\n",
      "   -6.35239353e-01  -5.05425187e-01  -5.29008655e-01  -4.21314614e-01\n",
      "   -5.65733032e-01  -5.03379054e-01   2.33386913e-02  -2.61860791e-01\n",
      "   -3.41004010e-01  -2.84991200e-01  -7.81667360e-01  -3.33455253e-01\n",
      "   -1.52190045e-01  -2.13482608e-01  -4.44572900e-01  -1.58531213e-01\n",
      "   -3.78972519e-01  -5.59592827e-01  -5.12177407e-01  -3.87734328e-01\n",
      "   -3.46161350e-01  -6.36662493e-01  -2.34132072e-01  -6.55165366e-01\n",
      "   -5.80068747e-01  -4.07391805e-01  -5.28321748e-01  -5.85604255e-01\n",
      "   -6.42788110e-01  -7.88841609e-01  -6.34713207e-01  -4.32758333e-01\n",
      "   -7.97298338e-01  -4.06891224e-01  -3.08097848e-01  -2.36309741e-01\n",
      "   -4.39530654e-01  -3.38957877e-01  -5.61993367e-01  -5.87071257e-01\n",
      "   -5.37494609e-01  -5.66160525e-01  -4.92514466e-01  -6.34919635e-01\n",
      "   -3.79343367e-01  -5.27992907e-01  -3.52197438e-01  -7.58146001e-01\n",
      "   -4.15048337e-01  -7.37883829e-01  -4.13396813e-01  -4.05793266e-01\n",
      "   -6.38034502e-01  -4.78620882e-01  -4.02408008e-01  -6.41083600e-01\n",
      "   -7.58635604e-01  -7.10770784e-01  -5.41501005e-01  -6.94482123e-01\n",
      "   -7.15122463e-01  -5.31893333e-01  -4.27412811e-01  -3.18522160e-01\n",
      "   -5.82215091e-02  -3.87787314e-01  -1.19807928e-02  -6.60589439e-01\n",
      "   -4.61979620e-01  -6.63658392e-02  -4.32392948e-01  -5.71144322e-01\n",
      "   -5.42589840e-01  -2.02873775e-01  -4.23903335e-01  -1.75970817e-01\n",
      "   -1.50564302e+00  -6.72424107e-01  -6.72855260e-01  -3.51894163e-01\n",
      "   -1.49413143e-01  -3.88494322e-01  -3.62298373e-01  -4.88524512e-01\n",
      "   -7.16602248e-01  -1.23730556e-01  -5.70104813e-01  -5.53490971e-01\n",
      "   -6.19180881e-01  -4.54167790e-01  -2.89585852e-01  -1.99430067e-01\n",
      "   -3.35000806e-01  -3.25968605e-01  -3.67044662e-01  -3.11150245e+00\n",
      "   -2.73001594e+01  -9.04020985e+00  -1.81093469e-01]\n",
      " [  4.47844714e-02   1.63653118e-01   1.83272338e-01   8.44525132e-02\n",
      "    1.87454622e-01   1.11195699e-01   1.70232651e-01   1.66276884e-01\n",
      "    2.80419525e-02  -1.02649648e-01  -1.22008679e-01  -1.66727733e-01\n",
      "    1.28593900e-01   1.38657797e-01   2.80953884e-01   8.64653800e-02\n",
      "   -5.04926521e-02   2.36287306e-01   1.92355382e-01   1.27420639e-01\n",
      "    2.17613517e-01   8.56074079e-02   1.30082014e-01   1.39201793e-01\n",
      "    1.40566151e-01   1.91988493e-01   2.28008883e-01   2.24403827e-01\n",
      "    9.05790103e-02   2.09018930e-01   2.93852761e-01   1.24883894e-01\n",
      "    1.89730741e-01   1.72858168e-01   3.71022132e-01   7.75559403e-02\n",
      "    1.72804817e-01   7.02588069e-02   3.76681175e-02   1.58628165e-01\n",
      "    5.10572021e-02   1.13296025e-01   6.73000714e-02   1.15554651e-01\n",
      "    2.11609463e-01   8.16866248e-02   2.10855566e-01   3.61800039e-02\n",
      "    1.07713085e-01   2.39174762e-01   3.04618080e-01   3.64248440e-01\n",
      "    8.47669276e-02   1.20716913e-01   4.86424619e-02   1.99619720e-01\n",
      "    6.69139397e-02   2.73934442e-02   8.27899193e-02   1.49317290e-01\n",
      "    2.29636493e-01   2.12065124e-01   1.96538980e-01   6.28895186e-02\n",
      "   -5.49915390e-02  -1.52781964e-02  -1.16215401e-01  -4.51371058e-02\n",
      "   -9.08011527e-02  -3.43901545e-02   2.94238028e-02  -1.07376366e-01\n",
      "    4.20786704e-02   1.34124802e-01   2.20360164e-01   2.49082993e-02\n",
      "    2.14759295e-01   1.39813130e-01   1.45606845e-01   1.61533550e-01\n",
      "    1.20263440e-01   1.16500520e-01   1.80523941e-01   2.24421319e-01\n",
      "    6.26861910e-03   7.65230505e-02  -2.45890718e-02   1.98680425e-02\n",
      "    2.77891511e-01   1.37520832e-01   1.46237425e-01  -5.05634944e-02\n",
      "    1.52048195e-01   1.79088303e-01   1.22940556e-01   8.25454720e-02\n",
      "    1.16430553e-01   3.29282942e-03   1.45887151e-01   3.96984762e-02\n",
      "   -7.53462884e-02   2.78923963e-01   1.35822817e-01   8.65169803e-02\n",
      "    1.76901393e-01   2.58427967e-01   2.89198199e-01   2.33860773e-02\n",
      "    2.75616267e-01   3.62568792e-01   1.98516863e-01   1.88522933e-01\n",
      "    1.90676172e-01   2.84980055e-01   2.43529484e-03   1.12982048e-01\n",
      "   -2.05652633e-01   2.45354172e-01   1.08606041e-01   1.67170278e-01\n",
      "    1.87123154e-01   1.57490762e-01   1.90360882e-01   1.47811684e-01\n",
      "    9.57946224e-02   2.38930752e-01   2.05331676e-02   1.25215800e-01\n",
      "   -3.72605560e-02   2.31456950e-01   1.27701382e-01   1.04125521e-01\n",
      "    1.24778505e-01  -2.85094174e-02   1.58208800e-01   1.05356767e+00\n",
      "    8.45996709e+00   3.25884415e+00   7.68724484e-02]\n",
      " [ -4.31665780e-02  -2.17574886e-01  -1.99439124e-01  -1.32448106e-01\n",
      "   -1.14558480e-01  -1.28801722e-01  -2.86412703e-01  -2.05462897e-01\n",
      "   -5.26120076e-02  -7.56578362e-02  -7.30650238e-02  -5.36021437e-02\n",
      "   -1.75476497e-01  -3.73902663e-01  -2.71688098e-01  -2.45569100e-01\n",
      "   -6.51901440e-02  -2.70887263e-01  -1.27983378e-01  -2.88702546e-01\n",
      "   -1.13059570e-01  -1.15489487e-01   1.84324130e-02  -1.81153386e-01\n",
      "   -2.26872037e-01  -1.29729429e-01  -2.07061267e-01  -2.68446438e-01\n",
      "   -6.93704184e-02  -1.18085600e-01  -2.13355286e-01  -1.27291905e-01\n",
      "   -3.26870096e-01  -2.61979621e-01  -3.29792284e-01  -6.63771883e-02\n",
      "   -1.29068667e-01  -1.27871721e-01  -1.49775426e-01  -2.21090092e-01\n",
      "    4.14924502e-02   5.20794064e-02  -1.66702328e-01  -1.96154838e-01\n",
      "   -2.43763920e-01  -2.28991534e-02  -2.83598872e-01  -9.68449513e-02\n",
      "   -2.63834028e-01  -3.27793496e-01  -1.06445072e-01  -1.92807117e-01\n",
      "   -2.02347103e-01  -2.36804833e-01   8.77646388e-03  -2.76454790e-01\n",
      "   -7.22007529e-02   5.09131720e-02  -7.74455069e-02  -2.11747015e-01\n",
      "   -3.03806759e-01  -1.94645020e-01  -2.23506806e-01  -1.40751821e-01\n",
      "    9.78511512e-03   4.70916954e-02  -4.24462789e-02  -1.15747975e-02\n",
      "   -1.68005335e-01  -6.71572136e-02  -1.04551340e-01  -1.36620774e-01\n",
      "   -1.39603096e-01  -1.96038875e-01  -2.10647516e-01  -2.92357983e-02\n",
      "   -2.01325249e-01  -2.06362187e-01  -1.09448204e-01  -6.74099501e-02\n",
      "   -2.49450712e-01  -2.00735530e-01  -1.14172271e-01  -1.90380500e-01\n",
      "   -7.06241989e-02  -9.68482520e-02  -1.90404611e-01  -3.22006112e-02\n",
      "   -1.70636467e-01  -1.74254434e-01  -1.58658957e-01  -1.30754583e-01\n",
      "   -1.46301839e-01  -2.06333770e-01  -8.72874563e-02  -1.40797747e-01\n",
      "   -1.39729967e-01  -1.45307397e-01  -2.39011437e-01  -4.63596082e-02\n",
      "    4.80818316e-02  -1.82378750e-01  -1.85146654e-01  -6.56956170e-02\n",
      "   -1.87398178e-01  -3.43219633e-02  -3.47014548e-01  -1.50358543e-01\n",
      "   -2.23085579e-01  -2.57943727e-01  -2.45920291e-01  -2.34929616e-01\n",
      "   -1.87029478e-01  -3.23266338e-01  -1.95771930e-01  -1.91016145e-01\n",
      "    1.59172320e-01  -1.74939306e-01  -1.17372311e-01  -1.73320126e-01\n",
      "   -1.88880585e-01  -8.77371009e-02  -2.07577648e-01  -1.46575386e-01\n",
      "   -2.18849476e-01  -2.52604826e-01  -1.24025128e-01  -1.13104490e-01\n",
      "    4.36630293e-02  -1.67886071e-01  -5.09469015e-02  -2.35112316e-01\n",
      "   -6.59559607e-02  -6.76418767e-02  -2.76419772e-01  -1.16784629e+00\n",
      "   -7.94570522e+00  -3.64644808e+00   1.42557224e-03]\n",
      " [ -3.50962429e-02  -1.71842846e-01  -1.79919515e-01  -1.74574940e-02\n",
      "   -1.59477785e-01  -7.84966288e-02  -8.85954862e-02  -6.89114929e-02\n",
      "    4.23358736e-02  -4.19828989e-02   9.54703581e-02   8.77968776e-02\n",
      "    7.77851976e-02  -3.19362340e-03  -8.37564101e-02   5.76003284e-02\n",
      "   -1.54062334e-01  -6.05721241e-02  -3.58817069e-02   8.07939565e-02\n",
      "   -2.24883147e-01  -2.03516332e-01  -1.63157104e-01  -1.94858616e-01\n",
      "   -9.02677974e-02  -1.08694394e-02  -2.11443170e-01  -1.29968144e-01\n",
      "   -1.77755670e-01  -2.00271732e-01  -1.65397616e-01  -1.99353654e-01\n",
      "   -1.75498809e-01  -1.86923604e-01  -3.32949735e-01  -1.10357315e-01\n",
      "   -6.89056541e-02  -1.04201643e-01  -8.65118117e-02  -2.87414399e-01\n",
      "    7.06557522e-03  -4.35528519e-02  -1.58818904e-01  -1.24291161e-01\n",
      "   -1.90515745e-01  -9.86301645e-03  -2.41041156e-01  -1.44114148e-03\n",
      "   -1.60064671e-01  -2.01093291e-01  -3.30623213e-01  -3.40453532e-01\n",
      "   -8.81724453e-02  -1.50300509e-01  -2.02104385e-01  -1.53982163e-01\n",
      "   -4.92216608e-02   3.47565770e-02  -1.60404038e-01  -5.15692025e-02\n",
      "   -1.75663821e-01  -2.67415562e-01  -1.23705443e-01   7.83650764e-02\n",
      "    1.45929612e-01  -2.42860606e-02   1.08814983e-01   5.63545612e-02\n",
      "    1.80031978e-01   1.83116237e-01   1.29996933e-01   1.25742407e-01\n",
      "    1.14229267e-01  -1.07693762e-01  -2.22117235e-01  -1.91839347e-01\n",
      "   -2.43550207e-01  -1.78332046e-01  -1.91179298e-01  -8.50893545e-02\n",
      "   -9.30231990e-02  -1.37309264e-01  -1.07607752e-01  -2.13700031e-01\n",
      "   -2.26289255e-01   5.76096705e-02   5.56875051e-02   1.64845310e-02\n",
      "   -3.00932211e-01  -7.55077219e-02  -3.18617372e-01   8.47997963e-02\n",
      "   -1.73753334e-01   3.80105190e-02  -1.74911924e-01  -4.60712448e-02\n",
      "   -4.73228508e-02   8.09542978e-02  -1.09701937e-01   4.58408375e-02\n",
      "   -6.23246060e-02  -3.31370440e-01  -1.95515163e-01  -8.92543679e-02\n",
      "   -3.80325898e-01  -3.03343575e-01  -4.19024493e-01   4.54236355e-02\n",
      "   -2.33380521e-01  -4.03592690e-01  -1.47881020e-02   6.52738088e-02\n",
      "   -1.40057659e-01  -1.60239025e-01   1.63195186e-02  -2.51393371e-01\n",
      "    3.52827530e-01  -1.41474278e-01  -1.33213912e-01  -2.76081453e-01\n",
      "   -3.13276252e-01  -1.33130238e-01  -1.65408126e-01  -2.01932366e-01\n",
      "    1.22276694e-02  -3.65315968e-01  -1.47067635e-02  -1.01436900e-01\n",
      "   -1.59560340e-02  -2.03854531e-01  -2.27621032e-01  -1.62321532e-01\n",
      "   -2.27454852e-01  -1.03439236e-01  -1.89345478e-01  -7.00059463e-01\n",
      "   -5.77879180e+00  -2.06427840e+00  -8.40770927e-02]\n",
      " [ -3.66585009e-02  -1.21558117e-01  -1.57499139e-01  -1.06347045e-01\n",
      "   -7.92582259e-02  -7.49961009e-02  -4.29673838e-02  -8.72384684e-02\n",
      "   -1.90385996e-01  -9.87751682e-02  -7.30075646e-02   3.54199647e-02\n",
      "   -2.40306292e-01  -1.02263991e-02  -1.43664625e-01  -2.04959168e-01\n",
      "    2.25191003e-02  -1.41789510e-01  -1.09429575e-01  -6.80501040e-02\n",
      "   -1.89045910e-01  -3.25076124e-02  -9.52393275e-02  -1.50059999e-01\n",
      "   -1.37595933e-04  -2.14433133e-01  -1.94090516e-01   3.85242484e-03\n",
      "   -3.33591163e-03  -2.06068211e-01  -1.70803746e-01  -1.85345734e-01\n",
      "   -1.35019681e-01  -7.72239002e-02  -2.16254169e-01  -1.54984509e-01\n",
      "   -2.43080841e-01  -1.48865187e-01  -1.91742075e-01  -5.92840939e-02\n",
      "   -9.48926098e-02  -1.50238760e-01   7.30689191e-02  -8.52954075e-02\n",
      "   -9.31372405e-02  -1.42840955e-01  -2.40164485e-01  -3.11994019e-01\n",
      "    9.35806750e-02   9.45513548e-02   5.97293141e-02  -8.36406665e-02\n",
      "   -1.71994382e-01   8.11932497e-02  -2.39692092e-01  -3.89889560e-01\n",
      "    4.61853648e-03  -1.14177433e-01  -1.02018269e-01  -2.68670344e-01\n",
      "   -9.37715168e-02  -1.87143610e-01  -3.10921150e-01  -1.04362763e-01\n",
      "    3.74592729e-02  -6.18851183e-02   9.63599076e-02   4.63266190e-03\n",
      "    1.31490660e-01   4.26103100e-02   5.66265240e-02   1.09208420e-02\n",
      "    1.31520791e-01  -1.11834264e-01  -2.14926159e-01  -1.99316900e-01\n",
      "   -3.43428675e-02  -1.00805530e-01  -8.44726202e-02  -3.42609668e-02\n",
      "   -1.13543769e-01  -1.37423915e-01  -1.05038699e-01  -6.29421493e-02\n",
      "   -1.07805865e-01   1.90417135e-02  -4.54719103e-02   2.31400709e-02\n",
      "   -2.36427145e-01   2.77978678e-02  -2.08825655e-01   1.23855034e-01\n",
      "   -1.03149959e-01  -1.50520464e-02  -1.45934513e-01  -1.85377329e-01\n",
      "   -1.11618866e-01   1.55546889e-01  -1.10071458e-01  -8.64563621e-02\n",
      "    1.44247962e-01   1.11534997e-02  -9.41820709e-02  -9.79133387e-02\n",
      "   -3.61017357e-01  -2.10267959e-01  -3.51899536e-01   3.04026286e-02\n",
      "   -2.14769586e-01  -4.27724774e-01  -2.03155181e-01  -1.50455351e-01\n",
      "   -1.63098054e-01  -4.35256738e-01  -6.42036137e-02  -3.22880572e-01\n",
      "    4.60189334e-01  -6.37841065e-02   1.40914502e-01  -2.51657810e-01\n",
      "   -4.20855919e-01  -2.78533864e-01  -2.34920925e-01   4.87880799e-02\n",
      "    2.98178034e-02  -3.46735230e-01  -4.43584805e-02   8.95487131e-03\n",
      "   -8.90087739e-02  -1.86228723e-01  -3.13437487e-01  -2.51779589e-01\n",
      "   -2.55946921e-01  -1.39077528e-01  -1.29063327e-01  -8.04587052e-01\n",
      "   -8.22395265e+00  -4.03403103e+00  -1.43692985e-01]]\n",
      "Accuracy of:  0.23293768546\n",
      "CPU times: user 626 ms, sys: 52.1 ms, total: 678 ms\n",
      "Wall time: 229 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear boundaries visualization from sklearn documentation\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_decision_boundaries(lr,Xin,y,title=''):\n",
    "    Xb = copy.deepcopy(Xin)\n",
    "    lr.fit(Xb[:,:2],y) # train only on two features\n",
    "\n",
    "    h=0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "    y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb[:, 0], Xb[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Fear of Public Speaking')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 1e+03 ns, total: 20 µs\n",
      "Wall time: 6.91 µs\n",
      "BFGSBinaryLogisticRegression\n",
      "IN\n",
      "IN\n",
      "IN\n",
      "IN\n",
      "IN\n",
      "For  BFGSBinaryLogisticRegression  and cost  0.0  Accuracy of:  0.635014836795\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "BFGSBinaryLogisticRegression\n",
      "IN\n",
      "IN\n",
      "IN\n",
      "IN\n",
      "IN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  BFGSBinaryLogisticRegression  and cost  0.005  Accuracy of:  0.63649851632\n",
      "CPU times: user 13 µs, sys: 1e+03 ns, total: 14 µs\n",
      "Wall time: 7.87 µs\n",
      "StochasticLogisticRegression\n",
      "For  StochasticLogisticRegression  and cost  0.0  Accuracy of:  0.166172106825\n",
      "CPU times: user 16 µs, sys: 1e+03 ns, total: 17 µs\n",
      "Wall time: 6.91 µs\n",
      "StochasticLogisticRegression\n",
      "For  StochasticLogisticRegression  and cost  0.005  Accuracy of:  0.23293768546\n",
      "CPU times: user 13 µs, sys: 1 µs, total: 14 µs\n",
      "Wall time: 7.15 µs\n",
      "LineSearchLogisticRegression\n",
      "For  LineSearchLogisticRegression  and cost  0.0  Accuracy of:  0.23293768546\n",
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 5.01 µs\n",
      "LineSearchLogisticRegression\n",
      "For  LineSearchLogisticRegression  and cost  0.005  Accuracy of:  0.23293768546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "costs = [n for n in np.arange(0,0.01,0.005)]\n",
    "optimizations = [\"BFGSBinaryLogisticRegression\",\"StochasticLogisticRegression\",\"LineSearchLogisticRegression\"]\n",
    "\n",
    "for optimization in optimizations:\n",
    "    for cost in costs:\n",
    "        %%time\n",
    "        lr = MultiClassLogisticRegression(eta=0.1,\n",
    "                                           iterations=10,\n",
    "                                           C=cost,optimization=optimization) # get object\n",
    "        lr.fit(X,y)\n",
    "#         print(lr)\n",
    "        yhat = lr.predict(X)\n",
    "        print('For ',optimization,' and cost ', cost,' Accuracy of: ',accuracy_score(y,yhat+1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.166172106825\n",
      "CPU times: user 468 ms, sys: 17 ms, total: 485 ms\n",
      "Wall time: 127 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='lbfgs')#,max_iter=100,C=0.005) \n",
    "lr_sk.fit(X,y)\n",
    "# print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "# yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "3c220d2cc3f246239ec4813851db5a6c": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
