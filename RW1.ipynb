{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_imputed = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_imputed = df_imputed.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoking\n",
      "Alcohol\n",
      "Punctuality\n",
      "Lying\n",
      "Internet usage\n",
      "Gender\n",
      "Left - right handed\n",
      "Education\n",
      "Only child\n",
      "Village - town\n",
      "House - block of flats\n"
     ]
    }
   ],
   "source": [
    "for col in ['Smoking', 'Alcohol', 'Punctuality', 'Lying', 'Internet usage',\n",
    "        'Gender', 'Left - right handed', 'Education', 'Only child',\n",
    "        'Village - town', 'House - block of flats']:\n",
    "    print(col)\n",
    "    df_imputed = df_imputed.drop(col,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Fear of public speaking' in df_imputed:\n",
    "    y = df_imputed['Fear of public speaking'].values # get the labels we want\n",
    "    del df_imputed['Fear of public speaking'] # get rid of the class label\n",
    "#     norm_features = ['Music' ]\n",
    "#     df_imputed[norm_features] = (df_imputed[norm_features]-df_imputed[norm_features].mean()) / df_imputed[norm_features].std()\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = df_imputed.columns.to_series().groupby(df_imputed.dtypes).groups\n",
    "{k.name: v for k, v in g.items()}\n",
    "cols_not_intfloat = ['Smoking', 'Alcohol', 'Punctuality', 'Lying', 'Internet usage',\n",
    "        'Gender', 'Left - right handed', 'Education', 'Only child',\n",
    "        'Village - town', 'House - block of flats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "        \n",
    "blr = BinaryLogisticRegressionBase(0.1)\n",
    "print(blr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi # the actual update inside of sum\n",
    "            gradient += gradi.reshape(self.w_.shape) # reshape to be column vector and add to gradient\n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "            \n",
    "blr = BinaryLogisticRegression(0.1)\n",
    "print(blr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "lr = LogisticRegression(0.1,1500)\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# can we do better? Maybe more iterations?\n",
    "params = dict(eta=0.1,\n",
    "              iterations=500)\n",
    "\n",
    "blr = LogisticRegression(**params)\n",
    "blr.fit(X,y)\n",
    "# print(blr)\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# now lets do some vectorized coding\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "# use same params as defined above\n",
    "blr = VectorBinaryLogisticRegression(**params)\n",
    "blr.fit(X,y)\n",
    "print(blr.w_)\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    # extend init functions\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "        \n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self,X,y):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X,y)\n",
    "        \n",
    "        # add in regularization (to all except bias term)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now redefine the Logistic Regression Function where needed\n",
    "class RegularizedLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                      C=self.C)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = RegularizedLogisticRegression(eta=0.1,iterations=2000) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[    90.77700297]\n",
      " [   455.69584136]\n",
      " [   310.12282053]\n",
      " [   296.38078132]\n",
      " [   214.42342046]\n",
      " [   201.30932795]\n",
      " [   284.12760523]\n",
      " [   263.20815517]\n",
      " [   333.14489982]\n",
      " [   364.99230948]\n",
      " [   228.86059827]\n",
      " [   241.9717399 ]\n",
      " [   274.68285292]\n",
      " [   264.37882356]\n",
      " [   260.24370786]\n",
      " [   300.44206565]\n",
      " [   277.56896059]\n",
      " [   265.78549451]\n",
      " [   220.90069261]\n",
      " [   206.92855259]\n",
      " [   442.66107212]\n",
      " [   260.16446656]\n",
      " [   325.49531991]\n",
      " [   430.09429155]\n",
      " [   335.33045895]\n",
      " [   298.48972215]\n",
      " [   297.55702358]\n",
      " [   364.28971172]\n",
      " [   368.65984636]\n",
      " [   343.53062955]\n",
      " [   193.1951201 ]\n",
      " [   335.3350492 ]\n",
      " [   301.30380176]\n",
      " [   295.91607141]\n",
      " [   236.5990918 ]\n",
      " [   226.2106571 ]\n",
      " [   194.13126135]\n",
      " [   401.21182102]\n",
      " [   294.74523909]\n",
      " [   246.5078163 ]\n",
      " [   257.8183756 ]\n",
      " [   205.99101786]\n",
      " [   310.03972669]\n",
      " [   295.91459597]\n",
      " [   363.12174831]\n",
      " [   239.86763523]\n",
      " [   198.50729774]\n",
      " [   246.35113701]\n",
      " [   250.71823881]\n",
      " [   211.53509964]\n",
      " [   348.52190748]\n",
      " [   221.99802145]\n",
      " [   212.94308208]\n",
      " [   178.36042427]\n",
      " [   319.9578776 ]\n",
      " [   295.29779702]\n",
      " [   182.57322387]\n",
      " [   227.53275878]\n",
      " [   312.07155741]\n",
      " [   307.3914249 ]\n",
      " [   284.13006429]\n",
      " [   430.56441139]\n",
      " [   266.10204987]\n",
      " [   323.6206603 ]\n",
      " [   201.84844301]\n",
      " [   188.73943256]\n",
      " [   226.90481208]\n",
      " [   254.77263776]\n",
      " [   285.99251056]\n",
      " [   302.54026857]\n",
      " [   240.09658848]\n",
      " [   249.15349509]\n",
      " [   301.52537782]\n",
      " [   283.26972165]\n",
      " [   285.84943808]\n",
      " [   242.214218  ]\n",
      " [   287.64108582]\n",
      " [   279.36806749]\n",
      " [   325.5753809 ]\n",
      " [   252.27757258]\n",
      " [   366.32146047]\n",
      " [   382.01125293]\n",
      " [   257.11946644]\n",
      " [   359.84230303]\n",
      " [   314.25883788]\n",
      " [   208.64186069]\n",
      " [   247.0486527 ]\n",
      " [   316.82691475]\n",
      " [   323.004763  ]\n",
      " [   354.29633598]\n",
      " [   374.59988874]\n",
      " [   186.08244208]\n",
      " [   372.48734128]\n",
      " [   210.98549258]\n",
      " [   271.79600753]\n",
      " [   382.47833993]\n",
      " [   392.70181649]\n",
      " [   289.66135894]\n",
      " [   356.01800489]\n",
      " [   312.69188102]\n",
      " [   293.56465248]\n",
      " [   306.84263753]\n",
      " [   313.47822836]\n",
      " [   194.12814654]\n",
      " [   296.23672521]\n",
      " [   247.13240229]\n",
      " [   311.92791115]\n",
      " [   316.59378109]\n",
      " [   336.0391224 ]\n",
      " [   278.20748126]\n",
      " [   284.05090496]\n",
      " [   295.05408939]\n",
      " [   340.40753569]\n",
      " [   315.20317602]\n",
      " [   289.12158814]\n",
      " [   316.68466196]\n",
      " [   377.53753227]\n",
      " [   332.28709821]\n",
      " [   296.53319823]\n",
      " [   343.76818952]\n",
      " [   325.97593176]\n",
      " [   278.66678123]\n",
      " [   306.76790451]\n",
      " [   270.07573209]\n",
      " [   352.50042586]\n",
      " [   316.21888645]\n",
      " [   314.09830606]\n",
      " [   268.04504894]\n",
      " [   297.00307218]\n",
      " [   306.06415919]\n",
      " [   279.37134624]\n",
      " [   294.90478729]\n",
      " [   288.0345054 ]\n",
      " [   270.2342147 ]\n",
      " [   330.26198894]\n",
      " [  1935.50433196]\n",
      " [ 16491.43991674]\n",
      " [  6240.38185302]\n",
      " [   120.91203859]]\n",
      "Accuracy of:  0.23293768546\n",
      "CPU times: user 413 ms, sys: 13.8 ms, total: 427 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "blr.fit(X,y)\n",
    "print(blr)\n",
    "\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression() # all params default\n",
    "lr_sk.fit(X,y)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[   11.17005256]\n",
      " [   53.64658572]\n",
      " [   36.51024463]\n",
      " [   34.89088082]\n",
      " [   25.24340221]\n",
      " [   23.69933782]\n",
      " [   33.44906939]\n",
      " [   30.98625141]\n",
      " [   39.21866253]\n",
      " [   42.96797028]\n",
      " [   26.94169838]\n",
      " [   28.4842537 ]\n",
      " [   32.33723963]\n",
      " [   31.12399529]\n",
      " [   30.63793742]\n",
      " [   35.36991036]\n",
      " [   32.67621979]\n",
      " [   31.29024367]\n",
      " [   26.005367  ]\n",
      " [   24.3605168 ]\n",
      " [   52.11235818]\n",
      " [   30.62814249]\n",
      " [   38.3181155 ]\n",
      " [   50.63308568]\n",
      " [   39.47594372]\n",
      " [   35.13970846]\n",
      " [   35.03147514]\n",
      " [   42.88522335]\n",
      " [   43.39928271]\n",
      " [   40.44295936]\n",
      " [   22.74555442]\n",
      " [   39.47829115]\n",
      " [   35.4725825 ]\n",
      " [   34.83760253]\n",
      " [   27.85698671]\n",
      " [   26.63130657]\n",
      " [   22.85554831]\n",
      " [   47.23234342]\n",
      " [   34.69977482]\n",
      " [   29.02171749]\n",
      " [   30.35022348]\n",
      " [   24.2498103 ]\n",
      " [   36.49847953]\n",
      " [   34.836848  ]\n",
      " [   42.74886279]\n",
      " [   28.23789924]\n",
      " [   23.37262578]\n",
      " [   29.00304983]\n",
      " [   29.5155582 ]\n",
      " [   24.90329026]\n",
      " [   41.02883892]\n",
      " [   26.13633406]\n",
      " [   25.07020934]\n",
      " [   20.99761289]\n",
      " [   37.66803093]\n",
      " [   34.76724947]\n",
      " [   21.49266924]\n",
      " [   26.78503533]\n",
      " [   36.73860212]\n",
      " [   36.18892609]\n",
      " [   33.45032694]\n",
      " [   50.68913058]\n",
      " [   31.32921381]\n",
      " [   38.09691209]\n",
      " [   23.75993787]\n",
      " [   22.21847242]\n",
      " [   26.70973591]\n",
      " [   29.9910666 ]\n",
      " [   33.66528452]\n",
      " [   35.61324669]\n",
      " [   28.26279878]\n",
      " [   29.32992954]\n",
      " [   35.49370938]\n",
      " [   33.34836741]\n",
      " [   33.6535753 ]\n",
      " [   28.51606977]\n",
      " [   33.86305554]\n",
      " [   32.8895146 ]\n",
      " [   38.32832962]\n",
      " [   29.69842025]\n",
      " [   43.12530402]\n",
      " [   44.9725223 ]\n",
      " [   30.26936288]\n",
      " [   42.36237511]\n",
      " [   36.99676359]\n",
      " [   24.56066321]\n",
      " [   29.08319782]\n",
      " [   37.29601908]\n",
      " [   38.02777466]\n",
      " [   41.70793102]\n",
      " [   44.10161158]\n",
      " [   21.90447563]\n",
      " [   43.85093953]\n",
      " [   24.83732467]\n",
      " [   31.99788221]\n",
      " [   45.02701623]\n",
      " [   46.22979495]\n",
      " [   34.09726764]\n",
      " [   41.9123531 ]\n",
      " [   36.81000313]\n",
      " [   34.55695882]\n",
      " [   36.12337968]\n",
      " [   36.9048505 ]\n",
      " [   22.85395541]\n",
      " [   34.87866858]\n",
      " [   29.09529826]\n",
      " [   36.72659948]\n",
      " [   37.26898171]\n",
      " [   39.56179261]\n",
      " [   32.75692668]\n",
      " [   33.44057393]\n",
      " [   34.73480463]\n",
      " [   40.07497167]\n",
      " [   37.11094932]\n",
      " [   34.03633225]\n",
      " [   37.28472905]\n",
      " [   44.43621826]\n",
      " [   39.11800247]\n",
      " [   34.90736873]\n",
      " [   40.47226033]\n",
      " [   38.37952596]\n",
      " [   32.80743836]\n",
      " [   36.11589026]\n",
      " [   31.79417275]\n",
      " [   41.49627103]\n",
      " [   37.23090581]\n",
      " [   36.97612577]\n",
      " [   31.55463702]\n",
      " [   34.96328787]\n",
      " [   36.03255648]\n",
      " [   32.89119133]\n",
      " [   34.71990962]\n",
      " [   33.91060498]\n",
      " [   31.81376261]\n",
      " [   38.88131719]\n",
      " [  227.8590787 ]\n",
      " [ 1941.47039796]\n",
      " [  734.66563946]\n",
      " [   14.23509853]]\n",
      "Accuracy of:  0.23293768546\n",
      "CPU times: user 4.16 s, sys: 68.7 ms, total: 4.23 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    @staticmethod\n",
    "    def line_search_function(eta,X,y,w,grad):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)+np.sum(wnew**2)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ += gradient*eta # set new function values\n",
    "                \n",
    "            \n",
    "\n",
    "lslr = LineSearchLogisticRegression(eta=0.1,iterations=110, C=0.001)\n",
    "\n",
    "lslr.fit(X,y)\n",
    "\n",
    "yhat = lslr.predict(X)\n",
    "print(lslr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[   180.55      ]\n",
      " [   950.47628128]\n",
      " [   648.69707359]\n",
      " [   632.6019047 ]\n",
      " [   441.88531699]\n",
      " [   419.24571471]\n",
      " [   585.8666664 ]\n",
      " [   551.19829444]\n",
      " [   701.28440798]\n",
      " [   748.84377455]\n",
      " [   459.76109959]\n",
      " [   489.89457253]\n",
      " [   574.07760474]\n",
      " [   537.80302882]\n",
      " [   539.07795124]\n",
      " [   617.99387924]\n",
      " [   568.76047728]\n",
      " [   558.83407555]\n",
      " [   462.20116105]\n",
      " [   425.6008177 ]\n",
      " [   928.11766504]\n",
      " [   533.8148814 ]\n",
      " [   664.73165313]\n",
      " [   904.36447159]\n",
      " [   711.73914385]\n",
      " [   609.70005386]\n",
      " [   607.18910663]\n",
      " [   755.82029603]\n",
      " [   758.86380405]\n",
      " [   697.02290684]\n",
      " [   391.47757638]\n",
      " [   680.63779936]\n",
      " [   608.64734068]\n",
      " [   599.82924166]\n",
      " [   490.12747732]\n",
      " [   454.6999631 ]\n",
      " [   389.74341082]\n",
      " [   834.63900559]\n",
      " [   606.40247857]\n",
      " [   495.89074477]\n",
      " [   555.62370245]\n",
      " [   434.82821031]\n",
      " [   637.48914765]\n",
      " [   615.26622689]\n",
      " [   755.44574268]\n",
      " [   512.36148721]\n",
      " [   417.42540405]\n",
      " [   512.56003521]\n",
      " [   525.05493786]\n",
      " [   422.20973392]\n",
      " [   716.77234258]\n",
      " [   459.44893046]\n",
      " [   441.63444915]\n",
      " [   360.13945127]\n",
      " [   656.28061046]\n",
      " [   608.78460612]\n",
      " [   395.92909295]\n",
      " [   479.85170964]\n",
      " [   657.44043762]\n",
      " [   614.40697254]\n",
      " [   581.6250761 ]\n",
      " [   899.43928549]\n",
      " [   551.56388418]\n",
      " [   672.64853786]\n",
      " [   433.48636207]\n",
      " [   392.6441765 ]\n",
      " [   474.66259522]\n",
      " [   542.99857108]\n",
      " [   610.16234458]\n",
      " [   624.34387599]\n",
      " [   488.73488468]\n",
      " [   522.14550232]\n",
      " [   628.89513743]\n",
      " [   590.73417361]\n",
      " [   602.09126293]\n",
      " [   512.88090798]\n",
      " [   599.45541949]\n",
      " [   571.72830863]\n",
      " [   668.91443722]\n",
      " [   530.78567534]\n",
      " [   768.65943824]\n",
      " [   805.22249744]\n",
      " [   547.4140627 ]\n",
      " [   763.22119818]\n",
      " [   668.45891898]\n",
      " [   446.41560984]\n",
      " [   509.26561936]\n",
      " [   653.36601943]\n",
      " [   666.25689005]\n",
      " [   732.88621815]\n",
      " [   775.76626628]\n",
      " [   388.96485262]\n",
      " [   780.00125114]\n",
      " [   444.32864646]\n",
      " [   568.37140642]\n",
      " [   797.82987684]\n",
      " [   821.59514348]\n",
      " [   604.85371782]\n",
      " [   755.17113902]\n",
      " [   647.9079969 ]\n",
      " [   625.69902523]\n",
      " [   645.29661394]\n",
      " [   657.99198295]\n",
      " [   404.89871306]\n",
      " [   628.24902585]\n",
      " [   516.34254151]\n",
      " [   638.34818307]\n",
      " [   661.93147251]\n",
      " [   706.78811592]\n",
      " [   582.51795777]\n",
      " [   589.41090258]\n",
      " [   621.28672298]\n",
      " [   731.34027384]\n",
      " [   663.17192371]\n",
      " [   601.01308705]\n",
      " [   664.52367829]\n",
      " [   786.0022325 ]\n",
      " [   706.42187768]\n",
      " [   626.55524766]\n",
      " [   717.13677493]\n",
      " [   693.18558988]\n",
      " [   579.66355353]\n",
      " [   642.42046533]\n",
      " [   572.73087154]\n",
      " [   744.10668364]\n",
      " [   669.7381263 ]\n",
      " [   655.29264215]\n",
      " [   559.32126368]\n",
      " [   610.5055003 ]\n",
      " [   648.48160734]\n",
      " [   586.84481597]\n",
      " [   628.30641581]\n",
      " [   606.11015702]\n",
      " [   563.72129319]\n",
      " [   686.6457118 ]\n",
      " [  4024.57458428]\n",
      " [ 34450.21108363]\n",
      " [ 12974.97167169]\n",
      " [   257.35832675]]\n",
      "Accuracy of:  0.23293768546\n",
      "CPU times: user 36.3 ms, sys: 3.82 ms, total: 40.1 ms\n",
      "Wall time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "slr = StochasticLogisticRegression(0.1,1000, C=0.001) # take a lot more steps!!\n",
    "\n",
    "slr.fit(X,y)\n",
    "\n",
    "yhat = slr.predict(X)\n",
    "print(slr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[  2.45027462e+170]\n",
      " [ -1.31939092e+172]\n",
      " [  2.62226614e+171]\n",
      " [  2.69990104e+171]\n",
      " [ -8.10370053e+170]\n",
      " [ -1.25622611e+171]\n",
      " [  2.52865447e+176]\n",
      " [ -2.48917192e+175]\n",
      " [ -1.88031351e+175]\n",
      " [ -3.20587632e+175]\n",
      " [ -1.06333001e+176]\n",
      " [ -6.19847138e+174]\n",
      " [ -3.08920150e+174]\n",
      " [ -1.23578703e+174]\n",
      " [  8.72544465e+173]\n",
      " [ -2.69228095e+173]\n",
      " [ -5.04004842e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94636418e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94636418e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94636418e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]]\n",
      "Accuracy of:  0.207715133531\n",
      "CPU times: user 389 ms, sys: 9.55 ms, total: 399 ms\n",
      "Wall time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X + 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "hlr = HessianBinaryLogisticRegression(eta=0.1,iterations=20,C=0.1) # note that we need only a few iterations here\n",
    "\n",
    "hlr.fit(X,y)\n",
    "yhat = hlr.predict(X)\n",
    "print(hlr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 0.0053308 ]\n",
      " [ 0.02543407]\n",
      " [ 0.01737123]\n",
      " [ 0.01652036]\n",
      " [ 0.0119943 ]\n",
      " [ 0.01125106]\n",
      " [ 0.0158728 ]\n",
      " [ 0.01470071]\n",
      " [ 0.0185587 ]\n",
      " [ 0.02034076]\n",
      " [ 0.0127307 ]\n",
      " [ 0.01341243]\n",
      " [ 0.01534826]\n",
      " [ 0.01476222]\n",
      " [ 0.01457085]\n",
      " [ 0.01679544]\n",
      " [ 0.01546786]\n",
      " [ 0.01487498]\n",
      " [ 0.01232577]\n",
      " [ 0.01154665]\n",
      " [ 0.02472159]\n",
      " [ 0.01454181]\n",
      " [ 0.01813156]\n",
      " [ 0.0240279 ]\n",
      " [ 0.01868001]\n",
      " [ 0.01666729]\n",
      " [ 0.01669805]\n",
      " [ 0.02029976]\n",
      " [ 0.02052187]\n",
      " [ 0.01922505]\n",
      " [ 0.01087517]\n",
      " [ 0.01877569]\n",
      " [ 0.01690821]\n",
      " [ 0.01656991]\n",
      " [ 0.01338851]\n",
      " [ 0.01266578]\n",
      " [ 0.01091618]\n",
      " [ 0.02238595]\n",
      " [ 0.01650498]\n",
      " [ 0.01384812]\n",
      " [ 0.01431969]\n",
      " [ 0.0114766 ]\n",
      " [ 0.01726188]\n",
      " [ 0.01653915]\n",
      " [ 0.02029463]\n",
      " [ 0.0133663 ]\n",
      " [ 0.01126132]\n",
      " [ 0.01382762]\n",
      " [ 0.01398652]\n",
      " [ 0.01182857]\n",
      " [ 0.019413  ]\n",
      " [ 0.01248125]\n",
      " [ 0.01196867]\n",
      " [ 0.00996621]\n",
      " [ 0.01791798]\n",
      " [ 0.01666388]\n",
      " [ 0.01015415]\n",
      " [ 0.01263844]\n",
      " [ 0.0174242 ]\n",
      " [ 0.01723113]\n",
      " [ 0.01592406]\n",
      " [ 0.02409112]\n",
      " [ 0.01498263]\n",
      " [ 0.01799999]\n",
      " [ 0.01112975]\n",
      " [ 0.01049245]\n",
      " [ 0.0125308 ]\n",
      " [ 0.01411808]\n",
      " [ 0.01580104]\n",
      " [ 0.01671855]\n",
      " [ 0.01327062]\n",
      " [ 0.0138242 ]\n",
      " [ 0.01665875]\n",
      " [ 0.01584034]\n",
      " [ 0.01606416]\n",
      " [ 0.01359867]\n",
      " [ 0.01608808]\n",
      " [ 0.01564727]\n",
      " [ 0.01817769]\n",
      " [ 0.01403607]\n",
      " [ 0.02046036]\n",
      " [ 0.02134371]\n",
      " [ 0.01435557]\n",
      " [ 0.0200896 ]\n",
      " [ 0.01758139]\n",
      " [ 0.01156032]\n",
      " [ 0.01376269]\n",
      " [ 0.0175626 ]\n",
      " [ 0.01814352]\n",
      " [ 0.01969833]\n",
      " [ 0.02101395]\n",
      " [ 0.01028059]\n",
      " [ 0.02079183]\n",
      " [ 0.01173118]\n",
      " [ 0.01521328]\n",
      " [ 0.02134371]\n",
      " [ 0.02187337]\n",
      " [ 0.01600949]\n",
      " [ 0.01988628]\n",
      " [ 0.01737294]\n",
      " [ 0.01623673]\n",
      " [ 0.01715083]\n",
      " [ 0.01753697]\n",
      " [ 0.01085125]\n",
      " [ 0.01676298]\n",
      " [ 0.01388571]\n",
      " [ 0.01767536]\n",
      " [ 0.01757114]\n",
      " [ 0.01884746]\n",
      " [ 0.01579591]\n",
      " [ 0.01589672]\n",
      " [ 0.01645201]\n",
      " [ 0.01903369]\n",
      " [ 0.01779326]\n",
      " [ 0.01611713]\n",
      " [ 0.0178428 ]\n",
      " [ 0.02058509]\n",
      " [ 0.01852795]\n",
      " [ 0.01645201]\n",
      " [ 0.01930877]\n",
      " [ 0.01841347]\n",
      " [ 0.0156336 ]\n",
      " [ 0.01721575]\n",
      " [ 0.01505439]\n",
      " [ 0.01958557]\n",
      " [ 0.01787014]\n",
      " [ 0.01748058]\n",
      " [ 0.01491599]\n",
      " [ 0.0165101 ]\n",
      " [ 0.01715083]\n",
      " [ 0.01571561]\n",
      " [ 0.01658528]\n",
      " [ 0.01617522]\n",
      " [ 0.01511248]\n",
      " [ 0.01850574]\n",
      " [ 0.10817083]\n",
      " [ 0.92166979]\n",
      " [ 0.34932975]\n",
      " [ 0.00678994]]\n",
      "Accuracy of:  0.23293768546\n",
      "CPU times: user 125 ms, sys: 6.13 ms, total: 132 ms\n",
      "Wall time: 34.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += 2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "bfgslr.fit(X,y)\n",
    "yhat = bfgslr.predict(X)\n",
    "print(bfgslr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiClassLogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-dcfefae92726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr = MultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\\nlr.fit(X,y_not_binary)\\nprint(lr)\\n\\nyhat = lr.predict(X)\\nprint('Accuracy of: ',accuracy_score(y_not_binary,yhat))\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiClassLogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "lr.fit(X,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
