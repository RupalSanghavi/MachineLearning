{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_imputed = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_imputed = df_imputed.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoking\n",
      "Alcohol\n",
      "Punctuality\n",
      "Lying\n",
      "Internet usage\n",
      "Gender\n",
      "Left - right handed\n",
      "Education\n",
      "Only child\n",
      "Village - town\n",
      "House - block of flats\n"
     ]
    }
   ],
   "source": [
    "for col in ['Smoking', 'Alcohol', 'Punctuality', 'Lying', 'Internet usage',\n",
    "        'Gender', 'Left - right handed', 'Education', 'Only child',\n",
    "        'Village - town', 'House - block of flats']:\n",
    "    print(col)\n",
    "    df_imputed = df_imputed.drop(col,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Fear of public speaking' in df_imputed:\n",
    "    y = df_imputed['Fear of public speaking'].values # get the labels we want\n",
    "    del df_imputed['Fear of public speaking'] # get rid of the class label\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = df_imputed.columns.to_series().groupby(df_imputed.dtypes).groups\n",
    "{k.name: v for k, v in g.items()}\n",
    "cols_not_intfloat = ['Smoking', 'Alcohol', 'Punctuality', 'Lying', 'Internet usage',\n",
    "        'Gender', 'Left - right handed', 'Education', 'Only child',\n",
    "        'Village - town', 'House - block of flats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "        \n",
    "blr = BinaryLogisticRegressionBase(0.1)\n",
    "print(blr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi # the actual update inside of sum\n",
    "            gradient += gradi.reshape(self.w_.shape) # reshape to be column vector and add to gradient\n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "            \n",
    "blr = BinaryLogisticRegression(0.1)\n",
    "print(blr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "lr = LogisticRegression(0.1,1500)\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# can we do better? Maybe more iterations?\n",
    "params = dict(eta=0.1,\n",
    "              iterations=500)\n",
    "\n",
    "blr = LogisticRegression(**params)\n",
    "blr.fit(X,y)\n",
    "# print(blr)\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# now lets do some vectorized coding\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "# use same params as defined above\n",
    "blr = VectorBinaryLogisticRegression(**params)\n",
    "blr.fit(X,y)\n",
    "print(blr.w_)\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    # extend init functions\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "        \n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self,X,y):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X,y)\n",
    "        \n",
    "        # add in regularization (to all except bias term)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now redefine the Logistic Regression Function where needed\n",
    "class RegularizedLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                      C=self.C)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = RegularizedLogisticRegression(eta=0.1,iterations=2000) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[    90.77700297]\n",
      " [   455.69584136]\n",
      " [   310.12282053]\n",
      " [   296.38078132]\n",
      " [   214.42342046]\n",
      " [   201.30932795]\n",
      " [   284.12760523]\n",
      " [   263.20815517]\n",
      " [   333.14489982]\n",
      " [   364.99230948]\n",
      " [   228.86059827]\n",
      " [   241.9717399 ]\n",
      " [   274.68285292]\n",
      " [   264.37882356]\n",
      " [   260.24370786]\n",
      " [   300.44206565]\n",
      " [   277.56896059]\n",
      " [   265.78549451]\n",
      " [   220.90069261]\n",
      " [   206.92855259]\n",
      " [   442.66107212]\n",
      " [   260.16446656]\n",
      " [   325.49531991]\n",
      " [   430.09429155]\n",
      " [   335.33045895]\n",
      " [   298.48972215]\n",
      " [   297.55702358]\n",
      " [   364.28971172]\n",
      " [   368.65984636]\n",
      " [   343.53062955]\n",
      " [   193.1951201 ]\n",
      " [   335.3350492 ]\n",
      " [   301.30380176]\n",
      " [   295.91607141]\n",
      " [   236.5990918 ]\n",
      " [   226.2106571 ]\n",
      " [   194.13126135]\n",
      " [   401.21182102]\n",
      " [   294.74523909]\n",
      " [   246.5078163 ]\n",
      " [   257.8183756 ]\n",
      " [   205.99101786]\n",
      " [   310.03972669]\n",
      " [   295.91459597]\n",
      " [   363.12174831]\n",
      " [   239.86763523]\n",
      " [   198.50729774]\n",
      " [   246.35113701]\n",
      " [   250.71823881]\n",
      " [   211.53509964]\n",
      " [   348.52190748]\n",
      " [   221.99802145]\n",
      " [   212.94308208]\n",
      " [   178.36042427]\n",
      " [   319.9578776 ]\n",
      " [   295.29779702]\n",
      " [   182.57322387]\n",
      " [   227.53275878]\n",
      " [   312.07155741]\n",
      " [   307.3914249 ]\n",
      " [   284.13006429]\n",
      " [   430.56441139]\n",
      " [   266.10204987]\n",
      " [   323.6206603 ]\n",
      " [   201.84844301]\n",
      " [   188.73943256]\n",
      " [   226.90481208]\n",
      " [   254.77263776]\n",
      " [   285.99251056]\n",
      " [   302.54026857]\n",
      " [   240.09658848]\n",
      " [   249.15349509]\n",
      " [   301.52537782]\n",
      " [   283.26972165]\n",
      " [   285.84943808]\n",
      " [   242.214218  ]\n",
      " [   287.64108582]\n",
      " [   279.36806749]\n",
      " [   325.5753809 ]\n",
      " [   252.27757258]\n",
      " [   366.32146047]\n",
      " [   382.01125293]\n",
      " [   257.11946644]\n",
      " [   359.84230303]\n",
      " [   314.25883788]\n",
      " [   208.64186069]\n",
      " [   247.0486527 ]\n",
      " [   316.82691475]\n",
      " [   323.004763  ]\n",
      " [   354.29633598]\n",
      " [   374.59988874]\n",
      " [   186.08244208]\n",
      " [   372.48734128]\n",
      " [   210.98549258]\n",
      " [   271.79600753]\n",
      " [   382.47833993]\n",
      " [   392.70181649]\n",
      " [   289.66135894]\n",
      " [   356.01800489]\n",
      " [   312.69188102]\n",
      " [   293.56465248]\n",
      " [   306.84263753]\n",
      " [   313.47822836]\n",
      " [   194.12814654]\n",
      " [   296.23672521]\n",
      " [   247.13240229]\n",
      " [   311.92791115]\n",
      " [   316.59378109]\n",
      " [   336.0391224 ]\n",
      " [   278.20748126]\n",
      " [   284.05090496]\n",
      " [   295.05408939]\n",
      " [   340.40753569]\n",
      " [   315.20317602]\n",
      " [   289.12158814]\n",
      " [   316.68466196]\n",
      " [   377.53753227]\n",
      " [   332.28709821]\n",
      " [   296.53319823]\n",
      " [   343.76818952]\n",
      " [   325.97593176]\n",
      " [   278.66678123]\n",
      " [   306.76790451]\n",
      " [   270.07573209]\n",
      " [   352.50042586]\n",
      " [   316.21888645]\n",
      " [   314.09830606]\n",
      " [   268.04504894]\n",
      " [   297.00307218]\n",
      " [   306.06415919]\n",
      " [   279.37134624]\n",
      " [   294.90478729]\n",
      " [   288.0345054 ]\n",
      " [   270.2342147 ]\n",
      " [   330.26198894]\n",
      " [  1935.50433196]\n",
      " [ 16491.43991674]\n",
      " [  6240.38185302]\n",
      " [   120.91203859]]\n",
      "Accuracy of:  0.23293768546\n",
      "CPU times: user 413 ms, sys: 13.8 ms, total: 427 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "blr.fit(X,y)\n",
    "print(blr)\n",
    "\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression() # all params default\n",
    "lr_sk.fit(X,y)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[   11.17005256]\n",
      " [   53.64658572]\n",
      " [   36.51024463]\n",
      " [   34.89088082]\n",
      " [   25.24340221]\n",
      " [   23.69933782]\n",
      " [   33.44906939]\n",
      " [   30.98625141]\n",
      " [   39.21866253]\n",
      " [   42.96797028]\n",
      " [   26.94169838]\n",
      " [   28.4842537 ]\n",
      " [   32.33723963]\n",
      " [   31.12399529]\n",
      " [   30.63793742]\n",
      " [   35.36991036]\n",
      " [   32.67621979]\n",
      " [   31.29024367]\n",
      " [   26.005367  ]\n",
      " [   24.3605168 ]\n",
      " [   52.11235818]\n",
      " [   30.62814249]\n",
      " [   38.3181155 ]\n",
      " [   50.63308568]\n",
      " [   39.47594372]\n",
      " [   35.13970846]\n",
      " [   35.03147514]\n",
      " [   42.88522335]\n",
      " [   43.39928271]\n",
      " [   40.44295936]\n",
      " [   22.74555442]\n",
      " [   39.47829115]\n",
      " [   35.4725825 ]\n",
      " [   34.83760253]\n",
      " [   27.85698671]\n",
      " [   26.63130657]\n",
      " [   22.85554831]\n",
      " [   47.23234342]\n",
      " [   34.69977482]\n",
      " [   29.02171749]\n",
      " [   30.35022348]\n",
      " [   24.2498103 ]\n",
      " [   36.49847953]\n",
      " [   34.836848  ]\n",
      " [   42.74886279]\n",
      " [   28.23789924]\n",
      " [   23.37262578]\n",
      " [   29.00304983]\n",
      " [   29.5155582 ]\n",
      " [   24.90329026]\n",
      " [   41.02883892]\n",
      " [   26.13633406]\n",
      " [   25.07020934]\n",
      " [   20.99761289]\n",
      " [   37.66803093]\n",
      " [   34.76724947]\n",
      " [   21.49266924]\n",
      " [   26.78503533]\n",
      " [   36.73860212]\n",
      " [   36.18892609]\n",
      " [   33.45032694]\n",
      " [   50.68913058]\n",
      " [   31.32921381]\n",
      " [   38.09691209]\n",
      " [   23.75993787]\n",
      " [   22.21847242]\n",
      " [   26.70973591]\n",
      " [   29.9910666 ]\n",
      " [   33.66528452]\n",
      " [   35.61324669]\n",
      " [   28.26279878]\n",
      " [   29.32992954]\n",
      " [   35.49370938]\n",
      " [   33.34836741]\n",
      " [   33.6535753 ]\n",
      " [   28.51606977]\n",
      " [   33.86305554]\n",
      " [   32.8895146 ]\n",
      " [   38.32832962]\n",
      " [   29.69842025]\n",
      " [   43.12530402]\n",
      " [   44.9725223 ]\n",
      " [   30.26936288]\n",
      " [   42.36237511]\n",
      " [   36.99676359]\n",
      " [   24.56066321]\n",
      " [   29.08319782]\n",
      " [   37.29601908]\n",
      " [   38.02777466]\n",
      " [   41.70793102]\n",
      " [   44.10161158]\n",
      " [   21.90447563]\n",
      " [   43.85093953]\n",
      " [   24.83732467]\n",
      " [   31.99788221]\n",
      " [   45.02701623]\n",
      " [   46.22979495]\n",
      " [   34.09726764]\n",
      " [   41.9123531 ]\n",
      " [   36.81000313]\n",
      " [   34.55695882]\n",
      " [   36.12337968]\n",
      " [   36.9048505 ]\n",
      " [   22.85395541]\n",
      " [   34.87866858]\n",
      " [   29.09529826]\n",
      " [   36.72659948]\n",
      " [   37.26898171]\n",
      " [   39.56179261]\n",
      " [   32.75692668]\n",
      " [   33.44057393]\n",
      " [   34.73480463]\n",
      " [   40.07497167]\n",
      " [   37.11094932]\n",
      " [   34.03633225]\n",
      " [   37.28472905]\n",
      " [   44.43621826]\n",
      " [   39.11800247]\n",
      " [   34.90736873]\n",
      " [   40.47226033]\n",
      " [   38.37952596]\n",
      " [   32.80743836]\n",
      " [   36.11589026]\n",
      " [   31.79417275]\n",
      " [   41.49627103]\n",
      " [   37.23090581]\n",
      " [   36.97612577]\n",
      " [   31.55463702]\n",
      " [   34.96328787]\n",
      " [   36.03255648]\n",
      " [   32.89119133]\n",
      " [   34.71990962]\n",
      " [   33.91060498]\n",
      " [   31.81376261]\n",
      " [   38.88131719]\n",
      " [  227.8590787 ]\n",
      " [ 1941.47039796]\n",
      " [  734.66563946]\n",
      " [   14.23509853]]\n",
      "Accuracy of:  0.23293768546\n",
      "CPU times: user 4.16 s, sys: 68.7 ms, total: 4.23 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    @staticmethod\n",
    "    def line_search_function(eta,X,y,w,grad):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)+np.sum(wnew**2)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ += gradient*eta # set new function values\n",
    "                \n",
    "            \n",
    "\n",
    "lslr = LineSearchLogisticRegression(eta=0.1,iterations=110, C=0.001)\n",
    "\n",
    "lslr.fit(X,y)\n",
    "\n",
    "yhat = lslr.predict(X)\n",
    "print(lslr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[   180.55      ]\n",
      " [   950.47628128]\n",
      " [   648.69707359]\n",
      " [   632.6019047 ]\n",
      " [   441.88531699]\n",
      " [   419.24571471]\n",
      " [   585.8666664 ]\n",
      " [   551.19829444]\n",
      " [   701.28440798]\n",
      " [   748.84377455]\n",
      " [   459.76109959]\n",
      " [   489.89457253]\n",
      " [   574.07760474]\n",
      " [   537.80302882]\n",
      " [   539.07795124]\n",
      " [   617.99387924]\n",
      " [   568.76047728]\n",
      " [   558.83407555]\n",
      " [   462.20116105]\n",
      " [   425.6008177 ]\n",
      " [   928.11766504]\n",
      " [   533.8148814 ]\n",
      " [   664.73165313]\n",
      " [   904.36447159]\n",
      " [   711.73914385]\n",
      " [   609.70005386]\n",
      " [   607.18910663]\n",
      " [   755.82029603]\n",
      " [   758.86380405]\n",
      " [   697.02290684]\n",
      " [   391.47757638]\n",
      " [   680.63779936]\n",
      " [   608.64734068]\n",
      " [   599.82924166]\n",
      " [   490.12747732]\n",
      " [   454.6999631 ]\n",
      " [   389.74341082]\n",
      " [   834.63900559]\n",
      " [   606.40247857]\n",
      " [   495.89074477]\n",
      " [   555.62370245]\n",
      " [   434.82821031]\n",
      " [   637.48914765]\n",
      " [   615.26622689]\n",
      " [   755.44574268]\n",
      " [   512.36148721]\n",
      " [   417.42540405]\n",
      " [   512.56003521]\n",
      " [   525.05493786]\n",
      " [   422.20973392]\n",
      " [   716.77234258]\n",
      " [   459.44893046]\n",
      " [   441.63444915]\n",
      " [   360.13945127]\n",
      " [   656.28061046]\n",
      " [   608.78460612]\n",
      " [   395.92909295]\n",
      " [   479.85170964]\n",
      " [   657.44043762]\n",
      " [   614.40697254]\n",
      " [   581.6250761 ]\n",
      " [   899.43928549]\n",
      " [   551.56388418]\n",
      " [   672.64853786]\n",
      " [   433.48636207]\n",
      " [   392.6441765 ]\n",
      " [   474.66259522]\n",
      " [   542.99857108]\n",
      " [   610.16234458]\n",
      " [   624.34387599]\n",
      " [   488.73488468]\n",
      " [   522.14550232]\n",
      " [   628.89513743]\n",
      " [   590.73417361]\n",
      " [   602.09126293]\n",
      " [   512.88090798]\n",
      " [   599.45541949]\n",
      " [   571.72830863]\n",
      " [   668.91443722]\n",
      " [   530.78567534]\n",
      " [   768.65943824]\n",
      " [   805.22249744]\n",
      " [   547.4140627 ]\n",
      " [   763.22119818]\n",
      " [   668.45891898]\n",
      " [   446.41560984]\n",
      " [   509.26561936]\n",
      " [   653.36601943]\n",
      " [   666.25689005]\n",
      " [   732.88621815]\n",
      " [   775.76626628]\n",
      " [   388.96485262]\n",
      " [   780.00125114]\n",
      " [   444.32864646]\n",
      " [   568.37140642]\n",
      " [   797.82987684]\n",
      " [   821.59514348]\n",
      " [   604.85371782]\n",
      " [   755.17113902]\n",
      " [   647.9079969 ]\n",
      " [   625.69902523]\n",
      " [   645.29661394]\n",
      " [   657.99198295]\n",
      " [   404.89871306]\n",
      " [   628.24902585]\n",
      " [   516.34254151]\n",
      " [   638.34818307]\n",
      " [   661.93147251]\n",
      " [   706.78811592]\n",
      " [   582.51795777]\n",
      " [   589.41090258]\n",
      " [   621.28672298]\n",
      " [   731.34027384]\n",
      " [   663.17192371]\n",
      " [   601.01308705]\n",
      " [   664.52367829]\n",
      " [   786.0022325 ]\n",
      " [   706.42187768]\n",
      " [   626.55524766]\n",
      " [   717.13677493]\n",
      " [   693.18558988]\n",
      " [   579.66355353]\n",
      " [   642.42046533]\n",
      " [   572.73087154]\n",
      " [   744.10668364]\n",
      " [   669.7381263 ]\n",
      " [   655.29264215]\n",
      " [   559.32126368]\n",
      " [   610.5055003 ]\n",
      " [   648.48160734]\n",
      " [   586.84481597]\n",
      " [   628.30641581]\n",
      " [   606.11015702]\n",
      " [   563.72129319]\n",
      " [   686.6457118 ]\n",
      " [  4024.57458428]\n",
      " [ 34450.21108363]\n",
      " [ 12974.97167169]\n",
      " [   257.35832675]]\n",
      "Accuracy of:  0.23293768546\n",
      "CPU times: user 36.3 ms, sys: 3.82 ms, total: 40.1 ms\n",
      "Wall time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "slr = StochasticLogisticRegression(0.1,1000, C=0.001) # take a lot more steps!!\n",
    "\n",
    "slr.fit(X,y)\n",
    "\n",
    "yhat = slr.predict(X)\n",
    "print(slr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[  2.45027462e+170]\n",
      " [ -1.31939092e+172]\n",
      " [  2.62226614e+171]\n",
      " [  2.69990104e+171]\n",
      " [ -8.10370053e+170]\n",
      " [ -1.25622611e+171]\n",
      " [  2.52865447e+176]\n",
      " [ -2.48917192e+175]\n",
      " [ -1.88031351e+175]\n",
      " [ -3.20587632e+175]\n",
      " [ -1.06333001e+176]\n",
      " [ -6.19847138e+174]\n",
      " [ -3.08920150e+174]\n",
      " [ -1.23578703e+174]\n",
      " [  8.72544465e+173]\n",
      " [ -2.69228095e+173]\n",
      " [ -5.04004842e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94636418e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94636418e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94636418e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]\n",
      " [ -4.94630900e+173]]\n",
      "Accuracy of:  0.207715133531\n",
      "CPU times: user 389 ms, sys: 9.55 ms, total: 399 ms\n",
      "Wall time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X + 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "hlr = HessianBinaryLogisticRegression(eta=0.1,iterations=20,C=0.1) # note that we need only a few iterations here\n",
    "\n",
    "hlr.fit(X,y)\n",
    "yhat = hlr.predict(X)\n",
    "print(hlr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 0.0053308 ]\n",
      " [ 0.02543407]\n",
      " [ 0.01737123]\n",
      " [ 0.01652036]\n",
      " [ 0.0119943 ]\n",
      " [ 0.01125106]\n",
      " [ 0.0158728 ]\n",
      " [ 0.01470071]\n",
      " [ 0.0185587 ]\n",
      " [ 0.02034076]\n",
      " [ 0.0127307 ]\n",
      " [ 0.01341243]\n",
      " [ 0.01534826]\n",
      " [ 0.01476222]\n",
      " [ 0.01457085]\n",
      " [ 0.01679544]\n",
      " [ 0.01546786]\n",
      " [ 0.01487498]\n",
      " [ 0.01232577]\n",
      " [ 0.01154665]\n",
      " [ 0.02472159]\n",
      " [ 0.01454181]\n",
      " [ 0.01813156]\n",
      " [ 0.0240279 ]\n",
      " [ 0.01868001]\n",
      " [ 0.01666729]\n",
      " [ 0.01669805]\n",
      " [ 0.02029976]\n",
      " [ 0.02052187]\n",
      " [ 0.01922505]\n",
      " [ 0.01087517]\n",
      " [ 0.01877569]\n",
      " [ 0.01690821]\n",
      " [ 0.01656991]\n",
      " [ 0.01338851]\n",
      " [ 0.01266578]\n",
      " [ 0.01091618]\n",
      " [ 0.02238595]\n",
      " [ 0.01650498]\n",
      " [ 0.01384812]\n",
      " [ 0.01431969]\n",
      " [ 0.0114766 ]\n",
      " [ 0.01726188]\n",
      " [ 0.01653915]\n",
      " [ 0.02029463]\n",
      " [ 0.0133663 ]\n",
      " [ 0.01126132]\n",
      " [ 0.01382762]\n",
      " [ 0.01398652]\n",
      " [ 0.01182857]\n",
      " [ 0.019413  ]\n",
      " [ 0.01248125]\n",
      " [ 0.01196867]\n",
      " [ 0.00996621]\n",
      " [ 0.01791798]\n",
      " [ 0.01666388]\n",
      " [ 0.01015415]\n",
      " [ 0.01263844]\n",
      " [ 0.0174242 ]\n",
      " [ 0.01723113]\n",
      " [ 0.01592406]\n",
      " [ 0.02409112]\n",
      " [ 0.01498263]\n",
      " [ 0.01799999]\n",
      " [ 0.01112975]\n",
      " [ 0.01049245]\n",
      " [ 0.0125308 ]\n",
      " [ 0.01411808]\n",
      " [ 0.01580104]\n",
      " [ 0.01671855]\n",
      " [ 0.01327062]\n",
      " [ 0.0138242 ]\n",
      " [ 0.01665875]\n",
      " [ 0.01584034]\n",
      " [ 0.01606416]\n",
      " [ 0.01359867]\n",
      " [ 0.01608808]\n",
      " [ 0.01564727]\n",
      " [ 0.01817769]\n",
      " [ 0.01403607]\n",
      " [ 0.02046036]\n",
      " [ 0.02134371]\n",
      " [ 0.01435557]\n",
      " [ 0.0200896 ]\n",
      " [ 0.01758139]\n",
      " [ 0.01156032]\n",
      " [ 0.01376269]\n",
      " [ 0.0175626 ]\n",
      " [ 0.01814352]\n",
      " [ 0.01969833]\n",
      " [ 0.02101395]\n",
      " [ 0.01028059]\n",
      " [ 0.02079183]\n",
      " [ 0.01173118]\n",
      " [ 0.01521328]\n",
      " [ 0.02134371]\n",
      " [ 0.02187337]\n",
      " [ 0.01600949]\n",
      " [ 0.01988628]\n",
      " [ 0.01737294]\n",
      " [ 0.01623673]\n",
      " [ 0.01715083]\n",
      " [ 0.01753697]\n",
      " [ 0.01085125]\n",
      " [ 0.01676298]\n",
      " [ 0.01388571]\n",
      " [ 0.01767536]\n",
      " [ 0.01757114]\n",
      " [ 0.01884746]\n",
      " [ 0.01579591]\n",
      " [ 0.01589672]\n",
      " [ 0.01645201]\n",
      " [ 0.01903369]\n",
      " [ 0.01779326]\n",
      " [ 0.01611713]\n",
      " [ 0.0178428 ]\n",
      " [ 0.02058509]\n",
      " [ 0.01852795]\n",
      " [ 0.01645201]\n",
      " [ 0.01930877]\n",
      " [ 0.01841347]\n",
      " [ 0.0156336 ]\n",
      " [ 0.01721575]\n",
      " [ 0.01505439]\n",
      " [ 0.01958557]\n",
      " [ 0.01787014]\n",
      " [ 0.01748058]\n",
      " [ 0.01491599]\n",
      " [ 0.0165101 ]\n",
      " [ 0.01715083]\n",
      " [ 0.01571561]\n",
      " [ 0.01658528]\n",
      " [ 0.01617522]\n",
      " [ 0.01511248]\n",
      " [ 0.01850574]\n",
      " [ 0.10817083]\n",
      " [ 0.92166979]\n",
      " [ 0.34932975]\n",
      " [ 0.00678994]]\n",
      "Accuracy of:  0.23293768546\n",
      "CPU times: user 125 ms, sys: 6.13 ms, total: 132 ms\n",
      "Wall time: 34.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += 2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "bfgslr.fit(X,y)\n",
    "yhat = bfgslr.predict(X)\n",
    "print(bfgslr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            hblr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ -1.68977348e-02  -6.54692943e-02   1.85648173e-01  -6.94735652e-03\n",
      "   -4.13950336e-02   1.15007357e-02   4.66314642e-02  -1.01079006e-02\n",
      "    1.68426711e-02   2.27031224e-01  -1.07720407e-01  -2.56570631e-01\n",
      "   -2.41356088e-01   1.68654003e-01  -1.94460663e-01   2.24846726e-02\n",
      "   -7.94882119e-02  -4.28154610e-02  -1.93635604e-01  -8.12881598e-02\n",
      "   -2.29203578e-01   2.34170647e-02  -4.19517079e-01   1.93012442e-01\n",
      "    7.78681739e-02  -1.99817865e-01   4.15530303e-02  -9.91375127e-02\n",
      "   -1.18865044e-01  -1.20559615e-01   8.22710360e-02   2.36236127e-02\n",
      "    6.07405829e-02   1.15526457e-01  -1.19166210e-01   2.35682099e-02\n",
      "    1.17088044e-02  -1.76163282e-02   3.78084858e-01  -5.12029460e-02\n",
      "   -1.49398345e-01  -1.01647155e-01   7.33428471e-04  -6.87444845e-02\n",
      "   -1.83340989e-01  -1.40875967e-01   2.82762920e-01  -3.39776023e-02\n",
      "    1.18769818e-01   1.44380840e-01  -2.64865201e-01  -1.78614749e-01\n",
      "    1.80375985e-01  -3.53049625e-02   1.19764936e-01   2.41210941e-01\n",
      "   -4.02310234e-02  -1.27524221e-01   9.12335427e-02   1.11640271e-01\n",
      "    2.11688390e-01  -1.86559180e-01  -1.18539324e-01  -1.26359155e-01\n",
      "   -2.17250659e-01   1.84417098e-01   2.12574262e-02  -6.64472673e-02\n",
      "    1.74508427e-01  -2.16627072e-01  -9.61959134e-02   1.63226248e-01\n",
      "   -2.74113662e-01  -3.28768052e-02   3.01641279e-02   3.68639007e-01\n",
      "   -6.25692587e-02  -8.14847498e-02  -3.72114353e-03  -2.80643887e-01\n",
      "    1.20819901e-01   1.85283935e-01  -1.59245345e-01  -2.17301453e-01\n",
      "    1.54308468e-01  -1.84971016e-01   8.61378276e-02   4.95775029e-02\n",
      "   -9.13380736e-02  -5.51371948e-02   3.61353131e-01  -2.43276245e-02\n",
      "   -6.28465749e-02  -4.92817161e-02   4.58955393e-02   2.51802421e-01\n",
      "   -1.59428837e-02   1.45345957e-01  -1.26838990e-01  -2.73628129e-01\n",
      "   -7.28225098e-03   3.52746850e-01   1.88253098e-01  -9.79939125e-02\n",
      "    3.89365178e-01  -1.32308040e-01   5.36311499e-01  -2.79698927e-03\n",
      "    2.41982932e-01   1.32796480e-01   9.38769974e-03   2.17650067e-01\n",
      "    6.79651267e-02   2.61096543e-01   2.66169112e-01   3.72695606e-01\n",
      "   -9.45104709e-01  -7.49103195e-02   5.11691476e-02   5.18084812e-02\n",
      "    2.19765943e-01  -1.05427996e-01   1.87304886e-01  -2.18351940e-02\n",
      "   -8.36519229e-02  -2.10751609e-02  -7.43199774e-02  -8.23665763e-02\n",
      "    1.53122961e-01   9.82924668e-02  -5.74550106e-02   9.03759749e-02\n",
      "    3.06045074e-02  -1.93102505e-01   1.94554151e-01  -4.46746018e-03\n",
      "   -5.00711762e-02   3.15333040e-02   2.84342578e-02]\n",
      " [ -2.62982754e-03  -8.91964239e-02   1.03923967e-01  -2.89686767e-01\n",
      "    7.74239143e-02  -1.60156942e-01  -1.61385780e-02   1.00201318e-01\n",
      "   -2.10509979e-01  -2.66484606e-01  -1.76971760e-01  -1.14235766e-01\n",
      "   -6.72184499e-03   7.44216573e-02   2.30158647e-01   6.92869783e-02\n",
      "   -3.04162147e-01  -1.05213459e-01   2.01397177e-01  -9.05973194e-02\n",
      "    1.26636213e-01   2.65585085e-03   2.24795527e-01  -1.11545002e-01\n",
      "    7.96419129e-03   9.90939965e-02   3.15757531e-02   2.31397137e-01\n",
      "   -3.72972340e-02  -6.61011222e-02   4.63120539e-01  -1.92383763e-01\n",
      "   -1.64772975e-01  -1.89497112e-01   3.33188756e-01  -1.25461131e-01\n",
      "    2.53455811e-01  -1.43216195e-01  -1.25232749e-01  -4.89159165e-02\n",
      "   -6.97427324e-02   8.15541140e-02  -1.74794620e-01  -2.56802488e-02\n",
      "    1.64360368e-01  -7.78443778e-02   9.63500061e-03  -1.37795792e-01\n",
      "   -1.34091468e-01  -3.46641908e-02   2.37345029e-01   2.42861061e-01\n",
      "   -1.89860137e-01   6.47017145e-02  -6.99698844e-02  -4.13463282e-02\n",
      "   -7.85442607e-02   6.53146573e-02  -1.99811270e-01   2.84058140e-02\n",
      "    2.04431573e-01   4.22657606e-02  -1.09338478e-01   8.26823654e-02\n",
      "   -1.23168336e-01   1.90367985e-02  -1.65330052e-01  -1.38627028e-01\n",
      "   -7.73216352e-02  -1.09626966e-01   1.15594727e-01  -1.48795437e-01\n",
      "    9.76692069e-02  -7.11100049e-02   5.65308107e-02  -1.27978017e-01\n",
      "    1.19891508e-01  -8.10914460e-02   1.21488917e-01   7.16882337e-02\n",
      "   -1.32163139e-01  -5.48536034e-02   4.76418790e-02   1.68136782e-01\n",
      "   -2.62397754e-01   1.25856406e-02  -6.92962395e-02  -6.81060723e-02\n",
      "    1.12695582e-01   9.23681368e-02  -1.12305829e-01  -1.43583555e-01\n",
      "   -3.11158963e-02   1.26360432e-01   2.88819376e-02  -1.50775101e-01\n",
      "    6.19703380e-02  -6.08551341e-02   1.98873658e-02   1.47786712e-01\n",
      "   -2.90367386e-01   9.40117825e-02  -2.14580072e-01  -7.49470124e-02\n",
      "    3.48207733e-02   1.08575691e-01  -5.71245409e-03  -1.06515149e-01\n",
      "    3.85569304e-01   2.17348794e-01   1.85967144e-01   1.60290969e-01\n",
      "   -1.03647476e-01   1.53760948e-01  -1.53975240e-01  -1.42505041e-01\n",
      "   -4.36133855e-01   2.42995081e-01   1.38904205e-01  -2.51104578e-01\n",
      "   -1.01170463e-01  -4.05331028e-02  -8.17063310e-03  -2.57911836e-02\n",
      "   -2.02600452e-02   9.82384731e-04  -3.90653869e-01   5.22806030e-02\n",
      "   -3.64405062e-01   2.66326415e-01  -6.57431570e-02  -1.38577009e-01\n",
      "    4.67106032e-02  -1.53777123e-01  -9.57571932e-03   1.31478267e-02\n",
      "    3.32994963e-02  -6.05170148e-03  -3.57175528e-02]\n",
      " [  5.49891688e-03  -2.74598809e-02  -2.33354413e-01  -1.26797704e-02\n",
      "    9.92260100e-02   9.19828215e-02  -8.13605680e-02   5.78163122e-02\n",
      "    4.74067004e-02   1.54693192e-01   9.19397215e-03   1.44465160e-01\n",
      "    4.91033661e-02  -3.53141343e-01  -5.43038567e-03  -2.13745695e-01\n",
      "    2.33115216e-01  -1.09720909e-02   1.09833279e-02  -1.08136418e-01\n",
      "    2.06970107e-01  -1.23526182e-01   2.60022793e-01  -7.64830467e-02\n",
      "   -1.36732239e-01   2.91483549e-03  -6.26482255e-03  -3.55118634e-01\n",
      "    2.73043291e-01   2.98734309e-01  -1.50268915e-01   1.52976220e-01\n",
      "   -8.26137389e-02   3.03844776e-02  -1.18497273e-01   9.77384626e-02\n",
      "   -1.35351163e-02   7.30162661e-02   4.31308625e-03  -4.82802649e-02\n",
      "    1.45042205e-01  -5.59058410e-03   1.21761319e-01  -3.94517834e-02\n",
      "   -5.02283239e-02   6.44134512e-02   7.29920783e-03  -3.65964216e-02\n",
      "   -8.00295358e-02  -3.06877995e-01   1.61130311e-01   2.49101776e-02\n",
      "   -4.13134693e-02  -8.68546485e-02   7.32904674e-02  -1.01189856e-01\n",
      "   -4.23794940e-02   1.93009044e-01   6.44332929e-02  -1.31906474e-01\n",
      "   -1.38709103e-01   1.04057249e-01   5.39243358e-03  -7.65994403e-02\n",
      "    1.32073341e-01   2.12365856e-01  -2.54340096e-02   1.18014100e-01\n",
      "   -2.25003902e-01   8.58041879e-02  -1.20997909e-02  -5.21494775e-02\n",
      "   -2.27639802e-02  -1.22789359e-01   4.28348918e-02   1.34359909e-01\n",
      "   -1.89087853e-02  -8.39231686e-02   2.73378163e-02   3.68806421e-02\n",
      "   -2.56553300e-01  -8.84771456e-02   1.18975796e-01   1.75151147e-02\n",
      "    1.25479377e-01  -1.30875642e-01  -5.31638923e-02  -1.80148406e-02\n",
      "    8.04625926e-02  -6.90160235e-02   1.59562727e-01  -9.28320926e-02\n",
      "    5.68033956e-02  -2.43246872e-01   5.20118559e-02   6.00122715e-02\n",
      "   -3.46635672e-02  -4.21184308e-02  -1.15447001e-01   1.00398724e-01\n",
      "    1.87625187e-01  -4.79978278e-02  -1.14116562e-01   2.28786576e-01\n",
      "   -5.72946453e-03   2.19533778e-01  -2.06717310e-01   9.18726973e-02\n",
      "   -1.65915478e-01   1.95804108e-02  -1.74159672e-01  -2.68466645e-01\n",
      "   -2.81689454e-02  -1.70035370e-01  -7.76590568e-02   3.38826682e-02\n",
      "    2.43561433e-01  -6.34339862e-02  -3.85966145e-02   1.48699412e-01\n",
      "    1.01391799e-01   1.83891590e-01  -1.34283935e-01  -3.52415439e-02\n",
      "   -6.34526781e-02   7.31319065e-02  -7.08946245e-04   1.27952014e-02\n",
      "    1.24450896e-01  -1.55487663e-01   1.64067606e-01  -1.41505344e-01\n",
      "    1.96955360e-01   5.16548893e-02  -1.16178350e-01  -5.14002650e-02\n",
      "    1.88958162e-03  -1.11233826e-02   1.89050357e-01]\n",
      " [ -7.89000830e-03  -7.50203159e-02  -1.05494610e-01   1.64052164e-01\n",
      "   -9.95602394e-02  -9.42666112e-02  -7.31356353e-02  -3.43333650e-02\n",
      "    2.41809807e-01  -6.06677264e-02   2.10017799e-01   5.18233792e-02\n",
      "    2.72725107e-01   7.50052748e-03   3.36960621e-02   2.96514041e-01\n",
      "   -1.68975968e-01   1.87828155e-01  -9.57654106e-03   3.62870415e-01\n",
      "   -1.47972196e-01  -1.45408690e-01  -2.15044445e-01  -5.72631642e-02\n",
      "    5.07132367e-02   1.54843417e-01  -9.95229086e-02   2.60681112e-02\n",
      "   -1.87190173e-01  -5.28536420e-02  -2.62914977e-01  -2.41504178e-01\n",
      "    1.07807236e-01  -3.73379895e-02  -1.71230717e-01  -3.59933876e-02\n",
      "   -9.94719437e-02   6.60819075e-02  -1.05424198e-02  -6.81790382e-02\n",
      "    6.36541818e-02  -1.11805039e-01  -6.22135042e-02   1.57660676e-01\n",
      "    2.48948298e-02   6.33592555e-02  -1.55365449e-01   1.87115786e-01\n",
      "    8.60205648e-02   5.68560782e-02  -3.07202336e-01  -3.11482828e-01\n",
      "    1.97170969e-01  -4.12583908e-02  -1.09232104e-01   6.48978577e-02\n",
      "    1.28279494e-01   3.56508289e-02  -2.53080959e-02   1.51838556e-01\n",
      "    2.24855524e-02  -1.60432482e-01   2.37384887e-01   1.01961353e-01\n",
      "    7.42321834e-02  -1.26212236e-01   9.92154952e-02   2.30706143e-02\n",
      "    1.35555010e-01   1.17143476e-01   4.54025286e-02   1.51871095e-01\n",
      "    3.80683452e-02   5.82377714e-02  -4.92795745e-02  -1.90780875e-01\n",
      "   -1.90744611e-01   1.46030781e-02  -1.60841282e-01   1.59449714e-01\n",
      "    2.23434106e-01  -1.46908747e-02  -5.13960309e-02  -4.17736461e-02\n",
      "   -2.02466495e-01   1.00461769e-01   5.17065328e-02  -2.07369702e-02\n",
      "   -1.24715391e-01   2.37497386e-03  -2.94428264e-01   7.21489158e-02\n",
      "    9.36939650e-02  -1.04136250e-02   3.15354822e-02   9.34551620e-02\n",
      "    1.02201112e-01  -4.75600260e-02   9.86282962e-02   1.24582642e-01\n",
      "   -1.44405119e-01  -2.27097485e-01  -9.58804732e-02   5.15366274e-02\n",
      "   -2.57366796e-01  -1.47136286e-01  -2.33106050e-01   1.26849495e-01\n",
      "   -1.39003842e-01  -4.44244910e-02   2.64070871e-01   2.16829297e-01\n",
      "    1.39157348e-01   1.36128992e-01  -5.78190636e-02  -4.07450014e-02\n",
      "    4.82029308e-01  -3.07348584e-02  -2.52340589e-01  -1.48028532e-01\n",
      "    3.35237726e-02  -3.63121236e-02   6.74863132e-02  -1.10997105e-01\n",
      "    2.24975191e-02  -1.02497947e-01   2.14572090e-01  -9.62854863e-02\n",
      "    1.11926664e-01  -1.06267900e-01  -1.13743579e-01   1.46729078e-01\n",
      "   -1.91464052e-01   9.62195898e-02  -1.13605861e-01   6.48927053e-04\n",
      "   -1.74468196e-03   3.93069301e-03  -1.54664367e-01]\n",
      " [  2.22758338e-03   2.07828527e-01  -5.35449237e-02   2.58776913e-01\n",
      "    1.12139087e-02   8.07369546e-02  -1.25410245e-02  -3.87745398e-02\n",
      "   -1.68017881e-01  -2.71346319e-02   2.89873784e-03   1.38021653e-01\n",
      "   -9.84070575e-02   3.28816073e-01  -2.10798701e-01  -3.02019062e-01\n",
      "    1.33519617e-01   1.28268448e-02   1.20500147e-01   1.06058710e-02\n",
      "   -1.32418432e-01   1.65066849e-01   9.77241921e-02   9.84489556e-02\n",
      "    8.86911043e-02  -2.85371050e-01   2.49855756e-02   2.23221578e-01\n",
      "    1.05724461e-01  -2.32392074e-01  -4.92339157e-02   1.77934713e-01\n",
      "    7.25629998e-02   1.29333169e-02  -1.76390842e-02  -1.78219406e-02\n",
      "   -1.98223011e-01  -7.90277220e-02   7.95313542e-02   3.58394747e-01\n",
      "   -4.41647397e-02  -7.42614430e-02   4.65676798e-02   1.56406230e-01\n",
      "    5.87297182e-03  -9.58023981e-02  -2.76798625e-01   1.30607939e-02\n",
      "    3.49255615e-01   3.63373126e-01   4.47745123e-01   9.88905585e-02\n",
      "   -3.35383759e-01   1.85590804e-01  -3.71090448e-02  -1.82900375e-01\n",
      "    9.06565017e-02  -2.62520531e-01  -1.10150030e-02  -7.06079246e-02\n",
      "   -2.22152120e-01   1.50687843e-01   4.73792135e-02  -4.51553103e-02\n",
      "   -1.29258910e-02  -3.20983520e-01   1.91457872e-01  -2.38186021e-02\n",
      "    1.99992840e-01  -3.63461348e-03   8.98359942e-02   2.13714106e-01\n",
      "    1.06054680e-01   1.52955762e-01  -2.73918553e-02  -2.04042978e-01\n",
      "    2.47597909e-02   4.27800964e-02   1.36080623e-01  -1.45578345e-01\n",
      "    1.50955170e-01   7.97413360e-02  -1.98514612e-02   1.30440680e-01\n",
      "    1.59981090e-01   1.45609704e-01   7.93190969e-02  -1.04396102e-01\n",
      "   -3.04116006e-02   1.16276760e-01  -1.57792328e-01   2.76227123e-01\n",
      "    3.40302296e-02  -3.60567000e-02  -6.01372519e-02  -3.64216480e-01\n",
      "    1.70149167e-02   1.41395279e-01   1.02552856e-01  -9.13767314e-02\n",
      "    2.75716324e-01  -5.83435639e-03   4.26654287e-01  -1.35271758e-01\n",
      "   -2.92745872e-01  -1.12584694e-01  -1.53783567e-02  -9.30219100e-02\n",
      "   -1.95188949e-01  -3.44082982e-01  -1.02877297e-01  -2.85478785e-01\n",
      "   -8.94649849e-02  -5.14577218e-01  -6.74452616e-02  -1.60848883e-01\n",
      "    1.11721538e+00  -1.36929449e-01   5.37145878e-02   1.82272453e-01\n",
      "   -1.58929575e-01  -3.01019188e-02   4.05579777e-02   2.00627657e-01\n",
      "    2.04616790e-01  -1.95035426e-02   2.49994117e-01   1.30535860e-01\n",
      "   -1.01718393e-01  -7.88506207e-02  -1.29129713e-01   1.26077419e-01\n",
      "   -1.22129025e-01   2.37699782e-01   1.93531482e-01   4.41953894e-02\n",
      "   -8.04272779e-02   1.57147193e-02  -3.06267939e-01]]\n",
      "Accuracy of:  0.635014836795\n",
      "CPU times: user 444 ms, sys: 70.7 ms, total: 515 ms\n",
      "Wall time: 166 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
