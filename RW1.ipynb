{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_imputed = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_imputed = df_imputed.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoking\n",
      "Alcohol\n",
      "Punctuality\n",
      "Lying\n",
      "Internet usage\n",
      "Gender\n",
      "Left - right handed\n",
      "Education\n",
      "Only child\n",
      "Village - town\n",
      "House - block of flats\n"
     ]
    }
   ],
   "source": [
    "for col in ['Smoking', 'Alcohol', 'Punctuality', 'Lying', 'Internet usage',\n",
    "        'Gender', 'Left - right handed', 'Education', 'Only child',\n",
    "        'Village - town', 'House - block of flats']:\n",
    "    print(col)\n",
    "    df_imputed = df_imputed.drop(col,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Fear of public speaking' in df_imputed:\n",
    "    y = df_imputed['Fear of public speaking'].values # get the labels we want\n",
    "    del df_imputed['Fear of public speaking'] # get rid of the class label\n",
    "#     norm_features = ['Music' ]\n",
    "#     df_imputed[norm_features] = (df_imputed[norm_features]-df_imputed[norm_features].mean()) / df_imputed[norm_features].std()\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = df_imputed.columns.to_series().groupby(df_imputed.dtypes).groups\n",
    "{k.name: v for k, v in g.items()}\n",
    "cols_not_intfloat = ['Smoking', 'Alcohol', 'Punctuality', 'Lying', 'Internet usage',\n",
    "        'Gender', 'Left - right handed', 'Education', 'Only child',\n",
    "        'Village - town', 'House - block of flats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Binary Logistic Regression Object, Not Trainable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "        \n",
    "blr = BinaryLogisticRegressionBase(0.1)\n",
    "print(blr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained Binary Logistic Regression Object\n"
     ]
    }
   ],
   "source": [
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi # the actual update inside of sum\n",
    "            gradient += gradi.reshape(self.w_.shape) # reshape to be column vector and add to gradient\n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "            \n",
    "blr = BinaryLogisticRegression(0.1)\n",
    "print(blr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained MultiClass Logistic Regression Object\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "lr = LogisticRegression(0.1,1500)\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5.,    3.,    2., ...,  163.,   48.,    1.],\n",
       "       [   4.,    4.,    2., ...,  163.,   58.,    2.],\n",
       "       [   5.,    5.,    2., ...,  176.,   67.,    2.],\n",
       "       ..., \n",
       "       [   4.,    3.,    1., ...,  173.,   75.,    0.],\n",
       "       [   5.,    3.,    3., ...,  173.,   58.,    1.],\n",
       "       [   5.,    5.,    4., ...,  185.,   72.,    1.]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.283382789318\n",
      "CPU times: user 1.76 s, sys: 92 ms, total: 1.85 s\n",
      "Wall time: 487 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# can we do better? Maybe more iterations?\n",
    "params = dict(eta=0.1,\n",
    "              iterations=500)\n",
    "\n",
    "blr = LogisticRegression(**params)\n",
    "blr.fit(X,y)\n",
    "# print(blr)\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    90.77700297]\n",
      " [   433.32418398]\n",
      " [   294.8977003 ]\n",
      " [   281.83048961]\n",
      " [   203.89658754]\n",
      " [   191.42633531]\n",
      " [   270.17878338]\n",
      " [   250.28635015]\n",
      " [   316.78976261]\n",
      " [   347.07366469]\n",
      " [   217.62514837]\n",
      " [   230.09272997]\n",
      " [   261.1977003 ]\n",
      " [   251.3995549 ]\n",
      " [   247.46735905]\n",
      " [   285.69228487]\n",
      " [   263.94221068]\n",
      " [   252.73709199]\n",
      " [   210.05593472]\n",
      " [   196.76973294]\n",
      " [   420.92930267]\n",
      " [   247.39206231]\n",
      " [   309.515727  ]\n",
      " [   408.97945104]\n",
      " [   318.86802671]\n",
      " [   283.83583086]\n",
      " [   282.94873887]\n",
      " [   346.4055638 ]\n",
      " [   350.56120178]\n",
      " [   326.66543027]\n",
      " [   183.71031157]\n",
      " [   318.87218101]\n",
      " [   286.5115727 ]\n",
      " [   281.3884273 ]\n",
      " [   224.98323442]\n",
      " [   215.10511869]\n",
      " [   184.60051929]\n",
      " [   381.51498516]\n",
      " [   280.27507418]\n",
      " [   234.40571217]\n",
      " [   245.16135015]\n",
      " [   195.87826409]\n",
      " [   294.81891691]\n",
      " [   281.38709199]\n",
      " [   345.29480712]\n",
      " [   228.09176558]\n",
      " [   188.76149852]\n",
      " [   234.25675074]\n",
      " [   238.40964392]\n",
      " [   201.15007418]\n",
      " [   331.41186944]\n",
      " [   211.09918398]\n",
      " [   202.48879822]\n",
      " [   169.60408012]\n",
      " [   304.24992582]\n",
      " [   280.80022255]\n",
      " [   173.6101632 ]\n",
      " [   216.36253709]\n",
      " [   296.75089021]\n",
      " [   292.30037092]\n",
      " [   270.1810089 ]\n",
      " [   409.4264095 ]\n",
      " [   253.03790801]\n",
      " [   307.73316024]\n",
      " [   191.93931751]\n",
      " [   179.47366469]\n",
      " [   215.76557864]\n",
      " [   242.26520772]\n",
      " [   271.95252226]\n",
      " [   287.68790801]\n",
      " [   228.3097181 ]\n",
      " [   236.92188427]\n",
      " [   286.72284866]\n",
      " [   269.3629822 ]\n",
      " [   271.81587537]\n",
      " [   230.32292285]\n",
      " [   273.51973294]\n",
      " [   265.65281899]\n",
      " [   309.59176558]\n",
      " [   239.89250742]\n",
      " [   348.33746291]\n",
      " [   363.25697329]\n",
      " [   244.49658754]\n",
      " [   342.1764095 ]\n",
      " [   298.83071217]\n",
      " [   198.39910979]\n",
      " [   234.92025223]\n",
      " [   301.27307122]\n",
      " [   307.14710682]\n",
      " [   336.90289318]\n",
      " [   356.209273  ]\n",
      " [   176.94725519]\n",
      " [   354.20066766]\n",
      " [   200.62759644]\n",
      " [   258.45252226]\n",
      " [   363.70118694]\n",
      " [   373.42284866]\n",
      " [   275.44124629]\n",
      " [   338.5398368 ]\n",
      " [   297.34094955]\n",
      " [   279.15289318]\n",
      " [   291.77863501]\n",
      " [   298.0884273 ]\n",
      " [   184.5977003 ]\n",
      " [   281.69295252]\n",
      " [   234.99962908]\n",
      " [   296.61372404]\n",
      " [   301.05133531]\n",
      " [   319.54161721]\n",
      " [   264.54873887]\n",
      " [   270.10578635]\n",
      " [   280.56891691]\n",
      " [   323.69569733]\n",
      " [   299.72833828]\n",
      " [   274.92767062]\n",
      " [   301.13716617]\n",
      " [   359.00400593]\n",
      " [   315.97403561]\n",
      " [   281.97559347]\n",
      " [   326.89117211]\n",
      " [   309.97218101]\n",
      " [   264.98590504]\n",
      " [   291.7074184 ]\n",
      " [   256.81683976]\n",
      " [   335.19517804]\n",
      " [   300.69413947]\n",
      " [   298.67826409]\n",
      " [   254.88590504]\n",
      " [   282.42232938]\n",
      " [   291.03827893]\n",
      " [   265.65578635]\n",
      " [   280.42663205]\n",
      " [   273.89369436]\n",
      " [   256.96743323]\n",
      " [   314.0481454 ]\n",
      " [  1840.48353116]\n",
      " [ 15681.81639466]\n",
      " [  5934.01802671]\n",
      " [   114.97596439]]\n",
      "Accuracy of:  0.166172106825\n",
      "CPU times: user 388 ms, sys: 12.7 ms, total: 401 ms\n",
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# now lets do some vectorized coding\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "# use same params as defined above\n",
    "blr = VectorBinaryLogisticRegression(**params)\n",
    "blr.fit(X,y)\n",
    "print(blr.w_)\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    # extend init functions\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "        \n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self,X,y):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X,y)\n",
    "        \n",
    "        # add in regularization (to all except bias term)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now redefine the Logistic Regression Function where needed\n",
    "class RegularizedLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                      C=self.C)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.192592592593\n",
      "confusion matrix\n",
      " [[26  0  0  0  0]\n",
      " [36  0  0  0  0]\n",
      " [33  0  0  0  0]\n",
      " [25  0  0  0  0]\n",
      " [14  1  0  0  0]]\n",
      "accuracy 0.162962962963\n",
      "confusion matrix\n",
      " [[ 0  0  0  0  0  0]\n",
      " [13  0 12  0  0  0]\n",
      " [ 9  0 22  0  0  0]\n",
      " [ 9  0 34  0  0  0]\n",
      " [ 6  0 17  0  0  0]\n",
      " [ 2  0 11  0  0  0]]\n",
      "accuracy 0.148148148148\n",
      "confusion matrix\n",
      " [[ 0  0  0  0  0  0]\n",
      " [21  0  0  0  2  0]\n",
      " [18  0  0  0 10  0]\n",
      " [11  0  0  0 29  0]\n",
      " [ 8  0  0  0 20  0]\n",
      " [ 4  0  0  0 12  0]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = RegularizedLogisticRegression(eta=0.1,iterations=2000) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
