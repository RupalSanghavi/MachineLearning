{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "df_imputed = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_imputed = df_imputed.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in ['Smoking', 'Alcohol', 'Punctuality', 'Lying', 'Internet usage',\n",
    "        'Gender', 'Left - right handed', 'Education', 'Only child',\n",
    "        'Village - town', 'House - block of flats']:\n",
    "    df_imputed = df_imputed.drop(col,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Fear of public speaking' in df_imputed:\n",
    "    y = df_imputed['Fear of public speaking'].values # get the labels we want\n",
    "    del df_imputed['Fear of public speaking'] # get rid of the class label\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31 µs, sys: 0 ns, total: 31 µs\n",
      "Wall time: 34.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "# blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "# blr.fit(X,y)\n",
    "# print(blr)\n",
    "\n",
    "# yhat = blr.predict(X)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53 µs, sys: 0 ns, total: 53 µs\n",
      "Wall time: 58.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X + 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "# hlr = HessianBinaryLogisticRegression(eta=0.1,iterations=20,C=0.1) # note that we need only a few iterations here\n",
    "\n",
    "# hlr.fit(X,y)\n",
    "# yhat = hlr.predict(X)\n",
    "# print(hlr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46 µs, sys: 1e+03 ns, total: 47 µs\n",
      "Wall time: 52 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += 2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "# bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "# bfgslr.fit(X,y)\n",
    "# yhat = bfgslr.predict(X)\n",
    "# print(bfgslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            hblr = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.318518518519\n",
      "confusion matrix\n",
      " [[ 8  9  4  3  1]\n",
      " [ 3  9  8  4  0]\n",
      " [ 3 11 22  5  5]\n",
      " [ 7  3 12  3  3]\n",
      " [ 0  1  6  4  1]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.303703703704\n",
      "confusion matrix\n",
      " [[ 5  2  5  2  1]\n",
      " [12  9  7  3  3]\n",
      " [ 5 15 15  5  5]\n",
      " [ 4  2 10 10  4]\n",
      " [ 0  1  5  3  2]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.392592592593\n",
      "confusion matrix\n",
      " [[11  6  3  2  0]\n",
      " [ 7 12  8  0  3]\n",
      " [ 2  8 19 11  4]\n",
      " [ 1  7 10  8  3]\n",
      " [ 0  1  4  2  3]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = MultiClassLogisticRegression(eta=0.1,iterations=10, C=0.0001) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ -3.21706244e+00   2.34172561e-02   6.63592728e-02   3.16253335e-02\n",
      "   -2.77161794e-02   2.73993499e-02   6.71017765e-02  -3.80480362e-02\n",
      "    2.24731870e-02   1.00764495e-01  -3.41310432e-02  -7.53741169e-02\n",
      "   -8.88313083e-02   6.28952354e-02  -7.61642514e-02   2.41592043e-03\n",
      "   -1.17151137e-02  -1.26963453e-02  -3.87819335e-02  -4.35865594e-02\n",
      "   -9.21133278e-02   6.22405342e-02  -1.54863995e-01   1.28738628e-01\n",
      "    1.78465088e-02  -2.71500022e-02   2.37307712e-03   2.10742002e-02\n",
      "   -6.22611823e-02  -5.92794663e-02   7.54948006e-03   1.88119046e-03\n",
      "    3.39335146e-02   3.71593252e-02  -2.61382559e-02   1.96259661e-02\n",
      "   -3.89869688e-02  -3.28651004e-02   8.00792579e-02   7.79963818e-03\n",
      "   -6.12550020e-02  -2.34705061e-03   3.37893317e-02  -7.69752796e-03\n",
      "   -4.65539728e-02  -1.90173488e-02   8.53542415e-02  -2.59244334e-02\n",
      "    2.94037088e-02   4.73827500e-02  -9.75182329e-02  -3.31325185e-02\n",
      "    5.10387086e-02  -3.62618843e-02   2.13583660e-02   5.87171433e-02\n",
      "    4.75676716e-03  -6.08219905e-02   3.99099997e-02   4.49480337e-02\n",
      "    5.98861812e-02  -1.61232584e-01  -3.52960828e-02  -3.50107901e-02\n",
      "   -5.66142869e-02   4.01832748e-02   1.21280514e-02  -1.80622778e-02\n",
      "    3.55893515e-02  -3.63993894e-02  -4.40225136e-02   4.73707523e-02\n",
      "   -7.58888659e-02  -9.82993941e-03  -3.95029795e-03   1.22193459e-01\n",
      "   -6.60737681e-03  -1.43778040e-02  -2.95211846e-02  -8.98777398e-02\n",
      "    4.33207969e-02   2.63114999e-02  -4.28756405e-02  -6.28126169e-02\n",
      "    5.08555547e-02  -3.63213080e-02   2.75821346e-02   5.63267968e-02\n",
      "   -1.29631383e-02  -1.86578655e-02   8.44259052e-02   2.57095243e-03\n",
      "   -3.09150323e-02   2.84283893e-02   2.10397157e-03   5.94055908e-02\n",
      "   -5.54026365e-03   4.14622917e-02   1.31246378e-02  -1.00770162e-01\n",
      "    1.90651344e-02   9.62132015e-02   1.14462910e-01  -5.98754062e-02\n",
      "    1.74479088e-01  -3.96856517e-02   1.63777482e-01  -4.38124642e-02\n",
      "    7.74695787e-02  -1.33254300e-02  -3.27676454e-02   5.81379714e-02\n",
      "    1.21028074e-02   8.35137073e-02   8.72560634e-02   7.49204080e-02\n",
      "   -2.71461626e-01  -3.50776603e-02   1.83980637e-02   1.02087374e-02\n",
      "    6.92979072e-03  -2.76055146e-02   7.62622957e-02   4.38665610e-02\n",
      "   -7.45159352e-03  -1.30470406e-02   7.13942839e-03  -1.01343749e-02\n",
      "    4.19527000e-02   1.45527336e-02   1.21991429e-03   5.26786580e-02\n",
      "   -3.91723623e-03  -3.79241006e-02   8.70349912e-02   5.60303181e-03\n",
      "   -4.10376234e-04   6.27733042e-03   2.24284318e-02]\n",
      " [  6.63008741e-01  -3.90932290e-02   6.90927829e-02  -1.33389564e-01\n",
      "    4.20758541e-02  -8.46375781e-02   1.63021381e-02   4.34921768e-02\n",
      "   -6.29339074e-02  -1.44263192e-01  -7.31256722e-02  -2.41272863e-02\n",
      "    9.09762671e-05   2.45676608e-02   9.63684383e-02   3.16390312e-02\n",
      "   -1.00177212e-01  -5.97641731e-02   7.38353961e-02  -5.49671009e-02\n",
      "    1.03559210e-01  -1.53698015e-02   1.06226150e-01  -5.86114481e-02\n",
      "   -1.17957533e-03   1.64059288e-02  -1.30034894e-02   1.12484489e-01\n",
      "   -2.66348717e-02  -2.12397422e-02   2.07647184e-01  -8.24849948e-02\n",
      "   -5.57302723e-02  -6.25021502e-02   1.30108805e-01  -8.16514539e-02\n",
      "    1.26775829e-01  -4.30438954e-02  -4.38124150e-02   3.91737944e-03\n",
      "   -4.28713025e-02   5.29722287e-02  -7.00177530e-02  -1.25942415e-02\n",
      "    7.61813715e-02  -3.42987502e-02  -1.27696887e-02  -1.93282372e-02\n",
      "   -7.18239041e-02  -1.20399566e-02   8.80322421e-02   8.45719823e-02\n",
      "   -7.10896944e-02   4.08943663e-02  -3.12686674e-02   4.44574060e-03\n",
      "   -4.14287688e-02   2.98568792e-02  -1.19818421e-01  -1.81454648e-02\n",
      "    7.62095003e-02   7.72369624e-02  -5.16576968e-02   4.41797328e-02\n",
      "   -5.35280336e-02  -6.94812499e-03  -4.86654698e-02  -7.46372939e-02\n",
      "   -2.85655058e-02  -5.12588170e-02   5.77378851e-02  -6.06882973e-02\n",
      "    4.40898604e-02  -1.30542803e-02  -7.56540469e-03  -4.17831115e-02\n",
      "    4.70924777e-02  -3.39174129e-02   5.02937853e-02   1.37618894e-02\n",
      "   -6.20292502e-02   1.29931309e-02   8.39085773e-03   4.31498184e-02\n",
      "   -1.00471136e-01   2.92463133e-02  -2.47729791e-02  -4.08482959e-02\n",
      "    3.71696233e-02   4.44082081e-02  -6.14818055e-02  -3.30778775e-02\n",
      "    7.73946892e-03   4.56156395e-02   9.38181121e-04  -7.24808183e-02\n",
      "    2.49973989e-02  -4.84318603e-02  -3.88401222e-03   4.83143760e-02\n",
      "   -1.24634357e-01   3.73255482e-02  -1.30603268e-01  -3.91131927e-03\n",
      "    7.36222242e-04   2.99468523e-02   4.75170593e-03  -7.04860181e-03\n",
      "    1.67167391e-01   7.30389521e-02   5.75288964e-02   7.12994092e-02\n",
      "   -3.58430724e-02   4.09204324e-02  -7.95647643e-02  -5.10063561e-02\n",
      "   -1.46963433e-01   1.05482519e-01   4.38569827e-02  -1.25672177e-01\n",
      "    4.04675258e-03  -1.18742123e-02  -1.92435189e-02  -1.87798176e-02\n",
      "   -6.48496503e-03  -4.32604761e-03  -1.82239682e-01   2.50294038e-02\n",
      "   -1.16344875e-01   1.10862815e-01  -1.66407344e-02  -6.31334319e-02\n",
      "    6.04801925e-03  -4.55778576e-02  -1.51192698e-03   4.38825034e-03\n",
      "    8.70644373e-03  -3.24341638e-03  -1.22663117e-02]\n",
      " [  8.74998077e-01  -4.76909873e-02  -1.10082535e-01  -3.20803289e-02\n",
      "    2.54017344e-02   6.11214031e-02  -5.30144467e-02   3.83466753e-02\n",
      "    4.68918346e-03   5.94467204e-02  -3.18400973e-03   7.74416925e-02\n",
      "    3.62273029e-02  -1.73312571e-01   1.83848695e-02  -9.89942584e-02\n",
      "    9.52106107e-02  -1.45796563e-03   4.56516839e-03  -2.28686031e-02\n",
      "    1.20512305e-01  -5.67072074e-02   1.30497436e-01  -7.28008339e-02\n",
      "   -6.14282128e-02  -2.51851158e-03   1.54852517e-02  -2.52952578e-01\n",
      "    2.02989116e-01   1.55340220e-01  -9.05377698e-02   7.82893468e-02\n",
      "   -4.65653786e-02   2.45809926e-02  -8.16804751e-02   5.04158754e-02\n",
      "   -7.20095090e-03   3.86783158e-02   1.01445237e-02  -4.03113961e-02\n",
      "    8.09845390e-02  -3.16483779e-02   4.28471408e-02  -2.04019446e-02\n",
      "   -4.17396914e-02   3.50662535e-02   2.03206212e-02  -2.21619590e-02\n",
      "   -6.21303143e-02  -1.38219757e-01   8.71253164e-02   2.12197395e-02\n",
      "   -1.58330624e-02  -5.50276649e-02   3.40454940e-02  -5.75062746e-02\n",
      "   -1.13630199e-02   1.03073070e-01   2.66131773e-02  -5.76387710e-02\n",
      "   -6.13799712e-02   7.83748102e-02  -5.96023715e-03  -3.19339892e-02\n",
      "    5.74467864e-02   1.03829344e-01  -3.35146836e-02   7.14138572e-02\n",
      "   -1.05752397e-01   3.22699635e-02  -4.56103736e-04  -3.06695217e-02\n",
      "   -7.78954139e-03  -5.27860825e-02   3.07312435e-02   6.20237855e-02\n",
      "   -9.69914868e-03  -2.89584744e-02   1.82075169e-02   4.70340270e-02\n",
      "   -1.38069071e-01  -3.06977869e-02   5.86365054e-02   2.84872606e-02\n",
      "    5.84589962e-02  -8.15780063e-02  -4.36336358e-02  -9.23356774e-03\n",
      "    4.83281245e-02  -2.69144688e-02   8.26738139e-02  -3.63193290e-02\n",
      "    2.13529483e-02  -1.09310195e-01   2.26696437e-02   6.65380613e-03\n",
      "   -2.76323793e-02   2.02930576e-02  -5.96733581e-02   3.99779652e-02\n",
      "    8.26986564e-02  -4.65281357e-02  -9.48165081e-02   1.09918716e-01\n",
      "   -2.85217604e-02   8.14975860e-02  -1.09523897e-01   4.62386977e-02\n",
      "   -9.44738344e-02   2.84492031e-02  -8.37593542e-02  -1.24172982e-01\n",
      "   -2.46529482e-02  -7.60573781e-02  -2.78727800e-02   1.87354742e-02\n",
      "    1.08267721e-01  -3.64237742e-02  -1.43266058e-02   1.28273927e-01\n",
      "    6.92398975e-02   8.59692455e-02  -8.28379299e-02  -1.60909385e-02\n",
      "   -2.45589493e-02   4.78939394e-02  -3.33457640e-03   1.29617616e-02\n",
      "    2.01598895e-02  -6.53370796e-02   7.88509955e-02  -7.88597076e-02\n",
      "    1.13910466e-01  -1.09162218e-02  -4.61820034e-02  -2.14039794e-02\n",
      "   -2.46376406e-03  -4.35483170e-03   5.91144294e-02]\n",
      " [ -1.15913313e+00  -5.73836307e-02  -3.39902629e-02   7.30905193e-02\n",
      "   -3.77518541e-02  -2.05668227e-02  -4.10728057e-02  -4.83875707e-02\n",
      "    7.77730133e-02  -3.45841812e-02   1.09355570e-01  -3.48869405e-03\n",
      "    1.02119664e-01   1.41435737e-02  -1.46853974e-03   1.40915576e-01\n",
      "   -4.68906648e-02   8.58149537e-02  -2.00422642e-02   1.45908871e-01\n",
      "   -6.05581744e-02  -4.04649934e-02  -9.22357667e-02  -1.14366158e-03\n",
      "    2.19765708e-02   7.81903997e-02  -3.68868071e-02   2.76669972e-02\n",
      "   -8.60645694e-02  -1.20085307e-02  -1.20071421e-01  -7.51492636e-02\n",
      "    4.85506657e-02  -1.57637359e-02  -5.00918410e-02   1.00132624e-02\n",
      "   -4.41605646e-02   6.36718510e-02  -4.13805444e-02  -1.61689189e-02\n",
      "    2.66959150e-02  -4.88595050e-02  -1.88209293e-02   4.19367235e-02\n",
      "    4.04888189e-03   3.82753860e-02  -4.91120386e-02   5.82999090e-02\n",
      "    3.29052902e-02   4.31286133e-02  -1.30785152e-01  -1.02996602e-01\n",
      "    8.36011902e-02  -7.79811527e-04  -3.62725122e-02   3.16994908e-02\n",
      "    4.61768212e-02  -6.34297733e-03   3.33726008e-02   4.87070100e-02\n",
      "    1.85548373e-02  -8.93582819e-02   8.09724519e-02   2.81607709e-02\n",
      "    2.68676259e-02  -3.65466775e-02   2.31043228e-02   1.45384455e-02\n",
      "    6.65325676e-02   5.53763505e-02  -2.38453536e-02   3.56888291e-02\n",
      "    2.37905238e-02   2.71286495e-02  -8.68955023e-03  -6.84902186e-02\n",
      "   -6.52054707e-02   1.96802768e-02  -6.98156638e-02   5.88864179e-02\n",
      "    1.17021957e-01  -2.19385347e-02  -2.75180260e-02  -2.37429759e-02\n",
      "   -7.42698726e-02   5.55951945e-02   2.45275710e-02   2.15987998e-02\n",
      "   -6.60026748e-02   4.58845693e-03  -1.15211246e-01   3.96894162e-02\n",
      "    3.19308351e-02  -1.05527769e-02  -4.80336592e-03   5.58897760e-02\n",
      "    2.93272521e-02  -4.04333201e-02   3.89537290e-02   3.70920533e-02\n",
      "   -5.00516558e-02  -8.73025954e-02  -2.56557723e-02   6.20548612e-03\n",
      "   -7.37103218e-02  -4.92532038e-02  -8.20378946e-02   3.07129649e-02\n",
      "   -6.40468877e-02  -4.13585487e-03   1.16747481e-01   8.17896059e-02\n",
      "    5.42730028e-02   6.02594521e-02  -1.48927790e-02  -6.54530854e-03\n",
      "    1.48078205e-01   6.19114033e-03  -8.42983254e-02  -9.66587783e-02\n",
      "   -2.31078526e-03  -9.85601860e-03   5.37184009e-02  -4.69101434e-02\n",
      "    1.46721346e-02  -4.62387594e-02   9.26601978e-02  -4.78457778e-02\n",
      "    6.42791953e-02  -3.67451342e-02  -5.42579521e-02   8.04835065e-02\n",
      "   -1.07428911e-01   5.35817139e-02  -6.03264779e-02   1.62027400e-03\n",
      "    3.62706607e-03  -1.95488702e-04  -3.54482478e-02]\n",
      " [ -1.20574672e+00   1.30995109e-01  -4.79288671e-03   7.28100631e-02\n",
      "    5.64099922e-03   4.42527076e-06   2.17872707e-04   1.36195518e-02\n",
      "   -4.54846448e-02   2.49413008e-02  -1.02843854e-02   1.72625833e-02\n",
      "   -4.81819879e-02   7.84662993e-02  -5.65431762e-02  -5.20892478e-02\n",
      "    4.10946431e-02   1.51550395e-03  -3.22228498e-03  -1.90592528e-02\n",
      "   -7.88298011e-02   4.09879929e-02   1.19089309e-02   8.55752525e-03\n",
      "    4.23148077e-02  -7.47836188e-02   2.27195862e-02   1.02442214e-01\n",
      "   -3.95924683e-02  -8.71942383e-02  -1.72662999e-05   5.97910384e-02\n",
      "    3.77089063e-02   1.10402268e-04   1.76829854e-02  -3.35723412e-04\n",
      "   -4.15879720e-02  -4.30976686e-02   7.96192823e-03   5.54098615e-02\n",
      "   -7.78249207e-03   2.39381747e-02   2.51305876e-03   1.69111942e-02\n",
      "    1.10831932e-03  -2.93791770e-02  -5.28025054e-02   4.92784556e-03\n",
      "    1.08797245e-01   7.95469366e-02   6.53598615e-02   5.47022029e-03\n",
      "   -5.85187035e-02   5.56051673e-02   2.15085032e-03  -3.90247069e-02\n",
      "    5.48212319e-03  -7.71506489e-02   2.30009318e-03   3.89176630e-04\n",
      "   -9.67328229e-02   7.25446337e-02   1.16906843e-02  -8.64704828e-03\n",
      "    1.16657333e-02  -8.73914265e-02   6.35703876e-02  -6.92427155e-03\n",
      "    4.35151811e-02  -2.42834421e-03   5.34527552e-03   2.98330889e-02\n",
      "    7.61343439e-03   4.99097668e-02   8.93757542e-03  -6.27150867e-02\n",
      "    1.80171934e-02   3.03987659e-02   3.64966490e-02  -4.36449386e-02\n",
      "    2.72001842e-02   1.34951857e-02  -8.86671838e-03   1.71673051e-02\n",
      "    5.77471944e-02   2.22284207e-02   3.18746413e-02  -2.66072287e-02\n",
      "   -4.78619201e-03  -6.29583818e-04   1.34099905e-03   2.97188830e-02\n",
      "   -1.47209600e-02   3.45164086e-02  -3.31671223e-03  -6.47122991e-02\n",
      "   -9.40255429e-03   3.30658779e-02   1.61515350e-02  -1.91611212e-02\n",
      "    6.14427450e-02   9.33716202e-03   1.65655791e-01  -5.59492331e-02\n",
      "   -8.53244958e-02  -2.01921881e-02   3.58531896e-02  -3.84045352e-02\n",
      "   -5.87158947e-02  -8.83920723e-02  -3.54313712e-02  -7.60885127e-02\n",
      "   -1.42355108e-02  -1.06273366e-01   2.49097991e-02  -4.08391770e-02\n",
      "    1.81730687e-01  -3.77386711e-02   3.33476810e-02   5.52405211e-02\n",
      "   -6.80332023e-02  -3.28433608e-02  -1.27720808e-02   3.20979149e-02\n",
      "    2.44895787e-02   4.55626580e-03   8.81885335e-02   4.81262264e-03\n",
      "   -1.68540255e-02  -2.43705181e-02  -2.16565162e-02   1.95173062e-02\n",
      "   -1.32201768e-02   4.82358539e-02   2.29261918e-02   8.58582056e-03\n",
      "   -1.15743113e-02   3.35256963e-03  -5.19072464e-02]]\n",
      "Accuracy of:  0.63649851632\n",
      "CPU times: user 1.09 s, sys: 24.9 ms, total: 1.12 s\n",
      "Wall time: 286 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear boundaries visualization from sklearn documentation\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_decision_boundaries(lr,Xin,y,title=''):\n",
    "    Xb = copy.deepcopy(Xin)\n",
    "    lr.fit(Xb[:,:2],y) # train only on two features\n",
    "\n",
    "    h=0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "    y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb[:, 0], Xb[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Fear of Public Speaking')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5.,    3.,    2., ...,  163.,   48.,    1.],\n",
       "       [   4.,    4.,    2., ...,  163.,   58.,    2.],\n",
       "       [   5.,    5.,    2., ...,  176.,   67.,    2.],\n",
       "       ..., \n",
       "       [   4.,    3.,    1., ...,  173.,   75.,    0.],\n",
       "       [   5.,    3.,    3., ...,  173.,   58.,    1.],\n",
       "       [   5.,    5.,    4., ...,  185.,   72.,    1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.lr_explor>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import widgets as wd\n",
    "\n",
    "cost_vals = np.logspace(-3,-2,15)\n",
    "def lr_explor(cost_idx):\n",
    "    C = cost_vals[cost_idx]\n",
    "    lr_clf = MultiClassLogisticRegression(eta=0.1,\n",
    "                                           iterations=2500,\n",
    "                                           C=C) # get object\n",
    "    \n",
    "    plot_decision_boundaries(lr_clf,X,y,title=\"C=%.5f\"%(C))\n",
    "\n",
    "wd.interact(lr_explor,cost_idx=(0,15,1),__manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.112759643917\n",
      "CPU times: user 640 ms, sys: 190 ms, total: 830 ms\n",
      "Wall time: 304 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression() # all params default\n",
    "lr_sk.fit(X,y)\n",
    "# print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "# yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "f978f21db7ed475a81c64c0dbfa4a7d6": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
